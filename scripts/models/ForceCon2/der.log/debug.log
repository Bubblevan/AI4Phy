2024-12-09 19:57:52,113 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-09 19:59:12,699 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-4.5827e-04
-5.6910e-06
-3.1902e-03
2024-12-09 19:59:12,720 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-5.4162e-03
-7.0859e-04
-2.1535e-03
2024-12-09 19:59:12,808 - logger.py:50 - Train Epoch [0], Step [0/80], Loss: 1.3418, MAE: 0.2881
2024-12-09 20:01:42,424 - logger.py:50 - 二阶导 hessian matrix (随机部分):
7.6110e-11
-5.4033e-10
-4.7731e-08
2024-12-09 20:01:42,479 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
4.4742e-05
-1.3305e-07
-1.6158e-11
2024-12-09 20:02:57,787 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-09 20:02:57,849 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
0.0000e+00
-7.3688e-09
0.0000e+00
2024-12-09 20:04:54,057 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-4.2689e-05
1.5000e-03
3.6918e-11
2024-12-09 20:04:54,085 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
1.2138e-07
-9.7072e-08
7.1868e-07
2024-12-09 20:05:50,120 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-09 20:13:15,827 - logger.py:50 - 二阶导 hessian matrix (随机部分):
1.7162e-04
2.3486e-12
0.0000e+00
-3.2804e-04
0.0000e+00
2024-12-09 20:13:15,942 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
2.1855e-07
9.2246e-08
-1.6268e-11
5.0500e-06
-9.9326e-11
2024-12-09 20:13:16,006 - logger.py:50 - Train Epoch [0], Step [0/20], Loss: 1.9307, MAE: 0.2411
2024-12-09 20:26:02,255 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
0.0000e+00
-5.8304e-08
2.5174e-12
1.1226e-07
2024-12-09 20:26:02,340 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-8.2858e-05
8.6442e-03
0.0000e+00
-1.7832e-07
0.0000e+00
2024-12-09 20:44:47,434 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-5.9956e-05
1.2424e-10
0.0000e+00
-1.0366e-07
0.0000e+00
2024-12-09 20:44:47,519 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
1.2951e-09
-3.6803e-07
1.3649e-07
-1.0041e-08
0.0000e+00
2024-12-09 21:11:22,237 - logger.py:50 - 二阶导 hessian matrix (随机部分):
3.4677e-04
-1.0253e-13
8.8318e-12
0.0000e+00
3.9956e-11
2024-12-09 21:11:22,275 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
0.0000e+00
-1.1614e-11
0.0000e+00
8.9983e-12
-1.0424e-11
2024-12-09 21:28:31,024 - logger.py:50 - 二阶导 hessian matrix (随机部分):
7.8156e-03
0.0000e+00
0.0000e+00
-2.2180e-11
1.2760e-11
2024-12-09 21:28:31,106 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-1.7153e-07
-9.1778e-12
-1.5293e-11
1.0287e-11
8.6990e-12
2024-12-09 21:44:45,047 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
-2.8224e-12
1.6317e-08
1.9433e-04
0.0000e+00
2024-12-09 21:44:45,115 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
1.1560e-07
-4.8940e-08
0.0000e+00
8.7069e-07
1.2697e-11
2024-12-09 22:07:11,545 - logger.py:50 - 二阶导 hessian matrix (随机部分):
7.8147e-13
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-09 22:07:11,617 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
1.8822e-09
3.4631e-06
1.0383e-07
1.0502e-13
2.3156e-08
2024-12-09 22:35:28,674 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
2.5555e-07
-1.5804e-12
1.6351e-11
7.9477e-07
2024-12-09 22:35:28,804 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-2.6229e-11
-1.8701e-08
0.0000e+00
-7.6642e-09
5.6593e-08
2024-12-09 23:02:23,964 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-3.0566e-11
3.1155e-08
0.0000e+00
6.5793e-07
0.0000e+00
2024-12-09 23:02:24,006 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
3.2940e-10
0.0000e+00
1.6793e-06
0.0000e+00
3.5488e-10
2024-12-09 23:14:54,864 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
-5.2232e-11
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-09 23:14:54,887 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
2.1140e-11
-2.2480e-10
-5.7062e-03
0.0000e+00
9.0249e-10
2024-12-09 23:33:09,231 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-3.3630e-12
-2.0453e-09
0.0000e+00
-1.7741e-12
3.5535e-12
2024-12-09 23:33:09,265 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-1.4165e-10
-3.6480e-04
3.8841e-12
7.5096e-11
0.0000e+00
2024-12-09 23:59:05,598 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-6.3031e-11
0.0000e+00
-5.9051e-08
-2.9746e-06
0.0000e+00
2024-12-09 23:59:05,641 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-3.2025e-13
0.0000e+00
-5.1436e-08
0.0000e+00
-1.3302e-03
2024-12-10 00:18:05,060 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-7.4045e-11
0.0000e+00
-1.4082e-11
0.0000e+00
0.0000e+00
2024-12-10 00:18:05,090 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-2.7746e-13
0.0000e+00
0.0000e+00
-4.4755e-13
-3.1831e-08
2024-12-10 00:35:51,807 - logger.py:50 - 二阶导 hessian matrix (随机部分):
8.7631e-08
0.0000e+00
3.5118e-11
5.0391e-11
-1.3388e-07
2024-12-10 00:35:51,832 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-7.4740e-14
0.0000e+00
1.2410e-08
-8.3800e-12
-2.0505e-07
2024-12-10 00:52:50,511 - logger.py:50 - 二阶导 hessian matrix (随机部分):
1.8812e-03
0.0000e+00
-7.5314e-12
nan
0.0000e+00
2024-12-10 00:52:50,539 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-7.5761e-11
6.5917e-12
9.1940e-15
-2.1006e-11
4.3588e-08
2024-12-10 01:03:06,369 - logger.py:50 - 二阶导 hessian matrix (随机部分):
-2.4284e-11
7.8586e-04
-1.5652e-11
1.2069e-12
9.3776e-11
2024-12-10 01:03:06,386 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-1.2973e-12
-1.0281e-07
-3.4556e-05
0.0000e+00
5.4702e-05
2024-12-10 01:16:53,625 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
-5.7363e-05
2.7068e-07
-7.2775e-05
0.0000e+00
2024-12-10 01:16:53,652 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-2.8751e-03
-3.9767e-11
6.7762e-07
0.0000e+00
0.0000e+00
2024-12-10 01:30:17,451 - logger.py:50 - 二阶导 hessian matrix (随机部分):
1.5325e-07
1.3087e-11
0.0000e+00
3.8009e-09
-4.4016e-13
2024-12-10 01:30:17,472 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
0.0000e+00
1.9308e-11
2.2362e-08
3.1865e-08
-2.5233e-11
2024-12-10 01:42:52,812 - logger.py:50 - 二阶导 hessian matrix (随机部分):
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
1.6935e-05
2024-12-10 01:42:52,828 - logger.py:50 - 最终二阶导 hessian matrix (随机部分):
-4.7779e-08
-2.3621e-07
0.0000e+00
2.1382e-11
0.0000e+00
2024-12-10 13:02:07,433 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 13:07:48,468 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 13:18:49,862 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 13:27:07,603 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 15:26:06,808 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 15:26:16,639 - logger.py:50 - Starting epoch 0 training.
2024-12-10 15:26:17,092 - logger.py:50 - Processing step 0/20
2024-12-10 15:26:17,171 - logger.py:50 - Starting data preprocessing.
2024-12-10 15:26:17,172 - logger.py:50 - Starting model forward pass.
2024-12-10 15:26:18,515 - logger.py:50 - Starting gradient computation.
2024-12-10 15:26:18,927 - logger.py:50 - Starting Hessian computation.
2024-12-10 15:35:50,844 - logger.py:50 - Sampled Hessian values:
1.7162e-04
2.3486e-12
0.0000e+00
-3.2804e-04
0.0000e+00
1.2968e-12
0.0000e+00
0.0000e+00
0.0000e+00
-3.3990e-04
2024-12-10 15:35:50,846 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 15:35:50,869 - logger.py:50 - Computing loss.
2024-12-10 15:35:50,889 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 15:53:03,386 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 15:53:12,428 - logger.py:50 - Starting epoch 0 training.
2024-12-10 15:53:12,744 - logger.py:50 - Processing step 0/20
2024-12-10 15:53:12,786 - logger.py:50 - Starting data preprocessing.
2024-12-10 15:53:12,786 - logger.py:50 - Starting model forward pass.
2024-12-10 15:53:13,941 - logger.py:50 - Starting gradient computation.
2024-12-10 15:53:14,295 - logger.py:50 - Starting Hessian computation.
2024-12-10 16:00:50,049 - logger.py:50 - Sampled Hessian values:
1.7162e-04
2.3486e-12
0.0000e+00
-3.2804e-04
0.0000e+00
1.2968e-12
0.0000e+00
0.0000e+00
0.0000e+00
-3.3990e-04
2024-12-10 16:00:50,051 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 16:00:50,071 - logger.py:50 - Computing loss.
2024-12-10 16:00:50,092 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 16:44:50,038 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 16:45:00,337 - logger.py:50 - Starting epoch 0 training.
2024-12-10 16:45:00,445 - logger.py:50 - Processing step 0/20
2024-12-10 16:45:00,489 - logger.py:50 - Starting data preprocessing.
2024-12-10 16:45:00,489 - logger.py:50 - Starting model forward pass.
2024-12-10 16:45:01,824 - logger.py:50 - Starting gradient computation.
2024-12-10 16:45:02,181 - logger.py:50 - Starting Hessian computation.
2024-12-10 16:52:48,245 - logger.py:50 - Sampled Hessian values:
1.7162e-04
2.3486e-12
0.0000e+00
-3.2804e-04
0.0000e+00
1.2969e-12
0.0000e+00
0.0000e+00
0.0000e+00
-3.3990e-04
2024-12-10 16:52:48,247 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 16:52:48,271 - logger.py:50 - Computing loss.
2024-12-10 16:52:48,292 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 17:02:11,716 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-10 17:02:21,722 - logger.py:50 - Processing step 0/20
2024-12-10 17:02:21,766 - logger.py:50 - Starting model forward pass.
2024-12-10 17:02:23,009 - logger.py:50 - Starting gradient computation.
2024-12-10 17:02:23,396 - logger.py:50 - Starting Hessian computation.
2024-12-10 17:11:08,016 - logger.py:50 - Sampled Hessian values:
1.7162e-04
2.3486e-12
0.0000e+00
-3.2804e-04
0.0000e+00
1.2968e-12
0.0000e+00
0.0000e+00
0.0000e+00
-3.3990e-04
2024-12-10 17:11:08,047 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 17:11:08,051 - logger.py:50 - Computing loss.
2024-12-10 17:11:08,072 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 17:11:08,101 - logger.py:50 - Epoch [0], Step [0/20], Loss: 1.9307, MAE: 0.2411
2024-12-10 17:11:08,196 - logger.py:50 - Processing step 1/20
2024-12-10 17:11:08,266 - logger.py:50 - Starting model forward pass.
2024-12-10 17:11:09,093 - logger.py:50 - Starting gradient computation.
2024-12-10 17:11:09,380 - logger.py:50 - Starting Hessian computation.
2024-12-10 17:23:40,586 - logger.py:50 - Sampled Hessian values:
0.0000e+00
3.8566e-13
3.2663e-13
1.5835e-09
-5.2448e-10
4.9950e-12
-2.9786e-11
-2.8247e-07
0.0000e+00
5.7314e-04
2024-12-10 17:23:40,588 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 17:23:40,591 - logger.py:50 - Computing loss.
2024-12-10 17:23:40,593 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 17:23:40,702 - logger.py:50 - Processing step 2/20
2024-12-10 17:23:40,769 - logger.py:50 - Starting model forward pass.
2024-12-10 17:23:41,060 - logger.py:50 - Starting gradient computation.
2024-12-10 17:23:41,558 - logger.py:50 - Starting Hessian computation.
2024-12-10 17:45:38,621 - logger.py:50 - Sampled Hessian values:
6.2917e-13
0.0000e+00
2.0350e-11
0.0000e+00
3.2448e-10
-9.0507e-12
4.6866e-12
0.0000e+00
-2.3916e-04
-3.9225e-11
2024-12-10 17:45:38,623 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 17:45:38,625 - logger.py:50 - Computing loss.
2024-12-10 17:45:38,628 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 17:45:38,743 - logger.py:50 - Processing step 3/20
2024-12-10 17:45:39,142 - logger.py:50 - Starting model forward pass.
2024-12-10 17:45:39,553 - logger.py:50 - Starting gradient computation.
2024-12-10 17:45:40,144 - logger.py:50 - Starting Hessian computation.
2024-12-10 18:15:32,707 - logger.py:50 - Sampled Hessian values:
7.4954e-08
-1.9393e-03
-6.5163e-12
0.0000e+00
2.1973e-11
3.1627e-12
4.9879e-08
-8.6158e-12
0.0000e+00
4.1035e-08
2024-12-10 18:15:32,709 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 18:15:32,712 - logger.py:50 - Computing loss.
2024-12-10 18:15:32,715 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 18:15:32,824 - logger.py:50 - Processing step 4/20
2024-12-10 18:15:32,894 - logger.py:50 - Starting model forward pass.
2024-12-10 18:15:33,078 - logger.py:50 - Starting gradient computation.
2024-12-10 18:15:33,744 - logger.py:50 - Starting Hessian computation.
2024-12-10 18:39:54,186 - logger.py:50 - Sampled Hessian values:
8.9041e-07
0.0000e+00
3.2490e-11
2.0992e-12
2.0949e-12
0.0000e+00
-1.0211e-11
-1.3433e-09
-1.2070e-12
-1.6354e-11
2024-12-10 18:39:54,188 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 18:39:54,191 - logger.py:50 - Computing loss.
2024-12-10 18:39:54,193 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 18:39:54,300 - logger.py:50 - Processing step 5/20
2024-12-10 18:39:54,367 - logger.py:50 - Starting model forward pass.
2024-12-10 18:39:54,825 - logger.py:50 - Starting gradient computation.
2024-12-10 18:39:55,499 - logger.py:50 - Starting Hessian computation.
2024-12-10 19:05:38,483 - logger.py:50 - Sampled Hessian values:
-2.2773e-11
-4.4161e-11
6.8944e-11
-3.3654e-11
-1.6224e-10
-2.2771e-11
5.2953e-07
3.5571e-10
1.7123e-11
-5.9422e-11
2024-12-10 19:05:38,484 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 19:05:38,486 - logger.py:50 - Computing loss.
2024-12-10 19:05:38,487 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 19:05:38,567 - logger.py:50 - Processing step 6/20
2024-12-10 19:05:38,620 - logger.py:50 - Starting model forward pass.
2024-12-10 19:05:38,745 - logger.py:50 - Starting gradient computation.
2024-12-10 19:05:39,414 - logger.py:50 - Starting Hessian computation.
2024-12-10 19:34:04,153 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
-2.9876e-12
0.0000e+00
0.0000e+00
0.0000e+00
-4.8943e-08
-1.3145e-08
0.0000e+00
0.0000e+00
2024-12-10 19:34:04,177 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 19:34:04,184 - logger.py:50 - Computing loss.
2024-12-10 19:34:04,244 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 19:34:04,728 - logger.py:50 - Processing step 7/20
2024-12-10 19:34:07,645 - logger.py:50 - Starting model forward pass.
2024-12-10 19:34:08,152 - logger.py:50 - Starting gradient computation.
2024-12-10 19:34:08,792 - logger.py:50 - Starting Hessian computation.
2024-12-10 20:04:04,713 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
-4.4697e-12
0.0000e+00
-1.8463e-10
8.4678e-10
-3.8735e-12
-1.9411e-11
-4.9036e-11
1.8803e-12
2024-12-10 20:04:04,715 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 20:04:04,717 - logger.py:50 - Computing loss.
2024-12-10 20:04:04,720 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 20:04:04,878 - logger.py:50 - Processing step 8/20
2024-12-10 20:04:04,935 - logger.py:50 - Starting model forward pass.
2024-12-10 20:04:05,087 - logger.py:50 - Starting gradient computation.
2024-12-10 20:04:05,771 - logger.py:50 - Starting Hessian computation.
2024-12-10 20:34:12,192 - logger.py:50 - Sampled Hessian values:
-6.1392e-04
0.0000e+00
1.5897e-08
2.7105e-07
5.1119e-11
0.0000e+00
0.0000e+00
3.6149e-03
3.7156e-08
-1.2415e-03
2024-12-10 20:34:12,193 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 20:34:12,196 - logger.py:50 - Computing loss.
2024-12-10 20:34:12,199 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 20:34:12,317 - logger.py:50 - Processing step 9/20
2024-12-10 20:34:12,370 - logger.py:50 - Starting model forward pass.
2024-12-10 20:34:12,529 - logger.py:50 - Starting gradient computation.
2024-12-10 20:34:13,213 - logger.py:50 - Starting Hessian computation.
2024-12-10 20:57:06,425 - logger.py:50 - Sampled Hessian values:
-3.5948e-11
0.0000e+00
0.0000e+00
0.0000e+00
4.2562e-10
-6.1254e-09
0.0000e+00
0.0000e+00
-4.8925e-13
3.2625e-11
2024-12-10 20:57:06,427 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 20:57:06,429 - logger.py:50 - Computing loss.
2024-12-10 20:57:06,431 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 20:57:06,557 - logger.py:50 - Processing step 10/20
2024-12-10 20:57:06,630 - logger.py:50 - Starting model forward pass.
2024-12-10 20:57:06,796 - logger.py:50 - Starting gradient computation.
2024-12-10 20:57:07,460 - logger.py:50 - Starting Hessian computation.
2024-12-10 21:23:25,465 - logger.py:50 - Sampled Hessian values:
1.5669e-10
0.0000e+00
-7.3983e-11
8.2015e-05
-1.5295e-07
-9.9597e-15
-4.8644e-08
0.0000e+00
8.3277e-09
0.0000e+00
2024-12-10 21:23:25,466 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 21:23:25,469 - logger.py:50 - Computing loss.
2024-12-10 21:23:25,471 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 21:23:25,598 - logger.py:50 - Processing step 11/20
2024-12-10 21:23:25,868 - logger.py:50 - Starting model forward pass.
2024-12-10 21:23:26,026 - logger.py:50 - Starting gradient computation.
2024-12-10 21:23:26,702 - logger.py:50 - Starting Hessian computation.
2024-12-10 21:53:48,443 - logger.py:50 - Sampled Hessian values:
0.0000e+00
-1.4511e-08
1.7584e-06
2.9015e-07
1.7103e-07
0.0000e+00
0.0000e+00
-2.0361e-04
0.0000e+00
0.0000e+00
2024-12-10 21:53:48,444 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 21:53:48,447 - logger.py:50 - Computing loss.
2024-12-10 21:53:48,450 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 21:53:48,592 - logger.py:50 - Processing step 12/20
2024-12-10 21:53:48,654 - logger.py:50 - Starting model forward pass.
2024-12-10 21:53:49,129 - logger.py:50 - Starting gradient computation.
2024-12-10 21:53:49,829 - logger.py:50 - Starting Hessian computation.
2024-12-10 22:21:02,047 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
-9.0343e-09
0.0000e+00
-3.8876e-08
6.5792e-12
1.1080e-10
7.9398e-13
6.5026e-12
-1.2301e-08
2024-12-10 22:21:02,049 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 22:21:02,052 - logger.py:50 - Computing loss.
2024-12-10 22:21:02,054 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 22:21:02,190 - logger.py:50 - Processing step 13/20
2024-12-10 22:21:02,265 - logger.py:50 - Starting model forward pass.
2024-12-10 22:21:02,724 - logger.py:50 - Starting gradient computation.
2024-12-10 22:21:03,424 - logger.py:50 - Starting Hessian computation.
2024-12-10 22:48:00,038 - logger.py:50 - Sampled Hessian values:
8.0733e-12
2.8115e-04
4.3236e-08
-3.0072e-11
2.3330e-10
-9.5200e-12
0.0000e+00
-3.1108e-10
0.0000e+00
0.0000e+00
2024-12-10 22:48:00,040 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 22:48:00,043 - logger.py:50 - Computing loss.
2024-12-10 22:48:00,046 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 22:48:00,189 - logger.py:50 - Processing step 14/20
2024-12-10 22:48:00,262 - logger.py:50 - Starting model forward pass.
2024-12-10 22:48:00,456 - logger.py:50 - Starting gradient computation.
2024-12-10 22:48:01,167 - logger.py:50 - Starting Hessian computation.
2024-12-10 23:13:36,766 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
-9.0751e-10
8.8612e-13
0.0000e+00
0.0000e+00
4.0186e-04
5.2342e-13
-5.3045e-12
0.0000e+00
2024-12-10 23:13:36,770 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 23:13:36,773 - logger.py:50 - Computing loss.
2024-12-10 23:13:36,776 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 23:13:36,901 - logger.py:50 - Processing step 15/20
2024-12-10 23:13:37,210 - logger.py:50 - Starting model forward pass.
2024-12-10 23:13:37,660 - logger.py:50 - Starting gradient computation.
2024-12-10 23:13:38,288 - logger.py:50 - Starting Hessian computation.
2024-12-10 23:34:24,488 - logger.py:50 - Sampled Hessian values:
3.9073e-04
0.0000e+00
6.6065e-13
6.2235e-04
2.3569e-16
9.3036e-12
0.0000e+00
3.3813e-11
0.0000e+00
-5.5564e-12
2024-12-10 23:34:24,490 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 23:34:24,494 - logger.py:50 - Computing loss.
2024-12-10 23:34:24,497 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 23:34:24,615 - logger.py:50 - Processing step 16/20
2024-12-10 23:34:24,677 - logger.py:50 - Starting model forward pass.
2024-12-10 23:34:24,853 - logger.py:50 - Starting gradient computation.
2024-12-10 23:34:25,498 - logger.py:50 - Starting Hessian computation.
2024-12-10 23:58:57,641 - logger.py:50 - Sampled Hessian values:
-5.1319e-11
2.5613e-08
-5.6536e-11
0.0000e+00
0.0000e+00
0.0000e+00
3.3985e-03
0.0000e+00
-1.7224e-07
0.0000e+00
2024-12-10 23:58:57,643 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-10 23:58:57,646 - logger.py:50 - Computing loss.
2024-12-10 23:58:57,648 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-10 23:58:57,772 - logger.py:50 - Processing step 17/20
2024-12-10 23:58:57,820 - logger.py:50 - Starting model forward pass.
2024-12-10 23:58:57,958 - logger.py:50 - Starting gradient computation.
2024-12-10 23:58:58,601 - logger.py:50 - Starting Hessian computation.
2024-12-11 00:22:15,945 - logger.py:50 - Sampled Hessian values:
-2.9921e-07
-1.4833e-09
2.3337e-10
-6.3689e-11
0.0000e+00
0.0000e+00
0.0000e+00
3.3514e-12
-6.0973e-08
-3.5611e-09
2024-12-11 00:22:15,946 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-11 00:22:15,950 - logger.py:50 - Computing loss.
2024-12-11 00:22:15,955 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-11 00:22:16,078 - logger.py:50 - Processing step 18/20
2024-12-11 00:22:16,141 - logger.py:50 - Starting model forward pass.
2024-12-11 00:22:16,308 - logger.py:50 - Starting gradient computation.
2024-12-11 00:22:16,959 - logger.py:50 - Starting Hessian computation.
2024-12-11 00:45:49,772 - logger.py:50 - Sampled Hessian values:
4.5839e-12
0.0000e+00
-1.8419e-10
0.0000e+00
-8.0127e-07
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
-3.5113e-11
2024-12-11 00:45:49,775 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-11 00:45:49,777 - logger.py:50 - Computing loss.
2024-12-11 00:45:49,781 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-11 00:45:49,952 - logger.py:50 - Processing step 19/20
2024-12-11 00:45:50,279 - logger.py:50 - Starting model forward pass.
2024-12-11 00:45:50,709 - logger.py:50 - Starting gradient computation.
2024-12-11 00:45:51,340 - logger.py:50 - Starting Hessian computation.
2024-12-12 00:31:03,129 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 00:31:59,087 - logger.py:50 - Processing step 0/2
2024-12-12 00:31:59,139 - logger.py:50 - Starting model forward pass.
2024-12-12 00:32:00,950 - logger.py:50 - Starting gradient computation.
2024-12-12 16:25:59,305 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:26:06,966 - logger.py:50 - Processing step 0/2
2024-12-12 16:26:07,021 - logger.py:50 - Starting model forward pass.
2024-12-12 16:26:08,736 - logger.py:50 - Starting gradient computation.
2024-12-12 16:28:02,109 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:28:09,566 - logger.py:50 - Processing step 0/4
2024-12-12 16:28:09,779 - logger.py:50 - Starting model forward pass.
2024-12-12 16:28:11,536 - logger.py:50 - Starting gradient computation.
2024-12-12 16:29:59,964 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9486e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 16:29:59,991 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:29:59,993 - logger.py:50 - Computing loss.
2024-12-12 16:30:00,026 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 16:30:00,028 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f661bb84610>, requires_grad: True
2024-12-12 16:30:00,028 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:30:00,030 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:30:00,090 - logger.py:50 - 参数未更新
2024-12-12 16:30:00,091 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 16:30:00,146 - logger.py:50 - Processing step 1/4
2024-12-12 16:30:00,189 - logger.py:50 - Starting model forward pass.
2024-12-12 16:30:00,894 - logger.py:50 - Starting gradient computation.
2024-12-12 16:36:06,179 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:36:13,540 - logger.py:50 - Processing step 0/4
2024-12-12 16:36:13,739 - logger.py:50 - Starting model forward pass.
2024-12-12 16:36:14,671 - logger.py:50 - Starting gradient computation.
2024-12-12 16:38:02,990 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9486e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 16:38:03,021 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:38:03,022 - logger.py:50 - Computing loss.
2024-12-12 16:38:03,040 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 16:38:03,041 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f51947a7af0>, requires_grad: True
2024-12-12 16:38:03,041 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:38:03,043 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:38:03,105 - logger.py:50 - 参数未更新
2024-12-12 16:38:03,106 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 16:38:03,163 - logger.py:50 - Processing step 1/4
2024-12-12 16:38:03,205 - logger.py:50 - Starting model forward pass.
2024-12-12 16:38:03,936 - logger.py:50 - Starting gradient computation.
2024-12-12 16:38:41,768 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:38:49,036 - logger.py:50 - Processing step 0/4
2024-12-12 16:38:49,238 - logger.py:50 - Starting model forward pass.
2024-12-12 16:38:50,158 - logger.py:50 - Starting gradient computation.
2024-12-12 16:39:16,394 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:39:23,885 - logger.py:50 - Processing step 0/4
2024-12-12 16:39:24,098 - logger.py:50 - Starting model forward pass.
2024-12-12 16:39:25,078 - logger.py:50 - Starting gradient computation.
2024-12-12 16:43:17,177 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:43:24,586 - logger.py:50 - Processing step 0/4
2024-12-12 16:43:24,784 - logger.py:50 - Starting model forward pass.
2024-12-12 16:43:25,709 - logger.py:50 - Starting gradient computation.
2024-12-12 16:45:13,448 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9486e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 16:45:13,467 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:45:13,468 - logger.py:50 - Computing loss.
2024-12-12 16:45:13,486 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 16:45:13,486 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f98935fcaf0>, requires_grad: True
2024-12-12 16:45:13,486 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:45:13,488 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:45:13,542 - logger.py:50 - 参数未更新
2024-12-12 16:45:13,544 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 16:45:13,605 - logger.py:50 - Processing step 1/4
2024-12-12 16:45:13,652 - logger.py:50 - Starting model forward pass.
2024-12-12 16:45:14,343 - logger.py:50 - Starting gradient computation.
2024-12-12 16:47:06,096 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:47:13,504 - logger.py:50 - Processing step 0/4
2024-12-12 16:47:13,726 - logger.py:50 - Starting model forward pass.
2024-12-12 16:47:14,657 - logger.py:50 - Starting gradient computation.
2024-12-12 16:49:42,006 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9487e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 16:49:42,036 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:49:50,608 - logger.py:50 - Computing loss.
2024-12-12 16:49:50,623 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 16:49:50,623 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fafce8d9640>, requires_grad: True
2024-12-12 16:50:07,470 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:50:07,472 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:50:07,539 - logger.py:50 - 参数未更新
2024-12-12 16:50:07,540 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 16:50:07,603 - logger.py:50 - Processing step 1/4
2024-12-12 16:50:07,650 - logger.py:50 - Starting model forward pass.
2024-12-12 16:50:08,351 - logger.py:50 - Starting gradient computation.
2024-12-12 16:50:41,445 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 16:50:48,967 - logger.py:50 - Processing step 0/4
2024-12-12 16:50:49,185 - logger.py:50 - Starting model forward pass.
2024-12-12 16:50:50,162 - logger.py:50 - Starting gradient computation.
2024-12-12 16:54:01,866 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9487e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 16:54:01,888 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:54:01,890 - logger.py:50 - Computing loss.
2024-12-12 16:54:01,909 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 16:54:01,909 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b8379f670>, requires_grad: True
2024-12-12 16:54:01,910 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:54:01,912 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:54:01,975 - logger.py:50 - 参数未更新
2024-12-12 16:54:01,976 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 16:54:02,034 - logger.py:50 - Processing step 1/4
2024-12-12 16:54:02,077 - logger.py:50 - Starting model forward pass.
2024-12-12 16:54:02,772 - logger.py:50 - Starting gradient computation.
2024-12-12 16:58:06,476 - logger.py:50 - Sampled Hessian values:
-6.4617e-11
6.7074e-08
-2.5392e-07
-1.1414e-04
-3.1425e-11
0.0000e+00
-1.4037e-04
1.0651e-03
1.0717e-11
5.3880e-08
2024-12-12 16:58:06,478 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 16:58:06,479 - logger.py:50 - Computing loss.
2024-12-12 16:58:06,480 - logger.py:50 - loss的值：2.080552577972412
2024-12-12 16:58:06,481 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b702179d0>, requires_grad: True
2024-12-12 16:58:06,481 - logger.py:50 - Visualizing computation graph.
2024-12-12 16:58:06,483 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 16:58:06,524 - logger.py:50 - 参数未更新
2024-12-12 16:58:06,585 - logger.py:50 - Processing step 2/4
2024-12-12 16:58:06,631 - logger.py:50 - Starting model forward pass.
2024-12-12 16:58:06,746 - logger.py:50 - Starting gradient computation.
2024-12-12 17:03:12,752 - logger.py:50 - Sampled Hessian values:
4.8809e-11
-1.0975e-03
-4.2938e-13
-1.1765e-08
-5.9687e-07
-6.2322e-13
9.4015e-11
5.7402e-08
3.3440e-11
-6.5715e-11
2024-12-12 17:03:12,753 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:03:12,755 - logger.py:50 - Computing loss.
2024-12-12 17:03:12,756 - logger.py:50 - loss的值：0.8949596881866455
2024-12-12 17:03:12,756 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b8379f670>, requires_grad: True
2024-12-12 17:03:12,756 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:03:12,758 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:03:12,800 - logger.py:50 - 参数未更新
2024-12-12 17:03:12,847 - logger.py:50 - Processing step 3/4
2024-12-12 17:03:12,881 - logger.py:50 - Starting model forward pass.
2024-12-12 17:03:12,991 - logger.py:50 - Starting gradient computation.
2024-12-12 17:06:26,562 - logger.py:50 - Sampled Hessian values:
-2.7214e-07
-4.9483e-04
-1.9476e-03
1.3257e-03
-1.0890e-11
-8.0036e-10
-1.4467e-07
-4.0330e-07
8.3834e-03
-3.1301e-04
2024-12-12 17:06:26,564 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:06:26,565 - logger.py:50 - Computing loss.
2024-12-12 17:06:26,566 - logger.py:50 - loss的值：0.286880224943161
2024-12-12 17:06:26,566 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b702179d0>, requires_grad: True
2024-12-12 17:06:26,566 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:06:26,569 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:06:26,602 - logger.py:50 - 参数未更新
2024-12-12 17:07:11,563 - logger.py:50 - Step [0/1], Loss: 1.3727, MAE: 0.3626
2024-12-12 17:08:01,622 - logger.py:50 - Step [0/1], Loss: 1.3524, MAE: 0.2774
2024-12-12 17:08:02,067 - logger.py:50 - Epoch: [0] train loss: 1.05318, train MAE: 0.17630,val loss: 1.37273, val MAE: 0.36264,test loss: 1.35236, test MAE: 0.27735,Time: 1033.15s
2024-12-12 17:08:02,068 - logger.py:50 - Best -- epoch=0, train loss: 1.05318, val loss: 1.37273, test loss: 1.35236

2024-12-12 17:08:02,142 - logger.py:50 - Processing step 0/4
2024-12-12 17:08:02,191 - logger.py:50 - Starting model forward pass.
2024-12-12 17:08:02,270 - logger.py:50 - Starting gradient computation.
2024-12-12 17:12:37,795 - logger.py:50 - Sampled Hessian values:
7.3676e-11
1.8305e-04
0.0000e+00
5.7061e-08
1.2634e-11
-7.5034e-12
0.0000e+00
6.0577e-07
6.4963e-11
-2.0082e-11
2024-12-12 17:12:37,796 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:12:37,797 - logger.py:50 - Computing loss.
2024-12-12 17:12:37,798 - logger.py:50 - loss的值：1.9741488695144653
2024-12-12 17:12:37,798 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b5417ea90>, requires_grad: True
2024-12-12 17:12:37,798 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:12:37,800 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:12:37,830 - logger.py:50 - 参数未更新
2024-12-12 17:12:37,832 - logger.py:50 - Epoch [1], Step [0/4], Loss: 1.9741, MAE: 0.2071
2024-12-12 17:12:37,892 - logger.py:50 - Processing step 1/4
2024-12-12 17:12:37,939 - logger.py:50 - Starting model forward pass.
2024-12-12 17:12:38,021 - logger.py:50 - Starting gradient computation.
2024-12-12 17:16:51,867 - logger.py:50 - Sampled Hessian values:
-3.2529e-08
-1.1405e-11
-1.9283e-07
0.0000e+00
1.5472e-09
5.2929e-08
2.5363e-04
4.9153e-05
2.5859e-05
-9.5832e-12
2024-12-12 17:16:51,868 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:16:51,870 - logger.py:50 - Computing loss.
2024-12-12 17:16:51,870 - logger.py:50 - loss的值：3.089205026626587
2024-12-12 17:16:51,871 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2cd18bbac0>, requires_grad: True
2024-12-12 17:16:51,871 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:16:51,873 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:16:51,903 - logger.py:50 - 参数未更新
2024-12-12 17:16:51,955 - logger.py:50 - Processing step 2/4
2024-12-12 17:16:51,994 - logger.py:50 - Starting model forward pass.
2024-12-12 17:16:52,088 - logger.py:50 - Starting gradient computation.
2024-12-12 17:20:04,548 - logger.py:50 - Sampled Hessian values:
0.0000e+00
-2.7277e-04
-1.3549e-11
-3.3145e-07
-1.7572e-10
-1.0843e-08
-5.7554e-03
-1.3935e-02
2.4893e-07
-6.1042e-12
2024-12-12 17:20:04,549 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:20:04,550 - logger.py:50 - Computing loss.
2024-12-12 17:20:04,551 - logger.py:50 - loss的值：0.286880224943161
2024-12-12 17:20:04,551 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2b70300cd0>, requires_grad: True
2024-12-12 17:20:04,551 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:20:04,554 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:20:04,587 - logger.py:50 - 参数未更新
2024-12-12 17:20:04,851 - logger.py:50 - Processing step 3/4
2024-12-12 17:20:04,895 - logger.py:50 - Starting model forward pass.
2024-12-12 17:20:04,992 - logger.py:50 - Starting gradient computation.
2024-12-12 17:22:33,111 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 17:22:40,641 - logger.py:50 - Processing step 0/4
2024-12-12 17:22:40,845 - logger.py:50 - Starting model forward pass.
2024-12-12 17:22:41,769 - logger.py:50 - Starting gradient computation.
2024-12-12 17:25:09,648 - logger.py:50 - Sampled Hessian values:
3.2684e-12
-8.3910e-08
-2.4355e-11
-1.6728e-08
-3.9486e-13
0.0000e+00
2.7405e-12
-2.8417e-06
9.5682e-05
-1.0660e-07
2024-12-12 17:25:09,670 - logger.py:50 - Processing Hessian matrix for subgraphs.
2024-12-12 17:25:09,672 - logger.py:50 - Computing loss.
2024-12-12 17:25:09,691 - logger.py:50 - loss的值：0.7932259440422058
2024-12-12 17:25:09,692 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fb47acd9670>, requires_grad: True
2024-12-12 17:25:09,692 - logger.py:50 - Visualizing computation graph.
2024-12-12 17:25:09,694 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 17:25:09,753 - logger.py:50 - 参数未更新
2024-12-12 17:25:09,754 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1636
2024-12-12 17:25:09,812 - logger.py:50 - Processing step 1/4
2024-12-12 17:25:09,855 - logger.py:50 - Starting model forward pass.
2024-12-12 17:25:10,531 - logger.py:50 - Starting gradient computation.
2024-12-12 18:18:40,970 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:18:48,382 - logger.py:50 - Processing step 0/4
2024-12-12 18:18:48,600 - logger.py:50 - Starting model forward pass.
2024-12-12 18:18:49,525 - logger.py:50 - Starting gradient computation.
2024-12-12 18:19:11,027 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:19:18,468 - logger.py:50 - Processing step 0/4
2024-12-12 18:19:18,667 - logger.py:50 - Starting model forward pass.
2024-12-12 18:19:19,591 - logger.py:50 - Starting gradient computation.
2024-12-12 18:19:19,834 - logger.py:50 - Computing loss.
2024-12-12 18:19:19,848 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 18:19:19,848 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f257c44cd60>, requires_grad: True
2024-12-12 18:19:19,848 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:19:19,848 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:19:19,896 - logger.py:50 - 参数未更新
2024-12-12 18:19:19,897 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 18:19:19,935 - logger.py:50 - Processing step 1/4
2024-12-12 18:19:19,966 - logger.py:50 - Starting model forward pass.
2024-12-12 18:19:20,789 - logger.py:50 - Starting gradient computation.
2024-12-12 18:19:20,975 - logger.py:50 - Computing loss.
2024-12-12 18:19:20,976 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 18:19:20,977 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f257c51f670>, requires_grad: True
2024-12-12 18:19:20,977 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:19:20,978 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:19:20,992 - logger.py:50 - 参数未更新
2024-12-12 18:19:21,033 - logger.py:50 - Processing step 2/4
2024-12-12 18:19:21,065 - logger.py:50 - Starting model forward pass.
2024-12-12 18:19:21,633 - logger.py:50 - Starting gradient computation.
2024-12-12 18:19:21,918 - logger.py:50 - Computing loss.
2024-12-12 18:19:21,919 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 18:19:21,920 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2560325ca0>, requires_grad: True
2024-12-12 18:19:21,920 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:19:21,920 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:19:21,935 - logger.py:50 - 参数未更新
2024-12-12 18:19:21,967 - logger.py:50 - Processing step 3/4
2024-12-12 18:19:21,992 - logger.py:50 - Starting model forward pass.
2024-12-12 18:19:22,537 - logger.py:50 - Starting gradient computation.
2024-12-12 18:20:36,599 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:20:44,120 - logger.py:50 - Processing step 0/4
2024-12-12 18:20:44,330 - logger.py:50 - Starting model forward pass.
2024-12-12 18:20:45,247 - logger.py:50 - Starting gradient computation.
2024-12-12 18:20:45,528 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:20:45,529 - logger.py:50 - Computing loss.
2024-12-12 18:20:45,543 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 18:20:45,543 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f70c503a640>, requires_grad: True
2024-12-12 18:20:45,543 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:20:45,543 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:20:45,590 - logger.py:50 - 参数未更新
2024-12-12 18:20:45,591 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 18:20:45,632 - logger.py:50 - Processing step 1/4
2024-12-12 18:20:45,664 - logger.py:50 - Starting model forward pass.
2024-12-12 18:20:46,447 - logger.py:50 - Starting gradient computation.
2024-12-12 18:20:46,675 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:20:46,675 - logger.py:50 - Computing loss.
2024-12-12 18:20:46,676 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 18:20:46,677 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f70b03fb850>, requires_grad: True
2024-12-12 18:20:46,677 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:20:46,677 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:20:46,692 - logger.py:50 - 参数未更新
2024-12-12 18:20:46,736 - logger.py:50 - Processing step 2/4
2024-12-12 18:20:46,769 - logger.py:50 - Starting model forward pass.
2024-12-12 18:20:47,330 - logger.py:50 - Starting gradient computation.
2024-12-12 18:20:47,668 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:20:47,669 - logger.py:50 - Computing loss.
2024-12-12 18:20:47,670 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 18:20:47,670 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f70c503a640>, requires_grad: True
2024-12-12 18:20:47,670 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:20:47,670 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:20:47,685 - logger.py:50 - 参数未更新
2024-12-12 18:22:04,212 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:22:11,620 - logger.py:50 - Processing step 0/4
2024-12-12 18:22:11,822 - logger.py:50 - Starting model forward pass.
2024-12-12 18:22:12,740 - logger.py:50 - Starting gradient computation.
2024-12-12 18:22:12,994 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:22:13,019 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:22:13,019 - logger.py:50 - Computing loss.
2024-12-12 18:22:13,033 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 18:22:13,033 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fd41009ee20>, requires_grad: True
2024-12-12 18:22:13,033 - logger.py:50 - Visualizing computation graph.
2024-12-12 18:22:13,033 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 18:22:13,081 - logger.py:50 - 参数未更新
2024-12-12 18:22:13,081 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 18:22:13,123 - logger.py:50 - Processing step 1/4
2024-12-12 18:22:13,154 - logger.py:50 - Starting model forward pass.
2024-12-12 18:26:12,204 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:26:19,528 - logger.py:50 - Processing step 0/4
2024-12-12 18:26:19,730 - logger.py:50 - Starting model forward pass.
2024-12-12 18:26:20,663 - logger.py:50 - Starting gradient computation.
2024-12-12 18:29:06,903 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:29:14,107 - logger.py:50 - Processing step 0/4
2024-12-12 18:29:14,311 - logger.py:50 - Starting model forward pass.
2024-12-12 18:29:15,228 - logger.py:50 - Starting gradient computation.
2024-12-12 18:36:50,066 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:36:57,397 - logger.py:50 - Processing step 0/4
2024-12-12 18:36:57,596 - logger.py:50 - Starting model forward pass.
2024-12-12 18:36:58,523 - logger.py:50 - Starting gradient computation.
2024-12-12 18:37:12,244 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:37:19,510 - logger.py:50 - Processing step 0/4
2024-12-12 18:37:19,714 - logger.py:50 - Starting model forward pass.
2024-12-12 18:37:20,644 - logger.py:50 - Starting gradient computation.
2024-12-12 18:38:58,031 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:39:05,474 - logger.py:50 - Processing step 0/4
2024-12-12 18:39:05,683 - logger.py:50 - Starting model forward pass.
2024-12-12 18:39:06,605 - logger.py:50 - Starting gradient computation.
2024-12-12 18:45:47,502 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:45:54,978 - logger.py:50 - Processing step 0/4
2024-12-12 18:45:55,189 - logger.py:50 - Starting model forward pass.
2024-12-12 18:45:55,189 - logger.py:50 - Starting gradient computation.
2024-12-12 18:48:42,296 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:48:49,588 - logger.py:50 - Processing step 0/4
2024-12-12 18:48:49,792 - logger.py:50 - Starting model forward pass.
2024-12-12 18:48:49,792 - logger.py:50 - Starting gradient computation.
2024-12-12 18:50:40,715 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:50:48,223 - logger.py:50 - Processing step 0/4
2024-12-12 18:50:48,485 - logger.py:50 - Starting model forward pass.
2024-12-12 18:50:48,486 - logger.py:50 - Starting gradient computation.
2024-12-12 18:50:50,409 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:51:33,327 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:51:40,578 - logger.py:50 - Processing step 0/4
2024-12-12 18:51:40,779 - logger.py:50 - Starting model forward pass.
2024-12-12 18:51:40,779 - logger.py:50 - Starting gradient computation.
2024-12-12 18:51:42,869 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:52:10,567 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:52:17,925 - logger.py:50 - Processing step 0/4
2024-12-12 18:52:18,128 - logger.py:50 - Starting model forward pass.
2024-12-12 18:52:18,128 - logger.py:50 - Starting gradient computation.
2024-12-12 18:52:20,028 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:53:54,918 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:54:02,503 - logger.py:50 - Processing step 0/4
2024-12-12 18:54:02,720 - logger.py:50 - Starting model forward pass.
2024-12-12 18:54:02,720 - logger.py:50 - Starting gradient computation.
2024-12-12 18:54:04,778 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:54:42,486 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:54:49,804 - logger.py:50 - Processing step 0/4
2024-12-12 18:54:50,001 - logger.py:50 - Starting model forward pass.
2024-12-12 18:54:50,002 - logger.py:50 - Starting gradient computation.
2024-12-12 18:54:51,824 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:56:05,171 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:56:12,586 - logger.py:50 - Processing step 0/4
2024-12-12 18:56:12,787 - logger.py:50 - Starting model forward pass.
2024-12-12 18:56:12,787 - logger.py:50 - Starting gradient computation.
2024-12-12 18:56:14,678 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 18:56:46,738 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 18:56:54,068 - logger.py:50 - Processing step 0/4
2024-12-12 18:56:54,268 - logger.py:50 - Starting model forward pass.
2024-12-12 18:56:54,268 - logger.py:50 - Starting gradient computation.
2024-12-12 19:00:44,791 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:00:52,394 - logger.py:50 - Processing step 0/4
2024-12-12 19:00:52,608 - logger.py:50 - Starting model forward pass.
2024-12-12 19:00:52,608 - logger.py:50 - Starting gradient computation.
2024-12-12 19:00:54,501 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:05:09,978 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:05:17,248 - logger.py:50 - Processing step 0/4
2024-12-12 19:05:17,453 - logger.py:50 - Starting model forward pass.
2024-12-12 19:05:17,453 - logger.py:50 - Starting gradient computation.
2024-12-12 19:05:19,501 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:05:22,435 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:05:22,436 - logger.py:50 - Computing loss.
2024-12-12 19:05:22,451 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 19:05:22,451 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fd7e81fafd0>, requires_grad: True
2024-12-12 19:05:22,451 - logger.py:50 - Visualizing computation graph.
2024-12-12 19:05:22,452 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 19:09:37,164 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:09:44,602 - logger.py:50 - Processing step 0/4
2024-12-12 19:09:44,803 - logger.py:50 - Starting model forward pass.
2024-12-12 19:09:44,803 - logger.py:50 - Starting gradient computation.
2024-12-12 19:09:46,708 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:09:49,440 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:09:49,441 - logger.py:50 - Computing loss.
2024-12-12 19:09:49,460 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 19:09:49,460 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda28ed6280>, requires_grad: True
2024-12-12 19:09:49,460 - logger.py:50 - Visualizing computation graph.
2024-12-12 19:09:49,461 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 19:16:16,801 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:16:24,127 - logger.py:50 - Processing step 0/4
2024-12-12 19:16:24,330 - logger.py:50 - Starting gradient computation.
2024-12-12 19:16:24,412 - logger.py:50 - Starting model forward pass.
2024-12-12 19:16:26,414 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:16:26,415 - logger.py:50 - Starting model forward pass.
2024-12-12 19:21:06,906 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:21:14,288 - logger.py:50 - Processing step 0/4
2024-12-12 19:21:14,501 - logger.py:50 - Starting gradient computation.
2024-12-12 19:21:14,550 - logger.py:50 - Starting model forward pass.
2024-12-12 19:21:16,555 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:21:16,556 - logger.py:50 - Starting model forward pass.
2024-12-12 19:21:19,510 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:21:19,511 - logger.py:50 - Computing loss.
2024-12-12 19:21:19,526 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 19:21:19,526 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f9930385f40>, requires_grad: True
2024-12-12 19:21:19,526 - logger.py:50 - Visualizing computation graph.
2024-12-12 19:21:19,527 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 19:24:30,133 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:24:37,553 - logger.py:50 - Processing step 0/4
2024-12-12 19:24:37,772 - logger.py:50 - Starting gradient computation.
2024-12-12 19:24:37,817 - logger.py:50 - Starting model forward pass.
2024-12-12 19:25:34,782 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:25:42,159 - logger.py:50 - Processing step 0/4
2024-12-12 19:25:42,374 - logger.py:50 - Starting gradient computation.
2024-12-12 19:25:42,438 - logger.py:50 - Starting model forward pass.
2024-12-12 19:26:09,321 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:26:16,666 - logger.py:50 - Processing step 0/4
2024-12-12 19:26:16,869 - logger.py:50 - Starting gradient computation.
2024-12-12 19:26:16,932 - logger.py:50 - Starting model forward pass.
2024-12-12 19:26:18,932 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:26:18,933 - logger.py:50 - Starting model forward pass.
2024-12-12 19:26:21,886 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:43:02,359 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:43:09,624 - logger.py:50 - Processing step 0/4
2024-12-12 19:43:09,824 - logger.py:50 - Starting gradient computation.
2024-12-12 19:43:09,889 - logger.py:50 - Starting model forward pass.
2024-12-12 19:43:11,922 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:43:11,923 - logger.py:50 - Starting model forward pass.
2024-12-12 19:43:14,867 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 19:54:50,946 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 19:54:58,520 - logger.py:50 - Processing step 0/4
2024-12-12 19:54:58,724 - logger.py:50 - Starting gradient computation.
2024-12-12 19:54:58,782 - logger.py:50 - Starting model forward pass.
2024-12-12 20:26:21,733 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 20:26:29,056 - logger.py:50 - Processing step 0/4
2024-12-12 20:26:29,307 - logger.py:50 - Starting model forward pass.
2024-12-12 20:26:30,213 - logger.py:50 - Starting gradient computation.
2024-12-12 20:26:31,300 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 20:26:31,301 - logger.py:50 - Starting model forward pass.
2024-12-12 20:26:32,123 - logger.py:50 - Starting gradient computation.
2024-12-12 20:26:34,273 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 20:30:42,183 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 20:30:49,449 - logger.py:50 - Processing step 0/4
2024-12-12 20:30:49,712 - logger.py:50 - Starting model forward pass.
2024-12-12 20:30:50,602 - logger.py:50 - Starting gradient computation.
2024-12-12 20:30:51,720 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 20:30:51,721 - logger.py:50 - Starting model forward pass.
2024-12-12 20:30:52,529 - logger.py:50 - Starting gradient computation.
2024-12-12 20:30:54,646 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:00:41,228 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:00:48,822 - logger.py:50 - Processing step 0/4
2024-12-12 21:00:49,111 - logger.py:50 - Starting model forward pass.
2024-12-12 21:00:50,074 - logger.py:50 - Starting gradient computation.
2024-12-12 21:04:31,816 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:04:39,143 - logger.py:50 - Processing step 0/4
2024-12-12 21:04:39,416 - logger.py:50 - Starting model forward pass.
2024-12-12 21:04:40,335 - logger.py:50 - Starting gradient computation.
2024-12-12 21:04:41,415 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:04:41,416 - logger.py:50 - Starting model forward pass.
2024-12-12 21:04:42,233 - logger.py:50 - Starting gradient computation.
2024-12-12 21:04:44,359 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:07:24,403 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:07:32,091 - logger.py:50 - Processing step 0/4
2024-12-12 21:07:32,417 - logger.py:50 - Starting model forward pass.
2024-12-12 21:07:33,371 - logger.py:50 - Starting gradient computation.
2024-12-12 21:07:34,250 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:07:34,250 - logger.py:50 - Starting model forward pass.
2024-12-12 21:07:35,119 - logger.py:50 - Starting gradient computation.
2024-12-12 21:07:37,010 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:08:23,640 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:08:31,061 - logger.py:50 - Processing step 0/4
2024-12-12 21:08:31,334 - logger.py:50 - Starting model forward pass.
2024-12-12 21:08:32,235 - logger.py:50 - Starting gradient computation.
2024-12-12 21:08:33,139 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:08:33,140 - logger.py:50 - Starting model forward pass.
2024-12-12 21:08:33,960 - logger.py:50 - Starting gradient computation.
2024-12-12 21:08:35,854 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:12:36,758 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:12:44,196 - logger.py:50 - Processing step 0/4
2024-12-12 21:12:44,452 - logger.py:50 - Starting model forward pass.
2024-12-12 21:12:45,350 - logger.py:50 - Starting gradient computation.
2024-12-12 21:14:36,233 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:14:43,506 - logger.py:50 - Processing step 0/4
2024-12-12 21:14:43,769 - logger.py:50 - Starting model forward pass.
2024-12-12 21:14:44,667 - logger.py:50 - Starting gradient computation.
2024-12-12 21:15:43,892 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:15:51,146 - logger.py:50 - Processing step 0/4
2024-12-12 21:15:51,405 - logger.py:50 - Starting model forward pass.
2024-12-12 21:15:52,308 - logger.py:50 - Starting gradient computation.
2024-12-12 21:15:53,168 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:15:53,169 - logger.py:50 - Starting model forward pass.
2024-12-12 21:15:53,989 - logger.py:50 - Starting gradient computation.
2024-12-12 21:15:55,879 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:34:32,221 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:34:39,780 - logger.py:50 - Processing step 0/4
2024-12-12 21:34:40,097 - logger.py:50 - Starting model forward pass.
2024-12-12 21:34:41,059 - logger.py:50 - Starting gradient computation.
2024-12-12 21:34:41,930 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:34:41,930 - logger.py:50 - Starting model forward pass.
2024-12-12 21:34:42,817 - logger.py:50 - Starting gradient computation.
2024-12-12 21:34:44,709 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:34:44,710 - logger.py:50 - Computing loss.
2024-12-12 21:34:44,725 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:34:44,725 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f3f0d581c10>, requires_grad: True
2024-12-12 21:34:44,725 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:34:44,726 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:34:47,099 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:34:47,101 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:34:47,101 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:34:47,101 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:34:47,101 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,101 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,102 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:34:47,102 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,102 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,102 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,102 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,103 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,104 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,105 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,105 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,105 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,105 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,105 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,106 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,107 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,107 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,107 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,107 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,107 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,108 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,108 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,108 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,108 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,108 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,109 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,110 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,111 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,112 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,112 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,112 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,112 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,112 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,113 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,113 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,113 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,113 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,113 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,114 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,115 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,116 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,117 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,117 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,117 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,117 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,117 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,118 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,118 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,118 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,118 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,118 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,119 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,119 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,119 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:34:47,119 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,119 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,120 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,121 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,122 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,123 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,123 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,123 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,123 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,123 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,124 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,124 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,124 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:34:47,124 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,124 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,125 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,126 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,127 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,127 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,127 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,127 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,127 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,128 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,129 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,129 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,129 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:34:47,129 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:34:47,129 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:34:47,130 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:34:47,131 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:34:47,132 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,132 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,132 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,132 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,132 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,133 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,133 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,133 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,133 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,133 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,134 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,134 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,134 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,134 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,134 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,135 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,136 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,136 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:34:47,136 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:34:47,272 - logger.py:50 - 参数已更新
2024-12-12 21:34:47,273 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 21:34:47,331 - logger.py:50 - Processing step 1/4
2024-12-12 21:34:47,373 - logger.py:50 - Starting model forward pass.
2024-12-12 21:34:47,999 - logger.py:50 - Starting gradient computation.
2024-12-12 21:34:50,948 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:34:50,948 - logger.py:50 - Starting model forward pass.
2024-12-12 21:34:51,537 - logger.py:50 - Starting gradient computation.
2024-12-12 21:34:54,352 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:34:54,353 - logger.py:50 - Computing loss.
2024-12-12 21:34:54,425 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 21:34:54,426 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f3fed8d2940>, requires_grad: True
2024-12-12 21:34:54,427 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:34:54,429 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:35:00,050 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:35:00,052 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:35:00,052 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:35:00,053 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:35:00,054 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,054 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,055 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:35:00,055 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,055 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,055 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,055 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,056 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,057 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,058 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,059 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,059 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,059 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,059 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,059 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,060 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,061 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,062 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,063 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,064 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,065 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,065 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,065 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,065 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,065 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,066 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,067 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,068 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,069 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,070 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,071 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,072 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,073 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,074 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,075 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,076 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,077 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,078 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:00,079 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:00,080 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,081 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,082 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,083 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,084 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,084 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,084 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:35:00,084 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:35:00,096 - logger.py:50 - 参数已更新
2024-12-12 21:35:00,145 - logger.py:50 - Processing step 2/4
2024-12-12 21:35:00,179 - logger.py:50 - Starting model forward pass.
2024-12-12 21:35:00,815 - logger.py:50 - Starting gradient computation.
2024-12-12 21:35:02,974 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:35:02,975 - logger.py:50 - Starting model forward pass.
2024-12-12 21:35:03,055 - logger.py:50 - Starting gradient computation.
2024-12-12 21:35:04,264 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:35:04,265 - logger.py:50 - Computing loss.
2024-12-12 21:35:04,363 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 21:35:04,364 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f3f0d581730>, requires_grad: True
2024-12-12 21:35:04,365 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:35:04,367 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:35:07,612 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:35:07,613 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:35:07,614 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:35:07,614 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:35:07,615 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,615 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,616 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:35:07,616 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,616 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,616 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,617 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,617 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,617 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,617 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,617 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,618 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,618 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,618 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,618 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:35:07,619 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,619 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,619 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,619 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,620 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,620 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,620 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,620 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,620 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,621 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,621 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,621 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,621 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,622 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,622 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,622 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,622 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,623 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,623 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,623 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,623 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,624 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,624 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,624 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,624 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,625 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,625 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,625 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:35:07,625 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,626 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,626 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,626 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,626 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,627 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,627 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,627 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,627 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,627 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,628 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,628 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,628 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,628 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,628 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,629 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,629 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,629 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,629 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,630 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,630 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,630 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,631 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,631 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,631 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,631 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,632 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,632 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:35:07,632 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,632 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,633 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,633 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,633 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,633 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,634 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,634 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,634 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,634 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,634 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,635 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,635 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,635 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,635 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,635 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,636 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,636 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,636 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,637 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,637 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,637 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,637 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,638 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,638 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,638 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,638 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,639 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:35:07,639 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,639 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,639 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,639 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,640 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,640 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,640 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,640 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,641 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,641 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,641 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,641 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,641 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,642 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,642 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,642 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,643 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,643 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,643 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,643 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,643 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,644 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,644 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,644 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,644 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,644 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,645 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,645 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:35:07,645 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,645 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,646 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,646 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,646 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,646 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,647 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,647 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,647 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,647 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,648 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,648 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,648 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,648 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,648 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,649 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,649 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,649 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,649 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,650 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,650 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,650 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,650 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,651 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,651 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,651 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,651 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,652 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:35:07,652 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:07,652 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,652 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,652 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,653 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:07,653 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:07,653 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:07,653 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:07,653 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:07,654 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:07,654 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:07,654 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:07,654 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:07,654 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:07,655 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,655 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,655 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,655 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,656 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,656 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,656 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,656 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,657 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,657 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,657 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,657 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,657 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,658 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,658 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,658 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:35:07,658 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:35:07,659 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,659 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,659 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,659 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,659 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,660 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:35:07,660 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:35:07,674 - logger.py:50 - 参数已更新
2024-12-12 21:35:07,712 - logger.py:50 - Processing step 3/4
2024-12-12 21:35:07,738 - logger.py:50 - Starting model forward pass.
2024-12-12 21:35:07,809 - logger.py:50 - Starting gradient computation.
2024-12-12 21:35:08,424 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:35:08,425 - logger.py:50 - Starting model forward pass.
2024-12-12 21:35:08,503 - logger.py:50 - Starting gradient computation.
2024-12-12 21:35:09,102 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:35:09,103 - logger.py:50 - Computing loss.
2024-12-12 21:35:09,189 - logger.py:50 - loss的值：0.28684836626052856
2024-12-12 21:35:09,190 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f3f3dbc4e50>, requires_grad: True
2024-12-12 21:35:09,191 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:35:09,193 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:35:11,054 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:35:11,055 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:35:11,056 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:35:11,057 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:35:11,058 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,059 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,060 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:35:11,060 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,061 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,061 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,061 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,061 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,061 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,062 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,062 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,062 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,062 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,062 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,063 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:35:11,063 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,063 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,063 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,064 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,064 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,064 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,064 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,064 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,066 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,066 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,066 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,067 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,067 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,067 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,067 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,068 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,068 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,068 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,068 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,069 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,069 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,069 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:35:11,069 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,069 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,070 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,070 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,072 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,072 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,072 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,072 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,072 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,073 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,073 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,073 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,073 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,074 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,074 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,074 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,074 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,075 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,075 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,075 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,075 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:35:11,076 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,076 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,076 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,076 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,076 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,077 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,077 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,077 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,077 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,077 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,078 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,078 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,078 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,078 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,078 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,079 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,079 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,079 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,079 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,080 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,080 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,080 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,080 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,081 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,081 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,081 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,081 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,082 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:35:11,082 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,082 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,082 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,082 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,083 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,083 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,083 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,083 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,083 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,084 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,084 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,084 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,084 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,084 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,085 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,085 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,085 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,085 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,085 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,086 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,086 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,086 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,086 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,087 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,087 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,087 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,087 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,087 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:35:11,088 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,088 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,088 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,088 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,088 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,089 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,089 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,089 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,089 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,089 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,090 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,090 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,090 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,090 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,090 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,091 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,091 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,091 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,091 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,091 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,092 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,092 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,092 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,092 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,092 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,093 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,093 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,093 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:35:11,093 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:35:11,093 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,094 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,094 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,094 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:35:11,094 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:35:11,094 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:35:11,095 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:35:11,095 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:35:11,095 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:35:11,095 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:35:11,095 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,096 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,097 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,097 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,097 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,097 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,097 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,098 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,098 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,098 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,098 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,098 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,099 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,099 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:35:11,099 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:35:11,099 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,099 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,100 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,100 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,100 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,100 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:35:11,100 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:35:11,114 - logger.py:50 - 参数已更新
2024-12-12 21:42:15,049 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:42:22,434 - logger.py:50 - Processing step 0/4
2024-12-12 21:42:22,729 - logger.py:50 - Starting model forward pass.
2024-12-12 21:42:23,647 - logger.py:50 - Starting gradient computation.
2024-12-12 21:42:24,516 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:42:24,517 - logger.py:50 - Starting model forward pass.
2024-12-12 21:42:25,335 - logger.py:50 - Starting gradient computation.
2024-12-12 21:42:27,224 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:42:27,225 - logger.py:50 - Computing loss.
2024-12-12 21:42:27,241 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:42:27,241 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f9eb07a0160>, requires_grad: True
2024-12-12 21:42:27,241 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:42:27,241 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:42:30,444 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:42:30,446 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:42:30,446 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:42:30,446 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:42:30,446 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,447 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,447 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:42:30,447 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,447 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,448 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,449 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,449 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,449 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,449 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:42:30,450 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,450 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,450 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,450 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,451 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,452 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,452 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,452 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,452 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,452 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,453 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,453 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,453 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,453 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,454 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,454 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,454 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,455 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,455 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,455 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,455 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,456 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,456 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:42:30,456 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,456 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,456 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,457 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,457 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,457 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,457 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,458 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,459 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,459 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,459 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,459 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,460 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,460 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,460 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,460 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,461 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,461 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,461 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,461 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,462 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,462 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,462 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:42:30,462 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,463 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,463 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,463 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,463 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,464 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,465 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,465 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,465 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,465 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,466 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,466 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,466 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,466 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,466 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,467 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,467 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,467 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,467 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,468 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,468 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,468 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:42:30,468 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,468 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,469 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,469 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,469 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,469 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,470 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,471 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,471 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,471 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,471 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,471 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,472 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,472 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,472 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,472 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,472 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,473 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,473 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,473 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,473 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,474 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,475 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,475 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,475 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,475 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,475 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,476 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,477 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,477 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,477 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,477 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,478 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,478 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,478 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,478 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,478 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,479 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,479 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,479 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:42:30,479 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:42:30,479 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:42:30,480 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:42:30,481 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,482 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,482 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,482 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,482 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,482 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,483 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,483 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,483 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,483 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,483 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:42:30,484 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:42:30,485 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,485 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,485 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,485 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,485 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:42:30,486 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:42:30,486 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:43:11,970 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:43:19,355 - logger.py:50 - Processing step 0/4
2024-12-12 21:43:19,627 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:20,526 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:21,616 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:21,617 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:22,419 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:24,554 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:24,555 - logger.py:50 - Computing loss.
2024-12-12 21:43:24,570 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:43:24,570 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fa7e271f670>, requires_grad: True
2024-12-12 21:43:24,570 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:43:24,571 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:43:27,544 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:43:27,544 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,545 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,546 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,547 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,548 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,549 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,550 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,551 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,552 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,553 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,554 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,555 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,556 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,557 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,558 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,559 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,560 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,561 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,562 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:27,563 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,564 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,565 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,566 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,567 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,567 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,567 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:43:27,567 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998809632:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998810512:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998806992:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998092352:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998833408:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358997392656:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358996978160:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358997392896:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358996978800:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358997392976:
2024-12-12 21:43:27,627 - logger.py:50 - Optimizer state for param 140358998907856:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358998886656:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996978560:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996978240:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996976480:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996979280:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996031024:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996031424:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996031904:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358729682432:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358998806832:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358996232288:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358998896208:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995744224:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995938096:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995717072:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995715472:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995714192:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995745344:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995746704:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995744624:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995744144:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995745184:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995451968:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358995455728:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358998907696:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358994574048:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358994700304:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358998886736:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358729682672:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358729428480:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358729428080:
2024-12-12 21:43:27,628 - logger.py:50 - Optimizer state for param 140358729038720:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729037840:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729636736:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729634896:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729634496:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726906480:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358728868624:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358728506880:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358728520864:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729634336:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729636896:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729634016:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729634656:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729635776:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729636256:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729635536:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729636416:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729635296:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729637216:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358728522064:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358728438864:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358729763472:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358727643840:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358998902400:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726907920:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726906240:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726581600:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726579600:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726305008:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358726306208:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358996802672:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358996803392:
2024-12-12 21:43:27,629 - logger.py:50 - Optimizer state for param 140358996801552:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221797232:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358996921616:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358994533008:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358996801392:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995037936:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358726908880:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995741968:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995739888:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995741888:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995739328:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358996482544:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358996480224:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358996481584:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995037776:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995937056:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358995809280:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358997392176:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358998843536:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221911360:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221795392:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221797792:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221463040:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221080272:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220953616:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220951616:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220916592:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221051280:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358221053760:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358218805392:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220852416:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220334080:
2024-12-12 21:43:27,630 - logger.py:50 - Optimizer state for param 140358220520240:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219499936:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358220133696:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358220206704:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358220206304:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219500016:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219499776:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219499136:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219499456:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219497536:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219497616:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358221844368:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358220666896:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219340112:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219340592:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358219036528:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218748048:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218806432:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218566528:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218637936:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218771088:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218772048:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217678256:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217725808:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217726448:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358216050144:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358220336080:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358218360768:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217484944:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217111008:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217631520:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217325680:
2024-12-12 21:43:27,631 - logger.py:50 - Optimizer state for param 140358217325760:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217322640:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217326480:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217325200:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217112688:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217112128:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217112208:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358217125456:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216665744:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216483120:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216483920:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215714672:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216050704:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216052304:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215294784:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215293104:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215094816:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215094416:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214556304:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214553744:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214557584:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358212803696:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358216693296:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358215755472:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214376880:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214025280:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214736608:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214534704:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214296016:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214296176:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214027920:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214026160:
2024-12-12 21:43:27,632 - logger.py:50 - Optimizer state for param 140358214027040:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358214027440:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358214026400:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213912832:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213911152:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213307664:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213306864:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213068656:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213187920:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358213209920:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212528304:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212553360:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212782576:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212783376:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212093648:
2024-12-12 21:43:27,633 - logger.py:50 - Optimizer state for param 140358212095408:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211924512:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358213306704:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358213306544:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358213305904:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358212095088:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211655856:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211757376:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211159440:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211161520:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358211562768:
2024-12-12 21:43:27,634 - logger.py:50 - Optimizer state for param 140358213572944:
2024-12-12 21:43:27,651 - logger.py:50 - 参数已更新
2024-12-12 21:43:27,652 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 21:43:27,694 - logger.py:50 - Processing step 1/4
2024-12-12 21:43:27,725 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:28,272 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:31,494 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:31,495 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:32,057 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:34,892 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:34,893 - logger.py:50 - Computing loss.
2024-12-12 21:43:34,983 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 21:43:34,984 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fa7d038c4f0>, requires_grad: True
2024-12-12 21:43:34,985 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:43:34,987 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:43:40,521 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:43:40,522 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:43:40,523 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:43:40,524 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:43:40,524 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,525 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,525 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:43:40,525 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,526 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,527 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,528 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,529 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,530 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,530 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,530 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,530 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,530 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,531 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,531 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,531 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,531 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,531 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,532 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,533 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,534 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,534 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,534 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,534 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,534 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,535 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,536 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,537 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,538 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,539 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,540 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,541 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,542 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,543 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:43:40,544 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,545 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,546 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,547 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,548 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:40,549 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:40,550 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,551 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,552 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,553 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,554 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,554 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:43:40,554 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998809632:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998810512:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998806992:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998092352:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998833408:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358997392656:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996978160:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358997392896:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996978800:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358997392976:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998907856:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358998886656:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996978560:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996978240:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996976480:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996979280:
2024-12-12 21:43:40,560 - logger.py:50 - Optimizer state for param 140358996031024:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358996031424:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358996031904:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729682432:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358998806832:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358996232288:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358998896208:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995744224:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995938096:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995717072:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995715472:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995714192:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995745344:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995746704:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995744624:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995744144:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995745184:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995451968:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358995455728:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358998907696:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358994574048:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358994700304:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358998886736:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729682672:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729428480:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729428080:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729038720:
2024-12-12 21:43:40,561 - logger.py:50 - Optimizer state for param 140358729037840:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729636736:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729634896:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729634496:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726906480:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358728868624:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358728506880:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358728520864:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729634336:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729636896:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729634016:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729634656:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729635776:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729636256:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729635536:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729636416:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729635296:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729637216:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358728522064:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358728438864:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358729763472:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358727643840:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358998902400:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726907920:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726906240:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726581600:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726579600:
2024-12-12 21:43:40,562 - logger.py:50 - Optimizer state for param 140358726305008:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358726306208:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996802672:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996803392:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996801552:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221797232:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996921616:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358994533008:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996801392:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995037936:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358726908880:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995741968:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995739888:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995741888:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995739328:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996482544:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996480224:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358996481584:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995037776:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995937056:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358995809280:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358997392176:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358998843536:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221911360:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221795392:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221797792:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221463040:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358221080272:
2024-12-12 21:43:40,563 - logger.py:50 - Optimizer state for param 140358220953616:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220951616:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220916592:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358221051280:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358221053760:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218805392:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220852416:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220334080:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220520240:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219499936:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220133696:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220206704:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220206304:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219500016:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219499776:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219499136:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219499456:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219497536:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219497616:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358221844368:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358220666896:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219340112:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219340592:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358219036528:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218748048:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218806432:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218566528:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218637936:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218771088:
2024-12-12 21:43:40,564 - logger.py:50 - Optimizer state for param 140358218772048:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217678256:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217725808:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217726448:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216050144:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358220336080:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358218360768:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217484944:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217111008:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217631520:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217325680:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217325760:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217322640:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217326480:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217325200:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217112688:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217112128:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217112208:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358217125456:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216665744:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216483120:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216483920:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358215714672:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216050704:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358216052304:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358215294784:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358215293104:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358215094816:
2024-12-12 21:43:40,565 - logger.py:50 - Optimizer state for param 140358215094416:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214556304:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214553744:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214557584:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358212803696:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358216693296:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358215755472:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214376880:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214025280:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214736608:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214534704:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214296016:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214296176:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214027920:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214026160:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214027040:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214027440:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358214026400:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213912832:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213911152:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213307664:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213306864:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213068656:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213187920:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358213209920:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358212528304:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358212553360:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358212782576:
2024-12-12 21:43:40,566 - logger.py:50 - Optimizer state for param 140358212783376:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358212093648:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358212095408:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211924512:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358213306704:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358213306544:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358213305904:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358212095088:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211655856:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211757376:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211159440:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211161520:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358211562768:
2024-12-12 21:43:40,567 - logger.py:50 - Optimizer state for param 140358213572944:
2024-12-12 21:43:40,572 - logger.py:50 - 参数已更新
2024-12-12 21:43:40,616 - logger.py:50 - Processing step 2/4
2024-12-12 21:43:40,650 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:41,248 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:43,373 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:43,374 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:43,434 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:44,629 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:44,629 - logger.py:50 - Computing loss.
2024-12-12 21:43:44,705 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 21:43:44,706 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fa7d038c4f0>, requires_grad: True
2024-12-12 21:43:44,707 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:43:44,709 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:43:47,929 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:43:47,930 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:43:47,930 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:43:47,931 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:43:47,931 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,932 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,932 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:43:47,932 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,932 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,932 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,933 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,934 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,935 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,936 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,937 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,938 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,939 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,940 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,941 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,942 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,943 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,944 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,945 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,946 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,947 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,948 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:43:47,949 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,950 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,951 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,952 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:47,953 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:47,954 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,955 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,956 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,957 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,958 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:43:47,959 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998809632:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998810512:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998806992:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998092352:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998833408:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358997392656:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996978160:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358997392896:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996978800:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358997392976:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998907856:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358998886656:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996978560:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996978240:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996976480:
2024-12-12 21:43:47,964 - logger.py:50 - Optimizer state for param 140358996979280:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358996031024:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358996031424:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358996031904:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358729682432:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358998806832:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358996232288:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358998896208:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995744224:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995938096:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995717072:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995715472:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995714192:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995745344:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995746704:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995744624:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995744144:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995745184:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995451968:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358995455728:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358998907696:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358994574048:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358994700304:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358998886736:
2024-12-12 21:43:47,965 - logger.py:50 - Optimizer state for param 140358729682672:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729428480:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729428080:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729038720:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729037840:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729636736:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729634896:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729634496:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358726906480:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358728868624:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358728506880:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358728520864:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729634336:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729636896:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729634016:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729634656:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729635776:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729636256:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729635536:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729636416:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729635296:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729637216:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358728522064:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358728438864:
2024-12-12 21:43:47,966 - logger.py:50 - Optimizer state for param 140358729763472:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358727643840:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358998902400:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726907920:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726906240:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726581600:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726579600:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726305008:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726306208:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996802672:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996803392:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996801552:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358221797232:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996921616:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358994533008:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996801392:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995037936:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358726908880:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995741968:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995739888:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995741888:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995739328:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996482544:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996480224:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358996481584:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995037776:
2024-12-12 21:43:47,967 - logger.py:50 - Optimizer state for param 140358995937056:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358995809280:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358997392176:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358998843536:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221911360:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221795392:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221797792:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221463040:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221080272:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220953616:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220951616:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220916592:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221051280:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358221053760:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358218805392:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220852416:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220334080:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220520240:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219499936:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220133696:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220206704:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358220206304:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219500016:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219499776:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219499136:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219499456:
2024-12-12 21:43:47,968 - logger.py:50 - Optimizer state for param 140358219497536:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358219497616:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358221844368:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358220666896:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358219340112:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358219340592:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358219036528:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218748048:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218806432:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218566528:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218637936:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218771088:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218772048:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217678256:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217725808:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217726448:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358216050144:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358220336080:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358218360768:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217484944:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217111008:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217631520:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217325680:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217325760:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217322640:
2024-12-12 21:43:47,969 - logger.py:50 - Optimizer state for param 140358217326480:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358217325200:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358217112688:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358217112128:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358217112208:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358217125456:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216665744:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216483120:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216483920:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215714672:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216050704:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216052304:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215294784:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215293104:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215094816:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215094416:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214556304:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214553744:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214557584:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358212803696:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358216693296:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358215755472:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214376880:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214025280:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214736608:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214534704:
2024-12-12 21:43:47,970 - logger.py:50 - Optimizer state for param 140358214296016:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214296176:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214027920:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214026160:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214027040:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214027440:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358214026400:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213912832:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213911152:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213307664:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213306864:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213068656:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213187920:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213209920:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212528304:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212553360:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212782576:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212783376:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212093648:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212095408:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358211924512:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213306704:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213306544:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358213305904:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358212095088:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358211655856:
2024-12-12 21:43:47,971 - logger.py:50 - Optimizer state for param 140358211757376:
2024-12-12 21:43:47,972 - logger.py:50 - Optimizer state for param 140358211159440:
2024-12-12 21:43:47,972 - logger.py:50 - Optimizer state for param 140358211161520:
2024-12-12 21:43:47,972 - logger.py:50 - Optimizer state for param 140358211562768:
2024-12-12 21:43:47,972 - logger.py:50 - Optimizer state for param 140358213572944:
2024-12-12 21:43:47,977 - logger.py:50 - 参数已更新
2024-12-12 21:43:48,010 - logger.py:50 - Processing step 3/4
2024-12-12 21:43:48,035 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:48,087 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:48,696 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:48,696 - logger.py:50 - Starting model forward pass.
2024-12-12 21:43:48,758 - logger.py:50 - Starting gradient computation.
2024-12-12 21:43:49,343 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:43:49,344 - logger.py:50 - Computing loss.
2024-12-12 21:43:49,432 - logger.py:50 - loss的值：0.28684836626052856
2024-12-12 21:43:49,433 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fa7d03b29d0>, requires_grad: True
2024-12-12 21:43:49,434 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:43:49,435 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:43:51,172 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:43:51,173 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:43:51,174 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:43:51,175 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:43:51,175 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,175 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,176 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:43:51,177 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,177 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,178 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,179 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,180 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,181 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,182 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,183 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,184 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,185 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,186 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,187 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,188 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,189 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,190 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,191 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,192 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,193 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,194 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,195 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,196 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,197 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:43:51,198 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:43:51,199 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,200 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,201 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,202 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,203 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,203 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:43:51,203 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358998809632:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358998810512:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358998806992:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358998092352:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358998833408:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358997392656:
2024-12-12 21:43:51,208 - logger.py:50 - Optimizer state for param 140358996978160:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358997392896:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996978800:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358997392976:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358998907856:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358998886656:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996978560:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996978240:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996976480:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996979280:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996031024:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996031424:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996031904:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358729682432:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358998806832:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358996232288:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358998896208:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995744224:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995938096:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995717072:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995715472:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995714192:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995745344:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995746704:
2024-12-12 21:43:51,209 - logger.py:50 - Optimizer state for param 140358995744624:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358995744144:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358995745184:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358995451968:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358995455728:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358998907696:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358994574048:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358994700304:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358998886736:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729682672:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729428480:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729428080:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729038720:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729037840:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729636736:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729634896:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729634496:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358726906480:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358728868624:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358728506880:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358728520864:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729634336:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729636896:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729634016:
2024-12-12 21:43:51,210 - logger.py:50 - Optimizer state for param 140358729634656:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729635776:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729636256:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729635536:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729636416:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729635296:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729637216:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358728522064:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358728438864:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358729763472:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358727643840:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358998902400:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726907920:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726906240:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726581600:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726579600:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726305008:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358726306208:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358996802672:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358996803392:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358996801552:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358221797232:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358996921616:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358994533008:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358996801392:
2024-12-12 21:43:51,211 - logger.py:50 - Optimizer state for param 140358995037936:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358726908880:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995741968:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995739888:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995741888:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995739328:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358996482544:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358996480224:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358996481584:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995037776:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995937056:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358995809280:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358997392176:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358998843536:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221911360:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221795392:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221797792:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221463040:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221080272:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358220953616:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358220951616:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358220916592:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221051280:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358221053760:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358218805392:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358220852416:
2024-12-12 21:43:51,212 - logger.py:50 - Optimizer state for param 140358220334080:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358220520240:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219499936:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358220133696:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358220206704:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358220206304:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219500016:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219499776:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219499136:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219499456:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219497536:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219497616:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358221844368:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358220666896:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219340112:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219340592:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358219036528:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218748048:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218806432:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218566528:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218637936:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218771088:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358218772048:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358217678256:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358217725808:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358217726448:
2024-12-12 21:43:51,213 - logger.py:50 - Optimizer state for param 140358216050144:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358220336080:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358218360768:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217484944:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217111008:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217631520:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217325680:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217325760:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217322640:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217326480:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217325200:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217112688:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217112128:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217112208:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358217125456:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358216665744:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358216483120:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358216483920:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358215714672:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358216050704:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358216052304:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358215294784:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358215293104:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358215094816:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358215094416:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358214556304:
2024-12-12 21:43:51,214 - logger.py:50 - Optimizer state for param 140358214553744:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214557584:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358212803696:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358216693296:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358215755472:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214376880:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214025280:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214736608:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214534704:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214296016:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214296176:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214027920:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214026160:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214027040:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214027440:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358214026400:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213912832:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213911152:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213307664:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213306864:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213068656:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213187920:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358213209920:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358212528304:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358212553360:
2024-12-12 21:43:51,215 - logger.py:50 - Optimizer state for param 140358212782576:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358212783376:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358212093648:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358212095408:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211924512:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358213306704:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358213306544:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358213305904:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358212095088:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211655856:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211757376:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211159440:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211161520:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358211562768:
2024-12-12 21:43:51,216 - logger.py:50 - Optimizer state for param 140358213572944:
2024-12-12 21:43:51,221 - logger.py:50 - 参数已更新
2024-12-12 21:44:51,396 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:44:58,901 - logger.py:50 - Processing step 0/4
2024-12-12 21:44:59,151 - logger.py:50 - Starting model forward pass.
2024-12-12 21:45:00,058 - logger.py:50 - Starting gradient computation.
2024-12-12 21:45:01,092 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:45:01,093 - logger.py:50 - Starting model forward pass.
2024-12-12 21:45:01,940 - logger.py:50 - Starting gradient computation.
2024-12-12 21:45:03,850 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:45:03,851 - logger.py:50 - Computing loss.
2024-12-12 21:45:03,867 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:45:03,867 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fb6e9ec9790>, requires_grad: True
2024-12-12 21:45:03,867 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:45:03,868 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:45:06,222 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:45:06,223 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:45:06,224 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:45:06,224 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:45:06,224 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,224 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,224 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,225 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,226 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,226 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,226 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,226 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,226 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,227 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:45:06,227 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,227 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,227 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,227 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,228 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,228 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,228 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,228 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,228 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,229 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,230 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,230 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,230 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,230 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,231 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,231 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,231 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,231 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,232 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,232 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,232 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,232 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,233 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:45:06,233 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,233 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,233 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,233 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,234 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,235 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,235 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,235 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,235 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,235 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,236 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,236 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,236 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,236 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,237 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,237 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,237 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,237 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,237 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,238 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,238 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,238 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,238 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:45:06,239 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,239 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,239 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,239 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,239 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,240 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,241 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,241 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,241 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,241 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,241 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,242 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,242 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,242 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,242 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,242 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,243 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,243 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,243 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,243 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,244 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,244 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,244 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:45:06,244 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,244 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,245 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,245 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,245 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,245 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,245 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,246 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,247 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,247 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,247 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,247 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,248 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,248 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,248 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,248 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,248 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,249 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,249 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,249 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,249 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,250 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,250 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:45:06,250 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,250 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,250 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,251 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,251 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,251 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,251 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,251 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,252 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,253 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,253 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,253 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,253 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,253 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,254 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,254 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,254 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,254 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,255 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,255 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,255 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,255 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,255 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:45:06,256 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:45:06,256 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,256 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,256 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:45:06,257 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:45:06,258 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:45:06,258 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:45:06,258 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:45:06,258 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,258 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,259 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,259 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,259 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,259 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,259 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,260 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,260 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,260 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,260 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,260 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:45:06,261 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,262 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,262 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,262 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,262 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,262 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:45:06,263 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140424428515872:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963177984:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963179024:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963178304:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963966976:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140424358794000:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963869968:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423962497648:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963954128:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963932768:
2024-12-12 21:45:06,346 - logger.py:50 - Optimizer state for param 140423963966496:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423963933568:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140424358794160:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423963941360:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423963939360:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423963949312:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423957319488:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423957318608:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423957318368:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423585176048:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140424358796960:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423956999840:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423963965136:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955872240:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423956832960:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423956694336:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955872480:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955870720:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955870880:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955870640:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955870320:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955870800:
2024-12-12 21:45:06,347 - logger.py:50 - Optimizer state for param 140423955872320:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423956996640:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423956996400:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423956983696:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423955965728:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585774704:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585176608:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585175248:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423586187760:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423586188720:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584661232:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584658832:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585177488:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585175168:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423586194112:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423582911680:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585047712:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584545584:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584546704:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584074704:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584118560:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423585891968:
2024-12-12 21:45:06,348 - logger.py:50 - Optimizer state for param 140423584075664:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584075504:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584072064:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584073184:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584071904:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584074784:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584075424:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584632720:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584630640:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423583732576:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423583731776:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423583805008:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582909520:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582911280:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582635488:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582636208:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582535328:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423582534848:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423583725024:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423583066848:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423584310672:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423581441936:
2024-12-12 21:45:06,349 - logger.py:50 - Optimizer state for param 140423957053008:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423957052928:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423956495552:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586282528:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586003520:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586004480:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586280688:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586282848:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586281888:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586280288:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586281728:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586280528:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423586283408:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423957065536:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423957063296:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423962891664:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423962890784:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423581253936:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423581438656:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423581439776:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423580801040:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423580800240:
2024-12-12 21:45:06,350 - logger.py:50 - Optimizer state for param 140423580641856:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580642336:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580938128:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580935888:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580937248:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423578421184:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580790112:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580790032:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580113552:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579613920:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580114272:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580111872:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579612160:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579611760:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579614960:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579611280:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579614480:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579615120:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579611920:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579467424:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423580661456:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423578980576:
2024-12-12 21:45:06,351 - logger.py:50 - Optimizer state for param 140423579374752:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578730144:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578419904:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578420224:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578186752:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578187952:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577638528:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577638208:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577313744:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577317264:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577316624:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576052736:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578522544:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423578523824:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577239120:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576707936:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423577237280:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576710736:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576710336:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576707776:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576708336:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576709936:
2024-12-12 21:45:06,352 - logger.py:50 - Optimizer state for param 140423576709216:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576710256:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576707376:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576766560:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576767200:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576075136:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576075856:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575461456:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576055376:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576054096:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575549488:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575548128:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575272160:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575271840:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423574154672:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423574151392:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423574155072:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423572404400:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423575591088:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423576055536:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423574327024:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423573755360:
2024-12-12 21:45:06,353 - logger.py:50 - Optimizer state for param 140423573963696:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573966176:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573756720:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573756640:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573756960:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573754080:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573756240:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573754960:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573757200:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573607904:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573607584:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572898896:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572899776:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572547280:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572404160:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572403760:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572192448:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572191328:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572487120:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423572485200:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423571687600:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423571689280:
2024-12-12 21:45:06,354 - logger.py:50 - Optimizer state for param 140423573271312:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423573272192:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423573274112:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423572461264:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423572405760:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423572404960:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423571065488:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423570868320:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423570869520:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423571149984:
2024-12-12 21:45:06,355 - logger.py:50 - Optimizer state for param 140423572481024:
2024-12-12 21:46:30,274 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:46:37,668 - logger.py:50 - Processing step 0/4
2024-12-12 21:46:37,947 - logger.py:50 - Starting model forward pass.
2024-12-12 21:46:38,842 - logger.py:50 - Starting gradient computation.
2024-12-12 21:46:39,935 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:46:39,936 - logger.py:50 - Starting model forward pass.
2024-12-12 21:46:40,741 - logger.py:50 - Starting gradient computation.
2024-12-12 21:46:42,881 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:46:42,882 - logger.py:50 - Computing loss.
2024-12-12 21:46:42,897 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:46:42,897 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f822aa7daf0>, requires_grad: True
2024-12-12 21:46:42,897 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:46:42,898 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:46:45,872 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:46:45,873 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:46:45,874 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:46:45,874 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:46:45,874 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,875 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,875 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:46:45,875 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,875 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,875 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,876 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,876 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,876 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,876 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,876 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,877 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,877 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,877 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,877 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:46:45,877 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,878 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,878 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,878 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,878 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,878 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,879 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,880 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,880 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,880 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,880 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,881 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,881 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,881 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,881 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,881 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,882 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,882 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,882 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,882 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,883 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,883 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,883 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:46:45,883 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,883 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,884 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,884 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,884 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,884 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,885 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,886 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,886 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,886 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,886 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,887 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,887 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,887 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,887 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,887 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,888 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,888 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,888 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,888 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,889 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,889 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,889 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:46:45,889 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,889 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,890 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,890 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,890 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,890 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,890 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,891 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,892 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,892 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,892 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,892 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,893 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,893 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,893 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,893 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,893 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,894 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,894 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,894 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,894 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,895 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,895 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:46:45,895 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,895 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,896 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,896 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,896 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,896 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,896 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,897 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,898 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,898 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,898 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,898 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,899 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,899 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,899 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,899 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,899 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,900 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,900 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,900 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,900 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,900 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,901 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:46:45,901 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,901 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,901 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,901 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,902 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,902 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,902 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,902 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,902 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,903 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,904 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,904 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,904 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,904 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,904 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,905 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,905 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,905 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,905 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,905 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,906 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,906 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,906 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:46:45,906 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:46:45,906 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,907 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,907 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,907 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:46:45,907 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:46:45,907 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:46:45,908 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:46:45,909 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:46:45,909 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,909 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,909 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,909 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,910 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,910 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,910 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,910 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,910 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,911 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,911 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,911 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,911 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,911 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,912 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,913 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,913 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:46:45,913 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:46:45,913 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:46:46,002 - logger.py:50 - Optimizer state for param 140191356588416:
2024-12-12 21:46:46,002 - logger.py:50 - Optimizer state for param 140191356590656:
2024-12-12 21:46:46,002 - logger.py:50 - Optimizer state for param 140191356634192:
2024-12-12 21:46:46,002 - logger.py:50 - Optimizer state for param 140191356885328:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190959732992:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190959734432:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958773648:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190959014752:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958775248:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958773488:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958776048:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958774848:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958774688:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958772688:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958775328:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958772368:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140191356644784:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190953992576:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190958774928:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190623933120:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140191356634592:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140191356885648:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190625986176:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190625307120:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190626131296:
2024-12-12 21:46:46,003 - logger.py:50 - Optimizer state for param 140190625412736:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625414496:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625305360:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625306080:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625307040:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625305520:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625305840:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625307200:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625009472:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625006192:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625081440:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190625079600:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190624601008:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190624810064:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623935600:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623797136:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623795856:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623441280:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623444720:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623465200:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623464800:
2024-12-12 21:46:46,004 - logger.py:50 - Optimizer state for param 140190623463680:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190621247344:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190624875840:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190624876480:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622560144:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622446432:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622921728:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622539584:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622537584:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622538464:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622539184:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622536864:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622537184:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622448352:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622446512:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140191356649360:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190623564480:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622198256:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622197456:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190621697264:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622196576:
2024-12-12 21:46:46,005 - logger.py:50 - Optimizer state for param 140190622196336:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190621145984:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190621144304:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620780400:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620452960:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620254096:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620252736:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620252576:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620654064:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620204704:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190620201664:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190619960400:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626103440:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626104080:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626106640:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626105360:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626105280:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626105600:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626104800:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626103360:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626105520:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190626103680:
2024-12-12 21:46:46,006 - logger.py:50 - Optimizer state for param 140190624812944:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190624809104:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190624109488:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190621650352:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190625987136:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190622492528:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190622490768:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190620870992:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190619732144:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190619311216:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190619310656:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190618407456:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190618408016:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190618410816:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190313969840:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190618627744:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190316150592:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190315827088:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190314943168:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190315937440:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190315539648:
2024-12-12 21:46:46,007 - logger.py:50 - Optimizer state for param 140190315538528:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315540048:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315537408:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315538128:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315539728:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190314944448:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190314945328:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315364000:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190315065152:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190314431008:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190314430528:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190314243552:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313970080:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313968000:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313688656:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313687536:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313466192:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190313142448:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190312927936:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190312928816:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190312927856:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190311537456:
2024-12-12 21:46:46,008 - logger.py:50 - Optimizer state for param 140190312929296:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190314867424:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312736624:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190313607632:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312588368:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312484752:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312481392:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312483952:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312484192:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312481872:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312483712:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312482112:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190313610032:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312269200:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190312270800:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311594400:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311594000:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311066496:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311593200:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311538256:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311096704:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190311099104:
2024-12-12 21:46:46,009 - logger.py:50 - Optimizer state for param 140190310710864:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190310288736:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190310182160:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190310179760:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190310182720:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190307899008:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190311640416:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190309688320:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190309842352:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308915696:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190309464320:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308913456:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308915056:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308913936:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308916176:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308915216:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308914096:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308917136:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308917056:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190309633936:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190309004688:
2024-12-12 21:46:46,010 - logger.py:50 - Optimizer state for param 140190308553168:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308410448:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308335920:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307899648:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307897568:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307635264:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307637584:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307096432:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307094832:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307187904:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307187984:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308888144:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308885104:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308886384:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190308885744:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307995136:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190307992336:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190306568208:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190306285584:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190306286224:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190306937008:
2024-12-12 21:46:46,011 - logger.py:50 - Optimizer state for param 140190306938208:
2024-12-12 21:47:38,349 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:47:45,823 - logger.py:50 - Processing step 0/4
2024-12-12 21:47:46,081 - logger.py:50 - Starting model forward pass.
2024-12-12 21:47:46,988 - logger.py:50 - Starting gradient computation.
2024-12-12 21:47:48,081 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:47:48,082 - logger.py:50 - Starting model forward pass.
2024-12-12 21:47:48,890 - logger.py:50 - Starting gradient computation.
2024-12-12 21:47:51,045 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:47:51,046 - logger.py:50 - Computing loss.
2024-12-12 21:47:51,069 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:47:51,069 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f8033f1f0>, requires_grad: True
2024-12-12 21:47:51,069 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:47:51,071 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:47:54,101 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:47:54,102 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:47:54,102 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:47:54,103 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:47:54,103 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,103 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,103 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:47:54,103 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,104 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,104 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,104 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,104 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,104 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,105 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,105 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,105 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,105 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,105 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,106 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:47:54,106 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,106 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,106 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,106 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,107 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,107 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,107 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,107 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,107 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,108 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,109 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,109 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,109 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,109 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,110 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,110 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,110 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,110 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,111 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,111 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,111 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,111 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,112 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:47:54,112 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,112 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,112 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,112 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,113 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,113 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,113 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,113 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,113 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,114 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,115 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,115 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,115 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,115 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,116 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,116 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,116 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,116 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,117 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,117 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,117 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,117 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,117 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:47:54,118 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,118 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,118 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,118 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,119 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,120 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,120 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,120 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,120 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,120 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,121 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,121 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,121 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,121 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,122 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,122 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,122 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,122 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,122 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,123 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,123 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,123 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,123 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:47:54,124 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,124 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,124 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,124 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,124 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,125 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,126 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,126 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,126 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,126 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,126 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,127 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,127 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,127 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,127 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,128 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,128 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,128 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,128 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,128 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,129 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,129 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,129 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:47:54,129 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,129 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,130 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,131 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,132 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,132 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,132 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,132 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,132 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,133 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,134 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,134 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,134 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:47:54,134 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:47:54,134 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,135 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,135 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,135 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:47:54,135 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:47:54,135 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:47:54,136 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:47:54,137 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,137 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,137 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,137 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,137 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,138 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,138 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,138 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,138 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,139 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,139 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,139 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,139 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,139 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,140 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,141 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,141 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,141 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:47:54,141 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:47:54,229 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:47:54,230 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:47:54,231 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:47:54,232 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:47:54,233 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:47:54,234 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:47:54,235 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:47:54,236 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:47:54,237 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:47:54,238 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:47:54,239 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:47:54,265 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:47:54,267 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 21:47:54,329 - logger.py:50 - Processing step 1/4
2024-12-12 21:47:54,376 - logger.py:50 - Starting model forward pass.
2024-12-12 21:47:54,979 - logger.py:50 - Starting gradient computation.
2024-12-12 21:47:58,193 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:47:58,194 - logger.py:50 - Starting model forward pass.
2024-12-12 21:47:58,760 - logger.py:50 - Starting gradient computation.
2024-12-12 21:48:01,871 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:48:01,872 - logger.py:50 - Computing loss.
2024-12-12 21:48:01,968 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 21:48:01,969 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f941d7ee0>, requires_grad: True
2024-12-12 21:48:01,970 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:48:01,972 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:48:08,344 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:48:08,345 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:48:08,346 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:48:08,346 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:48:08,347 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,347 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,348 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:48:08,348 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,349 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,350 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,352 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,353 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,353 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,353 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,353 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,353 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,354 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,355 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,357 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,357 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,357 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,357 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,357 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,358 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,359 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,361 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,362 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:48:08,363 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,365 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,366 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,367 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,367 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,367 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,367 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,367 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,368 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,370 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,371 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:08,372 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:08,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,374 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,375 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,376 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,376 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,376 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,376 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,376 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,377 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,378 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,378 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,378 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:48:08,378 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:48:08,383 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:48:08,384 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:48:08,385 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:48:08,386 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:48:08,387 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:48:08,388 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:48:08,389 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:48:08,390 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:48:08,395 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:48:08,443 - logger.py:50 - Processing step 2/4
2024-12-12 21:48:08,481 - logger.py:50 - Starting model forward pass.
2024-12-12 21:48:09,087 - logger.py:50 - Starting gradient computation.
2024-12-12 21:48:11,497 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:48:11,498 - logger.py:50 - Starting model forward pass.
2024-12-12 21:48:11,559 - logger.py:50 - Starting gradient computation.
2024-12-12 21:48:13,015 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:48:13,016 - logger.py:50 - Computing loss.
2024-12-12 21:48:13,096 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 21:48:13,096 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f80281820>, requires_grad: True
2024-12-12 21:48:13,097 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:48:13,099 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:48:17,042 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:48:17,043 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:48:17,044 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:48:17,044 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:48:17,045 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,045 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,045 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:48:17,045 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,046 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,047 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,048 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,049 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,050 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,051 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,052 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,053 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,054 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,055 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,056 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,057 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,058 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,059 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,060 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,061 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,062 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,063 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,064 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,065 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,066 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,067 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:17,068 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,069 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,070 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:48:17,071 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:48:17,072 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:48:17,077 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:48:17,077 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:48:17,077 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:48:17,078 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:48:17,079 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:48:17,080 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:48:17,081 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:48:17,082 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:48:17,083 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:48:17,084 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:48:17,085 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:48:17,090 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:48:17,126 - logger.py:50 - Processing step 3/4
2024-12-12 21:48:17,155 - logger.py:50 - Starting model forward pass.
2024-12-12 21:48:17,208 - logger.py:50 - Starting gradient computation.
2024-12-12 21:48:18,075 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:48:18,076 - logger.py:50 - Starting model forward pass.
2024-12-12 21:48:18,141 - logger.py:50 - Starting gradient computation.
2024-12-12 21:48:18,970 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:48:18,970 - logger.py:50 - Computing loss.
2024-12-12 21:48:19,077 - logger.py:50 - loss的值：0.28684836626052856
2024-12-12 21:48:19,078 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f8033f220>, requires_grad: True
2024-12-12 21:48:19,078 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:48:19,080 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:48:21,548 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:48:21,549 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:48:21,550 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:48:21,551 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:48:21,551 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,552 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,552 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:48:21,553 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,553 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,553 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,554 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,555 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,556 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,557 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,558 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,559 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,560 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,561 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,562 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,563 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,564 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,565 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:48:21,566 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,567 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,568 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,569 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,569 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,569 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,569 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,569 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,570 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,571 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,572 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,573 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,574 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,574 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,574 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,574 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,574 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,575 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:48:21,576 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,577 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,578 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,579 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,580 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:48:21,581 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:48:21,586 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:48:21,587 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:48:21,588 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:48:21,589 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:48:21,590 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:48:21,591 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:48:21,592 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:48:21,593 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:48:21,598 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:49:13,024 - logger.py:50 - Step [0/1], Loss: 1.3727, MAE: 0.3626
2024-12-12 21:50:24,473 - logger.py:50 - Step [0/1], Loss: 1.3524, MAE: 0.2774
2024-12-12 21:50:24,929 - logger.py:50 - Epoch: [0] train loss: 1.05316, train MAE: 0.17617,val loss: 1.37274, val MAE: 0.36264,test loss: 1.35236, test MAE: 0.27735,Time: 159.16s
2024-12-12 21:50:24,929 - logger.py:50 - Best -- epoch=0, train loss: 1.05316, val loss: 1.37274, test loss: 1.35236

2024-12-12 21:50:24,993 - logger.py:50 - Processing step 0/4
2024-12-12 21:50:25,031 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:25,108 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:25,978 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:25,979 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:26,043 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:26,883 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:26,884 - logger.py:50 - Computing loss.
2024-12-12 21:50:26,885 - logger.py:50 - loss的值：0.44527027010917664
2024-12-12 21:50:26,885 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f64e35cd0>, requires_grad: True
2024-12-12 21:50:26,885 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:50:26,888 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:50:29,239 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:50:29,240 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:50:29,240 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:50:29,240 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:50:29,241 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,241 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,241 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:50:29,241 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,242 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,243 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,243 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,243 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,243 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,243 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:50:29,244 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,244 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,244 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,244 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,244 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,245 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,246 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,246 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,246 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,246 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,246 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,247 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,247 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,247 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,247 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,247 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,248 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,248 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,248 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,248 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,248 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,249 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,250 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,250 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,250 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,250 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,250 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,251 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,252 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,252 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,252 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,252 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,252 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,253 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,253 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,253 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,253 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,253 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,254 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,254 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,254 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:50:29,254 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,254 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,255 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,256 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,257 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,257 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,257 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,257 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,258 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,258 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,258 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,258 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,258 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,259 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,259 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,259 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,259 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,259 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,260 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,261 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,262 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,262 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,262 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,262 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,262 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,263 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,263 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,263 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,263 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,263 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,264 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,264 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,264 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,264 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,264 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,265 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,266 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,267 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,267 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,267 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,267 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,267 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,268 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,269 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,269 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,269 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,269 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,269 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,270 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:29,271 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,272 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,273 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,274 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,274 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,274 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,274 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,274 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,275 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,276 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,276 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,276 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:50:29,276 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:50:29,283 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:50:29,284 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:50:29,285 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:50:29,286 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:50:29,287 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:50:29,288 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:50:29,289 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:50:29,290 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:50:29,291 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:50:29,292 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:50:29,299 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:50:29,300 - logger.py:50 - Epoch [1], Step [0/4], Loss: 0.4453, MAE: 0.2040
2024-12-12 21:50:29,366 - logger.py:50 - Processing step 1/4
2024-12-12 21:50:29,422 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:29,495 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:30,354 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:30,355 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:30,419 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:31,280 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:31,281 - logger.py:50 - Computing loss.
2024-12-12 21:50:31,384 - logger.py:50 - loss的值：2.1890227794647217
2024-12-12 21:50:31,385 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f65287670>, requires_grad: True
2024-12-12 21:50:31,385 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:50:31,387 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:50:33,761 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:50:33,762 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:50:33,763 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:50:33,763 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:50:33,764 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,765 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,765 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,766 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,767 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,768 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,769 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,770 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,771 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,772 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,773 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,774 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:50:33,775 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,776 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,777 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,778 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:50:33,779 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,780 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,781 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,782 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:50:33,783 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,784 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,785 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,786 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:50:33,787 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:33,788 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,789 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,790 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,791 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,792 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:50:33,793 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:50:33,798 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:50:33,799 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:50:33,800 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:50:33,801 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:50:33,802 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:50:33,803 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:50:33,804 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:50:33,805 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:50:33,811 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:50:33,853 - logger.py:50 - Processing step 2/4
2024-12-12 21:50:34,052 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:34,105 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:34,959 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:34,959 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:35,021 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:35,858 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:35,858 - logger.py:50 - Computing loss.
2024-12-12 21:50:35,944 - logger.py:50 - loss的值：0.7932065725326538
2024-12-12 21:50:35,944 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f80281b20>, requires_grad: True
2024-12-12 21:50:35,945 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:50:35,946 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:50:38,322 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:50:38,323 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:50:38,324 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:50:38,325 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:50:38,325 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,325 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,326 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,327 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,328 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,329 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,330 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,331 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,332 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,333 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,334 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,335 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,336 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,337 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,338 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,339 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,340 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,341 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,342 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,343 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,344 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,345 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,346 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,347 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:38,348 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:38,349 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,350 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,351 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,352 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,353 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,354 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:50:38,354 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:50:38,359 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:50:38,359 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:50:38,360 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:50:38,361 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:50:38,362 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:50:38,363 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:50:38,364 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:50:38,365 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:50:38,366 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:50:38,367 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:50:38,373 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:50:38,414 - logger.py:50 - Processing step 3/4
2024-12-12 21:50:38,448 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:38,503 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:39,362 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:39,362 - logger.py:50 - Starting model forward pass.
2024-12-12 21:50:39,424 - logger.py:50 - Starting gradient computation.
2024-12-12 21:50:40,250 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:50:40,251 - logger.py:50 - Computing loss.
2024-12-12 21:50:40,366 - logger.py:50 - loss的值：0.12031616270542145
2024-12-12 21:50:40,367 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f7f800b77f0>, requires_grad: True
2024-12-12 21:50:40,368 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:50:40,370 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:50:42,764 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:50:42,765 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:50:42,766 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:50:42,767 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:50:42,768 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,768 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,769 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:50:42,770 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,770 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,770 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,770 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,771 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,772 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,773 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,774 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:50:42,775 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,777 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,778 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,779 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,780 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,781 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,782 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,783 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,784 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,785 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,786 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,787 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,788 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,789 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,790 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:50:42,791 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:50:42,792 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,793 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,794 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,795 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,796 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:50:42,796 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924979104:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185986401440:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924173472:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924173152:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924985616:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185923509680:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922885888:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185923509200:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185923507280:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924939584:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924951792:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924896064:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922884448:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922884608:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922886528:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922887488:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924960144:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922897536:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185922899136:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924939104:
2024-12-12 21:50:42,801 - logger.py:50 - Optimizer state for param 140185924896704:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185922698272:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185924985376:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921407392:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921614208:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921408752:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921407152:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921406992:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921408672:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921407952:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921405712:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921407472:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921406912:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921872416:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185921274240:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185920883504:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588421072:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185924895904:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588538640:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588429984:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588431424:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588431744:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185587846816:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185587442448:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588028816:
2024-12-12 21:50:42,802 - logger.py:50 - Optimizer state for param 140185588028256:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185924906096:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586183200:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185587197968:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185587268320:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185587268080:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586828048:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185587268880:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586770064:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586772544:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586829968:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586827648:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586831088:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586829408:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586830848:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586831248:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586413456:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586409696:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586113168:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586113968:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185587442688:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586183680:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185586184000:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185140010416:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185140008976:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185140225104:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185140226944:
2024-12-12 21:50:42,803 - logger.py:50 - Optimizer state for param 140185585993024:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185587964800:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185952119440:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185139092592:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185921696528:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185921346592:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185588474560:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920892736:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185140234336:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920889056:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920891856:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920890656:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920889136:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920889536:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920890256:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920892816:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185920892176:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185923122496:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185923121216:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185924965856:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185923938720:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185139005856:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185139090672:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185139090032:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138935744:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138922016:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138262480:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138392176:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138218384:
2024-12-12 21:50:42,804 - logger.py:50 - Optimizer state for param 140185138219904:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185138219664:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136199696:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137680192:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137676592:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137464720:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137361280:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137871504:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137653136:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137364000:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137362000:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137363680:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137363520:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137361440:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137363760:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185137362480:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185138781856:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185138109952:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136675792:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136675232:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136088384:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136199376:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185136198416:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135547632:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135549232:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135386512:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135384832:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135053296:
2024-12-12 21:50:42,805 - logger.py:50 - Optimizer state for param 140185135052816:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185135050816:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133183680:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185135188784:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185136266192:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134918928:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185135297216:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134833968:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134732128:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134734368:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134734448:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134732848:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134734208:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134733488:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134732368:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185135299616:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134119328:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185134118288:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133823136:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133825936:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133057664:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133852128:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133183440:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133155088:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185133157808:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185132934784:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185132518032:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185131900096:
2024-12-12 21:50:42,806 - logger.py:50 - Optimizer state for param 140185131897216:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131900656:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130145888:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185132910928:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131935280:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185132084016:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131162576:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131716992:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131794080:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131792320:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131794000:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131162176:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131162256:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131161616:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131161536:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131160576:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130839792:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130836992:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130636128:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130636048:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185131258240:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130144128:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185130144368:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129905504:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129904304:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129334960:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129334800:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129433504:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129434864:
2024-12-12 21:50:42,807 - logger.py:50 - Optimizer state for param 140185129278336:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129277856:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129278896:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129276336:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129490768:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129492048:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185128901024:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185128615264:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185128615184:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129291424:
2024-12-12 21:50:42,808 - logger.py:50 - Optimizer state for param 140185129289424:
2024-12-12 21:50:42,813 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:51:50,349 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 21:51:57,704 - logger.py:50 - Processing step 0/4
2024-12-12 21:51:57,959 - logger.py:50 - Starting model forward pass.
2024-12-12 21:51:58,862 - logger.py:50 - Starting gradient computation.
2024-12-12 21:51:59,741 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:51:59,742 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:00,551 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:02,424 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:02,424 - logger.py:50 - Computing loss.
2024-12-12 21:52:02,439 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 21:52:02,439 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d5003ca60>, requires_grad: True
2024-12-12 21:52:02,439 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:52:02,440 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:52:04,763 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:52:04,765 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:52:04,765 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:52:04,765 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:52:04,766 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,766 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,766 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,767 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,768 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,768 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,768 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,768 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,768 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,769 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:52:04,769 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,769 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,769 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,770 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,770 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,770 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,770 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,770 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,771 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,771 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,771 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,771 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,771 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,772 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,772 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,772 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,772 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,773 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,773 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,773 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,773 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,774 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,774 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,774 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,774 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,775 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,775 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,775 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:52:04,775 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,775 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,776 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,776 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,776 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,777 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,778 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,778 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,778 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,778 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,778 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,779 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,779 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,779 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,779 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,780 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,780 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,780 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,780 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,781 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,781 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,781 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,781 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:52:04,781 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,782 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,782 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,782 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,782 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,783 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,783 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,783 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,783 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,783 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,784 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,784 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,784 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,784 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,784 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,785 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,785 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,785 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,785 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,786 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,786 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,786 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,786 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,787 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,787 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,787 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,787 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,788 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:52:04,788 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,788 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,788 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,788 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,789 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,789 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,789 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,789 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,790 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,790 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,790 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,790 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,790 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,791 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,791 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,791 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,791 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,791 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,792 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,792 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,792 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,792 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,793 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,793 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,793 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,793 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,794 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,794 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:52:04,794 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,794 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,794 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,795 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,796 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,797 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,797 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,797 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,797 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,798 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,798 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,798 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,798 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,798 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,799 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,799 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,799 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,799 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:52:04,799 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:04,800 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:04,801 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:04,802 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:04,802 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,802 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,802 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,802 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,803 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,803 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,803 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,803 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,803 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,804 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,805 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:52:04,805 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:52:04,805 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,805 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,805 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,806 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,806 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,806 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:52:04,806 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107672289008:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107478320608:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107478322528:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107478322768:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107478343568:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107478339728:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107672264768:
2024-12-12 21:52:04,896 - logger.py:50 - Optimizer state for param 140107672268208:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477834144:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107672268448:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477833344:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477834784:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477833984:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477834064:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477833904:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477834864:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107672259456:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107472584848:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107472585728:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471489440:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107672203808:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107477910368:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107672204128:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471656736:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107472530304:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471655856:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471655936:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471655456:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471655216:
2024-12-12 21:52:04,897 - logger.py:50 - Optimizer state for param 140107471655696:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471655776:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471657216:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471657376:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471658496:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471658896:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471042656:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471042736:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107470997200:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471076064:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471836400:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471085792:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140106899420320:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107672286448:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107470612656:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107470526640:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140106898613248:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140106898613088:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140106897393440:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471769168:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140107471769088:
2024-12-12 21:52:04,898 - logger.py:50 - Optimizer state for param 140106898382016:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898008400:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898426912:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898009360:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898009520:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898008720:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898009600:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898009920:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898007760:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898007920:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106898008480:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897551008:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897564032:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897508208:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897509088:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897237072:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897507568:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106897363168:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106896739280:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106896736320:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106896063760:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106896062320:
2024-12-12 21:52:04,899 - logger.py:50 - Optimizer state for param 140106895805472:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106895485104:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106895483744:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106896627328:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107472804816:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107478062960:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107471366000:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129680:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898128960:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129920:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129200:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129440:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898130080:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129760:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898131680:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129040:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106898129520:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107472344944:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107471364960:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107472321168:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107472320608:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140107473002880:
2024-12-12 21:52:04,900 - logger.py:50 - Optimizer state for param 140106897115632:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106896628928:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895248096:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895336528:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895443184:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895465024:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106894475328:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106894477648:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106894475728:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106892398480:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895337088:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895338208:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106895162160:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646960:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106894361600:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646080:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893647040:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646240:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646400:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893647520:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646320:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893647200:
2024-12-12 21:52:04,901 - logger.py:50 - Optimizer state for param 140106893646560:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106896189616:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106893487296:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892995296:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892993056:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106893188048:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892397200:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892395120:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892245088:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106892245808:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891657920:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891660960:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891450944:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891449424:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891451824:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106889374592:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891866336:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891866816:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891185024:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106891828656:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106890809568:
2024-12-12 21:52:04,902 - logger.py:50 - Optimizer state for param 140106890665152:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891826656:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891825776:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891825456:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891828576:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891825376:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891826896:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891828496:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106891037168:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106890395376:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106889464464:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106889462944:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106889841696:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106889373792:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106889373872:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888854640:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888851760:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888120656:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888119296:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888317904:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888317584:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106888347456:
2024-12-12 21:52:04,903 - logger.py:50 - Optimizer state for param 140106885839904:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106888694016:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106888694736:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106887723824:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886846000:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106887693472:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886847040:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886845360:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886845440:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886846320:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886844560:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886845760:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886847920:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886847840:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106890767152:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106888800832:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886885840:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886401456:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106886441616:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106885837984:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106885837104:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106885715664:
2024-12-12 21:52:04,904 - logger.py:50 - Optimizer state for param 140106885707872:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885839744:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885450064:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885180688:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885180848:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885178288:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884768128:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884770208:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884768048:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885544672:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106885541952:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884998624:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884270576:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884271536:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884802576:
2024-12-12 21:52:04,905 - logger.py:50 - Optimizer state for param 140106884804096:
2024-12-12 21:52:04,930 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:52:04,937 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 21:52:04,954 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4952,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:04,957 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:04,959 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 21:52:05,014 - logger.py:50 - Processing step 1/4
2024-12-12 21:52:05,055 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:05,657 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:08,559 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:08,560 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:09,116 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:11,897 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:11,898 - logger.py:50 - Computing loss.
2024-12-12 21:52:11,979 - logger.py:50 - loss的值：2.0805492401123047
2024-12-12 21:52:11,980 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d5c05ecd0>, requires_grad: True
2024-12-12 21:52:11,981 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:52:11,983 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:52:17,603 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:52:17,604 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:52:17,605 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:52:17,605 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:52:17,606 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,606 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,607 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,608 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,609 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,610 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,611 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,611 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,611 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,611 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,611 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,612 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,613 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,614 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,615 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,616 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,616 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,616 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,616 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,616 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,617 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,618 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,619 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,620 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,621 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,622 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,623 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,624 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,625 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,626 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,627 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,628 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,629 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:17,630 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:17,631 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,632 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,633 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,634 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,635 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,636 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,636 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,636 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:52:17,636 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107672289008:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107478320608:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107478322528:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107478322768:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107478343568:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107478339728:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107672264768:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107672268208:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107477834144:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107672268448:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107477833344:
2024-12-12 21:52:17,642 - logger.py:50 - Optimizer state for param 140107477834784:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107477833984:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107477834064:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107477833904:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107477834864:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107672259456:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107472584848:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107472585728:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471489440:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107672203808:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107477910368:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107672204128:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471656736:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107472530304:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655856:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655936:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655456:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655216:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655696:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471655776:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471657216:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471657376:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471658496:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471658896:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471042656:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471042736:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107470997200:
2024-12-12 21:52:17,643 - logger.py:50 - Optimizer state for param 140107471076064:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107471836400:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107471085792:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106899420320:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107672286448:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107470612656:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107470526640:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898613248:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898613088:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897393440:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107471769168:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140107471769088:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898382016:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898008400:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898426912:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898009360:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898009520:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898008720:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898009600:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898009920:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898007760:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898007920:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106898008480:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897551008:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897564032:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897508208:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897509088:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897237072:
2024-12-12 21:52:17,644 - logger.py:50 - Optimizer state for param 140106897507568:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106897363168:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896739280:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896736320:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896063760:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896062320:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106895805472:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106895485104:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106895483744:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896627328:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107472804816:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107478062960:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107471366000:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129680:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898128960:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129920:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129200:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129440:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898130080:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129760:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898131680:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129040:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106898129520:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107472344944:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107471364960:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107472321168:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107472320608:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140107473002880:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106897115632:
2024-12-12 21:52:17,645 - logger.py:50 - Optimizer state for param 140106896628928:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895248096:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895336528:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895443184:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895465024:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106894475328:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106894477648:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106894475728:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106892398480:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895337088:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895338208:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106895162160:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646960:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106894361600:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646080:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893647040:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646240:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646400:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893647520:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646320:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893647200:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893646560:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106896189616:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893487296:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106892995296:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106892993056:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106893188048:
2024-12-12 21:52:17,646 - logger.py:50 - Optimizer state for param 140106892397200:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106892395120:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106892245088:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106892245808:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891657920:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891660960:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891450944:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891449424:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891451824:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889374592:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891866336:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891866816:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891185024:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891828656:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106890809568:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106890665152:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891826656:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891825776:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891825456:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891828576:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891825376:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891826896:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891828496:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106891037168:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106890395376:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889464464:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889462944:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889841696:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889373792:
2024-12-12 21:52:17,647 - logger.py:50 - Optimizer state for param 140106889373872:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888854640:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888851760:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888120656:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888119296:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888317904:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888317584:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888347456:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106885839904:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888694016:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888694736:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106887723824:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886846000:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106887693472:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886847040:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886845360:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886845440:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886846320:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886844560:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886845760:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886847920:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886847840:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106890767152:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106888800832:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886885840:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886401456:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106886441616:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106885837984:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106885837104:
2024-12-12 21:52:17,648 - logger.py:50 - Optimizer state for param 140106885715664:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885707872:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885839744:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885450064:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885180688:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885180848:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885178288:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884768128:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884770208:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884768048:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885544672:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106885541952:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884998624:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884270576:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884271536:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884802576:
2024-12-12 21:52:17,649 - logger.py:50 - Optimizer state for param 140106884804096:
2024-12-12 21:52:17,654 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:52:17,659 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 21:52:17,662 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:17,664 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:17,709 - logger.py:50 - Processing step 2/4
2024-12-12 21:52:17,742 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:18,348 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:20,483 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:20,483 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:20,546 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:21,753 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:21,754 - logger.py:50 - Computing loss.
2024-12-12 21:52:21,832 - logger.py:50 - loss的值：0.8949448466300964
2024-12-12 21:52:21,832 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d5003ca60>, requires_grad: True
2024-12-12 21:52:21,833 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:52:21,835 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:52:25,059 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:52:25,060 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:52:25,061 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:52:25,062 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:52:25,062 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,062 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,063 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:52:25,063 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,063 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,063 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,063 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,064 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,065 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,066 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,067 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,067 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,067 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,067 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,067 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,068 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,069 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,070 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,071 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,072 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,073 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,074 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,075 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,076 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,077 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,078 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,079 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,080 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,081 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,082 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,083 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,084 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,085 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,086 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:25,087 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,088 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,089 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,090 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,091 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,092 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:52:25,092 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:52:25,097 - logger.py:50 - Optimizer state for param 140107672289008:
2024-12-12 21:52:25,097 - logger.py:50 - Optimizer state for param 140107478320608:
2024-12-12 21:52:25,097 - logger.py:50 - Optimizer state for param 140107478322528:
2024-12-12 21:52:25,097 - logger.py:50 - Optimizer state for param 140107478322768:
2024-12-12 21:52:25,097 - logger.py:50 - Optimizer state for param 140107478343568:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107478339728:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672264768:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672268208:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477834144:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672268448:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477833344:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477834784:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477833984:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477834064:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477833904:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477834864:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672259456:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107472584848:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107472585728:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471489440:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672203808:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107477910368:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107672204128:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471656736:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107472530304:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471655856:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471655936:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471655456:
2024-12-12 21:52:25,098 - logger.py:50 - Optimizer state for param 140107471655216:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471655696:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471655776:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471657216:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471657376:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471658496:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471658896:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471042656:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471042736:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107470997200:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471076064:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471836400:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471085792:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106899420320:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107672286448:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107470612656:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107470526640:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898613248:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898613088:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106897393440:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471769168:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140107471769088:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898382016:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898008400:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898426912:
2024-12-12 21:52:25,099 - logger.py:50 - Optimizer state for param 140106898009360:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898009520:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898008720:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898009600:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898009920:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898007760:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898007920:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898008480:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897551008:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897564032:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897508208:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897509088:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897237072:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897507568:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106897363168:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106896739280:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106896736320:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106896063760:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106896062320:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106895805472:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106895485104:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106895483744:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106896627328:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140107472804816:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140107478062960:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140107471366000:
2024-12-12 21:52:25,100 - logger.py:50 - Optimizer state for param 140106898129680:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898128960:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129920:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129200:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129440:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898130080:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129760:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898131680:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129040:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106898129520:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140107472344944:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140107471364960:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140107472321168:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140107472320608:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140107473002880:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106897115632:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106896628928:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895248096:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895336528:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895443184:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895465024:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106894475328:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106894477648:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106894475728:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106892398480:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895337088:
2024-12-12 21:52:25,101 - logger.py:50 - Optimizer state for param 140106895338208:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106895162160:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646960:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106894361600:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646080:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893647040:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646240:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646400:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893647520:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646320:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893647200:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893646560:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106896189616:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893487296:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892995296:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892993056:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106893188048:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892397200:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892395120:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892245088:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106892245808:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106891657920:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106891660960:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106891450944:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106891449424:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106891451824:
2024-12-12 21:52:25,102 - logger.py:50 - Optimizer state for param 140106889374592:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891866336:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891866816:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891185024:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891828656:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106890809568:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106890665152:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891826656:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891825776:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891825456:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891828576:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891825376:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891826896:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891828496:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106891037168:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106890395376:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106889464464:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106889462944:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106889841696:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106889373792:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106889373872:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888854640:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888851760:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888120656:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888119296:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888317904:
2024-12-12 21:52:25,103 - logger.py:50 - Optimizer state for param 140106888317584:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106888347456:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885839904:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106888694016:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106888694736:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106887723824:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886846000:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106887693472:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886847040:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886845360:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886845440:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886846320:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886844560:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886845760:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886847920:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886847840:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106890767152:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106888800832:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886885840:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886401456:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106886441616:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885837984:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885837104:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885715664:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885707872:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885839744:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885450064:
2024-12-12 21:52:25,104 - logger.py:50 - Optimizer state for param 140106885180688:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106885180848:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106885178288:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884768128:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884770208:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884768048:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106885544672:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106885541952:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884998624:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884270576:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884271536:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884802576:
2024-12-12 21:52:25,105 - logger.py:50 - Optimizer state for param 140106884804096:
2024-12-12 21:52:25,110 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:52:25,115 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 21:52:25,118 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:25,120 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5056, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8199, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:25,152 - logger.py:50 - Processing step 3/4
2024-12-12 21:52:25,177 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:25,232 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:25,836 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:25,836 - logger.py:50 - Starting model forward pass.
2024-12-12 21:52:25,901 - logger.py:50 - Starting gradient computation.
2024-12-12 21:52:26,500 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 21:52:26,501 - logger.py:50 - Computing loss.
2024-12-12 21:52:26,605 - logger.py:50 - loss的值：0.28684836626052856
2024-12-12 21:52:26,606 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d2df8aaf0>, requires_grad: True
2024-12-12 21:52:26,607 - logger.py:50 - Visualizing computation graph.
2024-12-12 21:52:26,608 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 21:52:28,384 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 21:52:28,385 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 21:52:28,386 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 21:52:28,387 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 21:52:28,387 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,388 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,389 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 21:52:28,389 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,390 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,391 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,392 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,393 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,394 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,395 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,396 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,397 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,398 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,399 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,400 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,401 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 21:52:28,402 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,403 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,404 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,405 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,406 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,407 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,408 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,409 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,410 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 21:52:28,411 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,412 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,413 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,414 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,415 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,415 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,415 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 21:52:28,415 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107672289008:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107478320608:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107478322528:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107478322768:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107478343568:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107478339728:
2024-12-12 21:52:28,420 - logger.py:50 - Optimizer state for param 140107672264768:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107672268208:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477834144:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107672268448:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477833344:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477834784:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477833984:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477834064:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477833904:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477834864:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107672259456:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107472584848:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107472585728:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471489440:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107672203808:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107477910368:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107672204128:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471656736:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107472530304:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655856:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655936:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655456:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655216:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655696:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471655776:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471657216:
2024-12-12 21:52:28,421 - logger.py:50 - Optimizer state for param 140107471657376:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471658496:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471658896:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471042656:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471042736:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107470997200:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471076064:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471836400:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471085792:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106899420320:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107672286448:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107470612656:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107470526640:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898613248:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898613088:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106897393440:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471769168:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140107471769088:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898382016:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898008400:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898426912:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898009360:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898009520:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898008720:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898009600:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898009920:
2024-12-12 21:52:28,422 - logger.py:50 - Optimizer state for param 140106898007760:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898007920:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898008480:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897551008:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897564032:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897508208:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897509088:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897237072:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897507568:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106897363168:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106896739280:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106896736320:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106896063760:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106896062320:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106895805472:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106895485104:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106895483744:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106896627328:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140107472804816:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140107478062960:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140107471366000:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898129680:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898128960:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898129920:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898129200:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898129440:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898130080:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898129760:
2024-12-12 21:52:28,423 - logger.py:50 - Optimizer state for param 140106898131680:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106898129040:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106898129520:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140107472344944:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140107471364960:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140107472321168:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140107472320608:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140107473002880:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106897115632:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106896628928:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895248096:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895336528:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895443184:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895465024:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106894475328:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106894477648:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106894475728:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106892398480:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895337088:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895338208:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106895162160:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893646960:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106894361600:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893646080:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893647040:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893646240:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893646400:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893647520:
2024-12-12 21:52:28,424 - logger.py:50 - Optimizer state for param 140106893646320:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106893647200:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106893646560:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106896189616:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106893487296:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892995296:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892993056:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106893188048:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892397200:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892395120:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892245088:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106892245808:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891657920:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891660960:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891450944:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891449424:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891451824:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106889374592:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891866336:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891866816:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891185024:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891828656:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106890809568:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106890665152:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891826656:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891825776:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891825456:
2024-12-12 21:52:28,425 - logger.py:50 - Optimizer state for param 140106891828576:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106891825376:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106891826896:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106891828496:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106891037168:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106890395376:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106889464464:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106889462944:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106889841696:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106889373792:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106889373872:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888854640:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888851760:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888120656:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888119296:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888317904:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888317584:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888347456:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106885839904:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888694016:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106888694736:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106887723824:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886846000:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106887693472:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886847040:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886845360:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886845440:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886846320:
2024-12-12 21:52:28,426 - logger.py:50 - Optimizer state for param 140106886844560:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886845760:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886847920:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886847840:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106890767152:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106888800832:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886885840:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886401456:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106886441616:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885837984:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885837104:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885715664:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885707872:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885839744:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885450064:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885180688:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885180848:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885178288:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884768128:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884770208:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884768048:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885544672:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106885541952:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884998624:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884270576:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884271536:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884802576:
2024-12-12 21:52:28,427 - logger.py:50 - Optimizer state for param 140106884804096:
2024-12-12 21:52:28,432 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 21:52:28,436 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 21:52:28,438 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5056, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8199, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:52:28,440 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5056, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8199, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4912, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 21:53:06,520 - logger.py:50 - Step [0/1], Loss: 1.3727, MAE: 0.3626
2024-12-12 22:04:47,059 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:04:54,402 - logger.py:50 - Processing step 0/4
2024-12-12 22:04:54,667 - logger.py:50 - Starting model forward pass.
2024-12-12 22:04:55,576 - logger.py:50 - Starting gradient computation.
2024-12-12 22:04:56,702 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:04:56,703 - logger.py:50 - Starting model forward pass.
2024-12-12 22:04:57,517 - logger.py:50 - Starting gradient computation.
2024-12-12 22:04:59,695 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:04:59,696 - logger.py:50 - Computing loss.
2024-12-12 22:04:59,711 - logger.py:50 - loss的值：0.7932066321372986
2024-12-12 22:04:59,711 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f2f9078b0a0>, requires_grad: True
2024-12-12 22:04:59,711 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:04:59,712 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:05:02,813 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 22:05:02,814 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 22:05:02,815 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 22:05:02,815 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 22:05:02,815 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,815 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,816 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 22:05:02,816 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,816 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,816 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,816 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,817 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,818 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,818 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 22:05:02,818 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,818 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,819 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,819 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,819 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,819 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,820 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,820 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,820 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,820 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,820 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,821 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,821 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,821 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,821 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,821 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,822 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,822 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,822 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,822 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,823 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,823 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,823 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,823 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,824 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,824 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,824 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,824 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 22:05:02,825 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,825 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,825 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,825 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,826 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,826 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,826 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,826 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,826 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,827 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,827 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,827 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,827 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,827 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,828 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,828 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,828 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,828 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,829 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,829 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,829 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,829 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,830 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,830 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,830 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,830 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,831 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,831 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 22:05:02,831 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,831 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,831 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,832 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,832 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,832 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,832 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,833 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,833 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,833 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,833 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,833 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,834 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,834 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,834 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,834 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,835 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,835 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,835 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,835 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,835 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,836 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,836 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,836 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,836 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,837 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,837 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,837 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 22:05:02,837 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,838 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,838 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,838 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,838 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,839 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,839 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,839 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,839 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,839 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,840 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,841 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,841 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,841 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,841 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,842 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,842 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,842 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,842 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,843 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,843 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,843 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,843 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 22:05:02,844 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,844 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,844 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,844 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,844 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,845 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,845 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,845 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,845 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,845 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,846 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,847 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,847 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,847 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,847 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,848 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,848 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,848 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,848 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,848 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,849 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,849 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,849 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 22:05:02,849 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:05:02,849 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,850 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,850 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,850 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:05:02,850 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:05:02,850 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:05:02,851 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:05:02,852 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,852 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,852 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,852 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,852 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,853 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,853 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,853 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,853 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,853 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,854 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,854 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,854 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,854 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,854 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,855 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 22:05:02,855 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 22:05:02,855 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,855 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,855 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,856 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,856 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,856 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 22:05:02,856 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 22:05:02,937 - logger.py:50 - Optimizer state for param 139842263467568:
2024-12-12 22:05:02,937 - logger.py:50 - Optimizer state for param 139842770948160:
2024-12-12 22:05:02,937 - logger.py:50 - Optimizer state for param 139842263468208:
2024-12-12 22:05:02,937 - logger.py:50 - Optimizer state for param 139842263468448:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842263224864:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842263224784:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257755968:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842264239456:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842264222912:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842264251824:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842263227984:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842263228064:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842263228144:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842658950176:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257754608:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257756048:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257861664:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257861984:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842258050000:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139841928357744:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842258461136:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257600320:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842264202592:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257298992:
2024-12-12 22:05:02,938 - logger.py:50 - Optimizer state for param 139842257298752:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257297552:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257300112:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257298192:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257298032:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257299552:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257300192:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257299952:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257297792:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842257162384:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842256841680:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842256206240:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842256204560:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928735696:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139842264218336:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928356464:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928791840:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928526000:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841927794912:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841927795712:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928198240:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928199120:
2024-12-12 22:05:02,939 - logger.py:50 - Optimizer state for param 139841928197120:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841925494240:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927188288:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927184848:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926969680:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927219056:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927219536:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927218496:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927220336:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927218816:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139842256799120:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927218416:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927218896:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927219216:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841927220976:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926915472:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926815888:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926570848:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926288944:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841926376400:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841925493360:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139841925495760:
2024-12-12 22:05:02,940 - logger.py:50 - Optimizer state for param 139842257010336:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842658948656:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842257672768:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842257404448:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139841925885280:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139841925630368:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264228944:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139841923758256:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139841926051696:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139841927057136:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256257408:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264218176:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256258608:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256168416:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256167616:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256169936:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842256259728:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264217696:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264217536:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264219296:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842264217856:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842263241648:
2024-12-12 22:05:02,941 - logger.py:50 - Optimizer state for param 139842263242208:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841924129872:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841924129072:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923843872:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923758736:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923756176:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923632880:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923631760:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923259904:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841923243680:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841924239744:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841924330256:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841924331056:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841920952496:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922534192:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922534752:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922699408:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922743488:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922742528:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922743168:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922741248:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922741168:
2024-12-12 22:05:02,942 - logger.py:50 - Optimizer state for param 139841922744048:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922742768:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922743968:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922742928:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922744128:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922602464:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841922700688:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841923645744:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841923647104:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921480960:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841920952896:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841920950736:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921020608:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921021008:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841819784832:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841819811088:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921375984:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921373344:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841921373504:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841817597648:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841819613040:
2024-12-12 22:05:02,943 - logger.py:50 - Optimizer state for param 139841819613360:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819249216:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819302144:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819300784:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819303184:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819302544:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819303424:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819302864:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819301984:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819301744:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819302384:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841819300384:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841921145088:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841818845888:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841921164608:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841818464560:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841817899872:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841817597248:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841817597168:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841921165248:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841818884992:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841817099696:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841817097216:
2024-12-12 22:05:02,944 - logger.py:50 - Optimizer state for param 139841818251728:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841818252288:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841818029888:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841815372240:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841816566656:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841816564576:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841816564416:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817636272:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817635232:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817633792:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817634912:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817636512:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841819613600:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817636032:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817633472:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817635472:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817635792:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841816952640:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841816950400:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817693136:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841817275904:
2024-12-12 22:05:02,945 - logger.py:50 - Optimizer state for param 139841815663056:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815373360:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815371920:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815329584:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815330384:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814572304:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814570864:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814697120:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814699120:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815519456:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815020944:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815374800:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815373520:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814364784:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814365744:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814392320:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841814739360:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841815798144:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841813627504:
2024-12-12 22:05:02,946 - logger.py:50 - Optimizer state for param 139841813628304:
2024-12-12 22:05:02,970 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 22:05:02,975 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 22:05:02,993 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4952,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:05:02,995 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:05:02,996 - logger.py:50 - Epoch [0], Step [0/4], Loss: 0.7932, MAE: 0.1635
2024-12-12 22:05:03,051 - logger.py:50 - Processing step 1/4
2024-12-12 22:05:03,094 - logger.py:50 - Starting model forward pass.
2024-12-12 22:05:03,698 - logger.py:50 - Starting gradient computation.
2024-12-12 22:05:06,949 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:05:06,950 - logger.py:50 - Starting model forward pass.
2024-12-12 22:05:07,515 - logger.py:50 - Starting gradient computation.
2024-12-12 22:16:42,097 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:16:49,332 - logger.py:50 - Processing step 0/9
2024-12-12 22:16:49,401 - logger.py:50 - Starting model forward pass.
2024-12-12 22:16:50,307 - logger.py:50 - Starting gradient computation.
2024-12-12 22:16:51,164 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:16:51,165 - logger.py:50 - Computing loss.
2024-12-12 22:16:51,180 - logger.py:50 - loss的值：1.142579197883606
2024-12-12 22:16:51,181 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f713d77a9a0>, requires_grad: True
2024-12-12 22:16:51,181 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:16:51,181 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:16:52,013 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 22:16:52,014 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 22:16:52,015 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,016 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 22:16:52,017 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,018 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,019 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,020 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,021 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,021 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,021 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,021 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,021 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,022 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,023 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,024 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,025 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,026 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,026 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,026 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,026 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,026 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,027 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,028 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,029 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,030 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,030 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,030 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,030 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,030 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,031 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,031 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,031 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,031 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,031 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,032 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,033 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,034 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,035 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,035 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,035 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,035 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,035 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,036 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,037 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,038 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,039 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,040 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,041 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,041 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,041 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,041 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 22:16:52,041 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:52,042 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:52,043 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:52,044 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,044 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,044 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,044 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,044 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,045 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,046 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,046 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,046 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,046 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,046 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,047 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,048 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 22:16:52,048 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 22:16:52,122 - logger.py:50 - Optimizer state for param 140125124084256:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125124086496:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125124084176:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125124086736:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123966832:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123966912:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125125347664:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123967792:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123105232:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123104992:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123105152:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123105072:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123108672:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123104832:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123105392:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125123105472:
2024-12-12 22:16:52,123 - logger.py:50 - Optimizer state for param 140125125316832:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125122516928:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125122518368:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125125302688:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125124848512:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125122371152:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125122039216:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121569952:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121571472:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121572032:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121570032:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121569872:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121572432:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121569632:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121570352:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121571792:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121571712:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121609216:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121607456:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125121030720:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125120967760:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125120380736:
2024-12-12 22:16:52,124 - logger.py:50 - Optimizer state for param 140125120255056:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140125120257296:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124922632128:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124922630448:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140125120137072:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140125120592208:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921706592:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921706032:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921707312:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124920847312:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124922098688:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124922098048:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140125120540976:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921746352:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124922074432:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921231456:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921232416:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921233216:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921229696:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921747152:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921747712:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921747872:
2024-12-12 22:16:52,125 - logger.py:50 - Optimizer state for param 140124921746912:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124921015008:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124921015968:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920121200:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920120720:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920086416:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920844912:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920848192:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919798416:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919409056:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920085216:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919949248:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919000016:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124918997136:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919000896:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140125121725600:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124920846912:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919301840:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140124919954224:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140125121978832:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140125121979632:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140125121978592:
2024-12-12 22:16:52,126 - logger.py:50 - Optimizer state for param 140125121980032:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121981072:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121980752:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121980832:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121979952:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121980192:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121979232:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125120473504:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125120474544:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140124920755280:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140124920860000:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121030240:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121724880:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121727600:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125122724624:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125122726064:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125125301328:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125124848432:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140124347233680:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140124347233600:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140125121023008:
2024-12-12 22:16:52,127 - logger.py:50 - Optimizer state for param 140124345538496:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125123966592:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124850528:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124346964784:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124840320:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124347066624:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125123529360:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124347858192:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124085296:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124085056:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124087616:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124839840:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124839200:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140125124841200:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124346399456:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124346398496:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124345972272:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124345972752:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124345731808:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124345539056:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124345538256:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124344851008:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124344850208:
2024-12-12 22:16:52,128 - logger.py:50 - Optimizer state for param 140124344791664:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344794704:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344064576:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344062016:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344065936:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342032320:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344791504:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124344792944:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124343839296:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342995600:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124343761232:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342996480:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342994960:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342994320:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342995120:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342994800:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342993920:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342993760:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342995440:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124343838576:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124343688624:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342471632:
2024-12-12 22:16:52,129 - logger.py:50 - Optimizer state for param 140124342470592:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342253344:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342031280:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342033040:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341465472:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341465152:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341252240:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341252880:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124340893792:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124340894752:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341039728:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124338925632:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341254000:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341253840:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124340776688:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124341533328:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124340412224:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342129344:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342130544:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342132464:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342129904:
2024-12-12 22:16:52,130 - logger.py:50 - Optimizer state for param 140124342129984:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124342129664:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124341532288:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124341530688:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124341523216:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124341273760:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124339439232:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124339440912:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338927152:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338966016:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338965296:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338345040:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338346720:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338223200:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338221360:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337789104:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337790544:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337790704:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337790384:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337790784:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124337790144:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338223920:
2024-12-12 22:16:52,131 - logger.py:50 - Optimizer state for param 140124338222400:
2024-12-12 22:16:52,132 - logger.py:50 - Optimizer state for param 140124338198304:
2024-12-12 22:16:52,132 - logger.py:50 - Optimizer state for param 140124337395088:
2024-12-12 22:16:52,132 - logger.py:50 - Optimizer state for param 140124337396048:
2024-12-12 22:16:52,132 - logger.py:50 - Optimizer state for param 140124338199744:
2024-12-12 22:16:52,132 - logger.py:50 - Optimizer state for param 140124337668960:
2024-12-12 22:16:52,155 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 22:16:52,160 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 22:16:52,176 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4952,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:16:52,179 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:16:52,180 - logger.py:50 - Epoch [0], Step [0/9], Loss: 1.1426, MAE: 0.1908
2024-12-12 22:16:52,205 - logger.py:50 - Processing step 1/9
2024-12-12 22:16:52,224 - logger.py:50 - Starting model forward pass.
2024-12-12 22:16:53,070 - logger.py:50 - Starting gradient computation.
2024-12-12 22:16:54,817 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:16:54,818 - logger.py:50 - Computing loss.
2024-12-12 22:16:54,854 - logger.py:50 - loss的值：0.3368831276893616
2024-12-12 22:16:54,855 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f713d61b280>, requires_grad: True
2024-12-12 22:16:54,856 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:16:54,857 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:16:56,346 - logger.py:50 - rbf.mean gradient norm: 0.0
2024-12-12 22:16:56,347 - logger.py:50 - rbf.std gradient norm: 0.0
2024-12-12 22:16:56,348 - logger.py:50 - rbf.weight gradient norm: 0.0
2024-12-12 22:16:56,348 - logger.py:50 - rbf.bias gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.exp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.exp.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.rad.offset gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,349 - logger.py:50 - edge_deg_embed.rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,350 - logger.py:50 - edge_deg_embed.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.resweight gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,351 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,352 - logger.py:50 - blocks.0.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,353 - logger.py:50 - blocks.0.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,354 - logger.py:50 - blocks.0.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.0.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.resweight gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,355 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,356 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,357 - logger.py:50 - blocks.1.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,358 - logger.py:50 - blocks.1.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.1.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.2.resweight gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.2.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.2.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.2.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,359 - logger.py:50 - blocks.2.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,360 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,361 - logger.py:50 - blocks.2.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,362 - logger.py:50 - blocks.2.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.2.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.2.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.3.resweight gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.3.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.3.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.3.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,363 - logger.py:50 - blocks.3.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,364 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,365 - logger.py:50 - blocks.3.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,366 - logger.py:50 - blocks.3.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.3.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.3.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.4.resweight gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.4.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.4.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,367 - logger.py:50 - blocks.4.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,368 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,369 - logger.py:50 - blocks.4.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,370 - logger.py:50 - blocks.4.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.4.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.4.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.4.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.5.resweight gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.5.ga.alpha_dot gradient norm: 0.0
2024-12-12 22:16:56,371 - logger.py:50 - blocks.5.ga.merge_src.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.merge_src.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.merge_dst.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.offset gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.weight gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.0.bias gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.weight gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.1.bias gradient norm: 0.0
2024-12-12 22:16:56,372 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.weight gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.3.bias gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.weight gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.4.bias gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.dtp_rad.net.6.weight gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_act.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,373 - logger.py:50 - blocks.5.ga.sep_alpha.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.sep_alpha.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.sep_value.dtp.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.sep_value.lin.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.sep_value.lin.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.proj.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,374 - logger.py:50 - blocks.5.ga.proj.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn.fctp_1.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn.fctp_1.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn.fctp_2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn.fctp_2.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn_shortcut.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - blocks.5.ffn_shortcut.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,375 - logger.py:50 - norm.affine_weight gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - norm.affine_bias gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - head.0.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - head.0.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - head.2.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - lrs.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - lrs.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,376 - logger.py:50 - atom_expand.tp.weight gradient norm: 0.0
2024-12-12 22:16:56,377 - logger.py:50 - atom_expand.bias.0 gradient norm: 0.0
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125124084256:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125124086496:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125124084176:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125124086736:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125123966832:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125123966912:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125125347664:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125123967792:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125123105232:
2024-12-12 22:16:56,383 - logger.py:50 - Optimizer state for param 140125123104992:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123105152:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123105072:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123108672:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123104832:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123105392:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125123105472:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125125316832:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125122516928:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125122518368:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125125302688:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125124848512:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125122371152:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125122039216:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121569952:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121571472:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121572032:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121570032:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121569872:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121572432:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121569632:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121570352:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121571792:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121571712:
2024-12-12 22:16:56,384 - logger.py:50 - Optimizer state for param 140125121609216:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125121607456:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125121030720:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120967760:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120380736:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120255056:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120257296:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124922632128:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124922630448:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120137072:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120592208:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921706592:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921706032:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921707312:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124920847312:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124922098688:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124922098048:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140125120540976:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921746352:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124922074432:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921231456:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921232416:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921233216:
2024-12-12 22:16:56,385 - logger.py:50 - Optimizer state for param 140124921229696:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921747152:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921747712:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921747872:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921746912:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921015008:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124921015968:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920121200:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920120720:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920086416:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920844912:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920848192:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919798416:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919409056:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920085216:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919949248:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919000016:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124918997136:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919000896:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140125121725600:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124920846912:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919301840:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140124919954224:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140125121978832:
2024-12-12 22:16:56,386 - logger.py:50 - Optimizer state for param 140125121979632:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121978592:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121980032:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121981072:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121980752:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121980832:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121979952:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121980192:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121979232:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125120473504:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125120474544:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140124920755280:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140124920860000:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121030240:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121724880:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121727600:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125122724624:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125122726064:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125125301328:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125124848432:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140124347233680:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140124347233600:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125121023008:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140124345538496:
2024-12-12 22:16:56,387 - logger.py:50 - Optimizer state for param 140125123966592:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124850528:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124346964784:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124840320:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124347066624:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125123529360:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124347858192:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124085296:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124085056:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124087616:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124839840:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124839200:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140125124841200:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124346399456:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124346398496:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124345972272:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124345972752:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124345731808:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124345539056:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124345538256:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124344851008:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124344850208:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124344791664:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124344794704:
2024-12-12 22:16:56,388 - logger.py:50 - Optimizer state for param 140124344064576:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124344062016:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124344065936:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342032320:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124344791504:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124344792944:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124343839296:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342995600:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124343761232:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342996480:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342994960:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342994320:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342995120:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342994800:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342993920:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342993760:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342995440:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124343838576:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124343688624:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342471632:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342470592:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342253344:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342031280:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124342033040:
2024-12-12 22:16:56,389 - logger.py:50 - Optimizer state for param 140124341465472:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341465152:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341252240:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341252880:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124340893792:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124340894752:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341039728:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124338925632:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341254000:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341253840:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124340776688:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341533328:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124340412224:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342129344:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342130544:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342132464:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342129904:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342129984:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124342129664:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341532288:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341530688:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341523216:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124341273760:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124339439232:
2024-12-12 22:16:56,390 - logger.py:50 - Optimizer state for param 140124339440912:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338927152:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338966016:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338965296:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338345040:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338346720:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338223200:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338221360:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337789104:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337790544:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337790704:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337790384:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337790784:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337790144:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338223920:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338222400:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338198304:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337395088:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337396048:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124338199744:
2024-12-12 22:16:56,391 - logger.py:50 - Optimizer state for param 140124337668960:
2024-12-12 22:16:56,397 - logger.py:50 - 参数已更新: 参数索引 2
2024-12-12 22:16:56,593 - logger.py:50 - 更新的参数名称: atom_embed.atom_type_lin.bias.0
2024-12-12 22:16:56,596 - logger.py:50 - 更新前的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0336, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:16:56,598 - logger.py:50 - 更新后的参数值: tensor([[0.0712, 0.3707, 0.6010, 0.2484, 0.7013, 0.9579, 0.7000, 0.8255, 0.4520,
         0.4788, 0.3573, 0.9362, 0.1257, 0.7177, 0.7009, 0.9605, 0.4396, 0.2831,
         0.9853, 0.1467, 0.5057, 0.1933, 0.8582, 0.8330, 0.2339, 0.4645, 0.4951,
         0.0335, 0.9599, 0.7081, 0.4999, 0.9226, 0.7514, 0.7243, 0.7059, 0.3263,
         0.0973, 0.6855, 0.4615, 0.7533, 0.1392, 0.5471, 0.8735, 0.5863, 0.1482,
         0.6488, 0.9949, 0.3496, 0.4169, 0.7934, 0.8231, 0.6827, 0.5991, 0.8721,
         0.9680, 0.0547, 0.8200, 0.8779, 0.7783, 0.6980, 0.6712, 0.8791, 0.5631,
         0.9514, 0.4240, 0.7672, 0.8948, 0.5240, 0.8785, 0.8180, 0.6862, 0.3135,
         0.1254, 0.6980, 0.7949, 0.0516, 0.4913, 0.5001, 0.3932, 0.7933, 0.9289,
         0.3252, 0.7028, 0.8717, 0.4606, 0.7689]], device='cuda:0')
2024-12-12 22:16:56,619 - logger.py:50 - Processing step 2/9
2024-12-12 22:16:56,637 - logger.py:50 - Starting model forward pass.
2024-12-12 22:16:57,208 - logger.py:50 - Starting gradient computation.
2024-12-12 22:17:00,133 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:17:00,133 - logger.py:50 - Computing loss.
2024-12-12 22:17:00,196 - logger.py:50 - loss的值：0.09148358553647995
2024-12-12 22:17:00,197 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f716c096f10>, requires_grad: True
2024-12-12 22:17:00,197 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:17:00,198 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:46:18,344 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:46:25,593 - logger.py:50 - Processing step 0/9
2024-12-12 22:46:25,832 - logger.py:50 - Starting model forward pass.
2024-12-12 22:46:26,728 - logger.py:50 - Starting gradient computation.
2024-12-12 22:51:22,761 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:51:30,040 - logger.py:50 - Processing step 0/9
2024-12-12 22:51:30,110 - logger.py:50 - Starting model forward pass.
2024-12-12 22:51:30,110 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:12,256 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:53:19,445 - logger.py:50 - Processing step 0/9
2024-12-12 22:53:19,506 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:19,506 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:20,429 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:20,430 - logger.py:50 - Computing loss.
2024-12-12 22:53:20,443 - logger.py:50 - loss的值：1.142579197883606
2024-12-12 22:53:20,443 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f820d80cee0>, requires_grad: True
2024-12-12 22:53:20,443 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:20,443 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:20,487 - logger.py:50 - 参数未更新
2024-12-12 22:53:20,488 - logger.py:50 - Epoch [0], Step [0/9], Loss: 1.1426, MAE: 0.1908
2024-12-12 22:53:20,506 - logger.py:50 - Processing step 1/9
2024-12-12 22:53:20,520 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:20,520 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:21,330 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:21,331 - logger.py:50 - Computing loss.
2024-12-12 22:53:21,331 - logger.py:50 - loss的值：0.3368831276893616
2024-12-12 22:53:21,331 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f820d80cd00>, requires_grad: True
2024-12-12 22:53:21,331 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:21,331 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:21,345 - logger.py:50 - 参数未更新
2024-12-12 22:53:21,536 - logger.py:50 - Processing step 2/9
2024-12-12 22:53:21,550 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:21,550 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:22,128 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:22,129 - logger.py:50 - Computing loss.
2024-12-12 22:53:22,129 - logger.py:50 - loss的值：0.09148358553647995
2024-12-12 22:53:22,129 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f82145ac640>, requires_grad: True
2024-12-12 22:53:22,129 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:22,130 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:22,143 - logger.py:50 - 参数未更新
2024-12-12 22:53:22,167 - logger.py:50 - Processing step 3/9
2024-12-12 22:53:22,186 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:22,186 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:22,752 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:22,752 - logger.py:50 - Computing loss.
2024-12-12 22:53:22,752 - logger.py:50 - loss的值：2.96457839012146
2024-12-12 22:53:22,752 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f821431d040>, requires_grad: True
2024-12-12 22:53:22,752 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:22,753 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:22,766 - logger.py:50 - 参数未更新
2024-12-12 22:53:22,785 - logger.py:50 - Processing step 4/9
2024-12-12 22:53:22,801 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:22,801 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:23,582 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:23,582 - logger.py:50 - Computing loss.
2024-12-12 22:53:23,582 - logger.py:50 - loss的值：0.21769176423549652
2024-12-12 22:53:23,583 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f82146d01c0>, requires_grad: True
2024-12-12 22:53:23,583 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:23,583 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:23,597 - logger.py:50 - 参数未更新
2024-12-12 22:53:23,625 - logger.py:50 - Processing step 5/9
2024-12-12 22:53:23,648 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:23,648 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:23,714 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:23,715 - logger.py:50 - Computing loss.
2024-12-12 22:53:23,715 - logger.py:50 - loss的值：1.4134669303894043
2024-12-12 22:53:23,715 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f821431d040>, requires_grad: True
2024-12-12 22:53:23,715 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:23,715 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:23,729 - logger.py:50 - 参数未更新
2024-12-12 22:53:23,747 - logger.py:50 - Processing step 6/9
2024-12-12 22:53:23,758 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:23,758 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:23,818 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:23,818 - logger.py:50 - Computing loss.
2024-12-12 22:53:23,818 - logger.py:50 - loss的值：0.9547231793403625
2024-12-12 22:53:23,818 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f82145ac640>, requires_grad: True
2024-12-12 22:53:23,818 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:23,819 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:23,833 - logger.py:50 - 参数未更新
2024-12-12 22:53:23,856 - logger.py:50 - Processing step 7/9
2024-12-12 22:53:23,875 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:23,875 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:23,941 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:23,941 - logger.py:50 - Computing loss.
2024-12-12 22:53:23,942 - logger.py:50 - loss的值：0.025959795340895653
2024-12-12 22:53:23,942 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f82146d0e50>, requires_grad: True
2024-12-12 22:53:23,942 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:23,942 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:23,956 - logger.py:50 - 参数未更新
2024-12-12 22:53:23,977 - logger.py:50 - Processing step 8/9
2024-12-12 22:53:23,994 - logger.py:50 - Starting model forward pass.
2024-12-12 22:53:23,994 - logger.py:50 - Starting gradient computation.
2024-12-12 22:53:24,055 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:53:24,056 - logger.py:50 - Computing loss.
2024-12-12 22:53:24,056 - logger.py:50 - loss的值：6.8596086502075195
2024-12-12 22:53:24,056 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f82145ac640>, requires_grad: True
2024-12-12 22:53:24,056 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:53:24,056 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:53:24,071 - logger.py:50 - 参数未更新
2024-12-12 22:54:57,753 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:55:05,136 - logger.py:50 - Processing step 0/9
2024-12-12 22:55:05,200 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:05,200 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:06,115 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:06,116 - logger.py:50 - Computing loss.
2024-12-12 22:55:06,129 - logger.py:50 - loss的值：1.142579197883606
2024-12-12 22:55:06,129 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8f3a670>, requires_grad: True
2024-12-12 22:55:06,129 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:06,129 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:06,173 - logger.py:50 - 参数未更新
2024-12-12 22:55:06,174 - logger.py:50 - Epoch [0], Step [0/9], Loss: 1.1426, MAE: 0.1908
2024-12-12 22:55:06,388 - logger.py:50 - Processing step 1/9
2024-12-12 22:55:06,402 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:06,402 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:07,208 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:07,209 - logger.py:50 - Computing loss.
2024-12-12 22:55:07,209 - logger.py:50 - loss的值：0.3368831276893616
2024-12-12 22:55:07,209 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8e6ad60>, requires_grad: True
2024-12-12 22:55:07,209 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:07,209 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:07,224 - logger.py:50 - 参数未更新
2024-12-12 22:55:07,241 - logger.py:50 - Processing step 2/9
2024-12-12 22:55:07,254 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:07,254 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:07,834 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:07,834 - logger.py:50 - Computing loss.
2024-12-12 22:55:07,835 - logger.py:50 - loss的值：0.09148358553647995
2024-12-12 22:55:07,835 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d903165b0>, requires_grad: True
2024-12-12 22:55:07,835 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:07,835 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:07,849 - logger.py:50 - 参数未更新
2024-12-12 22:55:07,872 - logger.py:50 - Processing step 3/9
2024-12-12 22:55:07,892 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:07,892 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:08,447 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:08,447 - logger.py:50 - Computing loss.
2024-12-12 22:55:08,447 - logger.py:50 - loss的值：2.96457839012146
2024-12-12 22:55:08,447 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8f3a670>, requires_grad: True
2024-12-12 22:55:08,447 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:08,448 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:08,461 - logger.py:50 - 参数未更新
2024-12-12 22:55:08,484 - logger.py:50 - Processing step 4/9
2024-12-12 22:55:08,503 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:08,503 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:09,291 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:09,292 - logger.py:50 - Computing loss.
2024-12-12 22:55:09,292 - logger.py:50 - loss的值：0.21769176423549652
2024-12-12 22:55:09,292 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8e6ad60>, requires_grad: True
2024-12-12 22:55:09,292 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:09,293 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:09,307 - logger.py:50 - 参数未更新
2024-12-12 22:55:09,334 - logger.py:50 - Processing step 5/9
2024-12-12 22:55:09,356 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:09,356 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:09,420 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:09,420 - logger.py:50 - Computing loss.
2024-12-12 22:55:09,420 - logger.py:50 - loss的值：1.4134669303894043
2024-12-12 22:55:09,420 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8f3a670>, requires_grad: True
2024-12-12 22:55:09,421 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:09,421 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:09,435 - logger.py:50 - 参数未更新
2024-12-12 22:55:09,452 - logger.py:50 - Processing step 6/9
2024-12-12 22:55:09,463 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:09,463 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:09,520 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:09,520 - logger.py:50 - Computing loss.
2024-12-12 22:55:09,520 - logger.py:50 - loss的值：0.9547231793403625
2024-12-12 22:55:09,521 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8e6ad60>, requires_grad: True
2024-12-12 22:55:09,521 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:09,521 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:09,535 - logger.py:50 - 参数未更新
2024-12-12 22:55:09,558 - logger.py:50 - Processing step 7/9
2024-12-12 22:55:09,577 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:09,577 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:09,644 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:09,644 - logger.py:50 - Computing loss.
2024-12-12 22:55:09,644 - logger.py:50 - loss的值：0.025959795340895653
2024-12-12 22:55:09,644 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6da8e6ad60>, requires_grad: True
2024-12-12 22:55:09,644 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:09,645 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:09,659 - logger.py:50 - 参数未更新
2024-12-12 22:55:09,679 - logger.py:50 - Processing step 8/9
2024-12-12 22:55:09,696 - logger.py:50 - Starting model forward pass.
2024-12-12 22:55:09,696 - logger.py:50 - Starting gradient computation.
2024-12-12 22:55:09,756 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:55:09,756 - logger.py:50 - Computing loss.
2024-12-12 22:55:09,756 - logger.py:50 - loss的值：6.8596086502075195
2024-12-12 22:55:09,756 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7f6d902e90d0>, requires_grad: True
2024-12-12 22:55:09,756 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:55:09,757 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:55:09,771 - logger.py:50 - 参数未更新
2024-12-12 22:56:31,696 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:56:39,046 - logger.py:50 - Processing step 0/9
2024-12-12 22:56:39,289 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:39,289 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:40,201 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:40,202 - logger.py:50 - Computing loss.
2024-12-12 22:56:40,215 - logger.py:50 - loss的值：1.142579197883606
2024-12-12 22:56:40,215 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda44258970>, requires_grad: True
2024-12-12 22:56:40,215 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:40,215 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:40,259 - logger.py:50 - 参数未更新
2024-12-12 22:56:40,259 - logger.py:50 - Epoch [0], Step [0/9], Loss: 1.1426, MAE: 0.1908
2024-12-12 22:56:40,277 - logger.py:50 - Processing step 1/9
2024-12-12 22:56:40,292 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:40,292 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:41,106 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:41,107 - logger.py:50 - Computing loss.
2024-12-12 22:56:41,107 - logger.py:50 - loss的值：0.3368831276893616
2024-12-12 22:56:41,107 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda44337ee0>, requires_grad: True
2024-12-12 22:56:41,107 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:41,107 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:41,121 - logger.py:50 - 参数未更新
2024-12-12 22:56:41,137 - logger.py:50 - Processing step 2/9
2024-12-12 22:56:41,150 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:41,150 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:41,726 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:41,726 - logger.py:50 - Computing loss.
2024-12-12 22:56:41,726 - logger.py:50 - loss的值：0.09148358553647995
2024-12-12 22:56:41,727 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbc28c3280>, requires_grad: True
2024-12-12 22:56:41,727 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:41,727 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:41,740 - logger.py:50 - 参数未更新
2024-12-12 22:56:41,764 - logger.py:50 - Processing step 3/9
2024-12-12 22:56:41,783 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:41,783 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:42,336 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:42,336 - logger.py:50 - Computing loss.
2024-12-12 22:56:42,337 - logger.py:50 - loss的值：2.96457839012146
2024-12-12 22:56:42,337 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda44337a00>, requires_grad: True
2024-12-12 22:56:42,337 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:42,337 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:42,350 - logger.py:50 - 参数未更新
2024-12-12 22:56:42,370 - logger.py:50 - Processing step 4/9
2024-12-12 22:56:42,385 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:42,385 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:43,166 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:43,167 - logger.py:50 - Computing loss.
2024-12-12 22:56:43,167 - logger.py:50 - loss的值：0.21769176423549652
2024-12-12 22:56:43,167 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda601734c0>, requires_grad: True
2024-12-12 22:56:43,167 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:43,167 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:43,180 - logger.py:50 - 参数未更新
2024-12-12 22:56:43,203 - logger.py:50 - Processing step 5/9
2024-12-12 22:56:43,222 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:43,222 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:43,286 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:43,286 - logger.py:50 - Computing loss.
2024-12-12 22:56:43,287 - logger.py:50 - loss的值：1.4134669303894043
2024-12-12 22:56:43,287 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda44258550>, requires_grad: True
2024-12-12 22:56:43,287 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:43,287 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:43,300 - logger.py:50 - 参数未更新
2024-12-12 22:56:43,314 - logger.py:50 - Processing step 6/9
2024-12-12 22:56:43,325 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:43,325 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:43,382 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:43,382 - logger.py:50 - Computing loss.
2024-12-12 22:56:43,382 - logger.py:50 - loss的值：0.9547231793403625
2024-12-12 22:56:43,383 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbc28c3280>, requires_grad: True
2024-12-12 22:56:43,383 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:43,383 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:43,396 - logger.py:50 - 参数未更新
2024-12-12 22:56:43,416 - logger.py:50 - Processing step 7/9
2024-12-12 22:56:43,431 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:43,431 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:43,493 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:43,494 - logger.py:50 - Computing loss.
2024-12-12 22:56:43,494 - logger.py:50 - loss的值：0.025959795340895653
2024-12-12 22:56:43,494 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda443374c0>, requires_grad: True
2024-12-12 22:56:43,494 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:43,494 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:43,507 - logger.py:50 - 参数未更新
2024-12-12 22:56:43,523 - logger.py:50 - Processing step 8/9
2024-12-12 22:56:43,536 - logger.py:50 - Starting model forward pass.
2024-12-12 22:56:43,536 - logger.py:50 - Starting gradient computation.
2024-12-12 22:56:43,595 - logger.py:50 - Sampled Hessian values:
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
0.0000e+00
2024-12-12 22:56:43,596 - logger.py:50 - Computing loss.
2024-12-12 22:56:43,596 - logger.py:50 - loss的值：6.8596086502075195
2024-12-12 22:56:43,596 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fda601734c0>, requires_grad: True
2024-12-12 22:56:43,596 - logger.py:50 - Visualizing computation graph.
2024-12-12 22:56:43,596 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 22:56:43,608 - logger.py:50 - 参数未更新
2024-12-12 22:58:12,653 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:58:19,892 - logger.py:50 - Processing step 0/9
2024-12-12 22:58:20,130 - logger.py:50 - Starting model forward pass.
2024-12-12 22:58:20,130 - logger.py:50 - Starting gradient computation.
2024-12-12 22:58:20,130 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 22:59:36,161 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 22:59:43,487 - logger.py:50 - Processing step 0/9
2024-12-12 22:59:43,718 - logger.py:50 - Starting model forward pass.
2024-12-12 22:59:43,718 - logger.py:50 - Starting gradient computation.
2024-12-12 22:59:43,718 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:00:45,112 - logger.py:50 - Sampled Hessian values:
-8.9428e-04
5.6449e-08
5.6155e-09
-2.2508e-07
-2.1046e-07
2.9987e-09
-1.6634e-03
-6.2498e-07
-2.5006e-04
2.4126e-07
2024-12-12 23:00:45,117 - logger.py:50 - Computing loss.
2024-12-12 23:00:45,144 - logger.py:50 - loss的值：1.1421982049942017
2024-12-12 23:00:45,144 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7ff2a85beb50>, requires_grad: True
2024-12-12 23:00:45,144 - logger.py:50 - Visualizing computation graph.
2024-12-12 23:00:45,147 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 23:00:45,213 - logger.py:50 - 参数未更新
2024-12-12 23:00:45,215 - logger.py:50 - Epoch [0], Step [0/9], Loss: 1.1422, MAE: 0.1908
2024-12-12 23:00:45,242 - logger.py:50 - Processing step 1/9
2024-12-12 23:00:45,262 - logger.py:50 - Starting model forward pass.
2024-12-12 23:00:45,262 - logger.py:50 - Starting gradient computation.
2024-12-12 23:00:45,262 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:01:37,343 - logger.py:50 - Sampled Hessian values:
2.1106e-06
5.0919e-03
3.2391e-08
-6.5528e-07
-4.7590e-06
2.3221e-06
2.7259e-07
-5.6936e-04
9.4829e-07
1.7145e-06
2024-12-12 23:01:37,345 - logger.py:50 - Computing loss.
2024-12-12 23:01:37,346 - logger.py:50 - loss的值：0.3364669382572174
2024-12-12 23:01:37,347 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7ff2d719e640>, requires_grad: True
2024-12-12 23:01:37,347 - logger.py:50 - Visualizing computation graph.
2024-12-12 23:01:37,351 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-12 23:01:37,395 - logger.py:50 - 参数未更新
2024-12-12 23:01:37,426 - logger.py:50 - Processing step 2/9
2024-12-12 23:01:37,444 - logger.py:50 - Starting model forward pass.
2024-12-12 23:01:37,444 - logger.py:50 - Starting gradient computation.
2024-12-12 23:01:37,444 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:08:00,766 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:08:08,124 - logger.py:50 - Processing step 0/9
2024-12-12 23:08:08,365 - logger.py:50 - Starting model forward pass.
2024-12-12 23:08:08,365 - logger.py:50 - Starting gradient computation.
2024-12-12 23:08:08,365 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:12:15,015 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:14:57,943 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:15:05,095 - logger.py:50 - Processing step 0/9
2024-12-12 23:15:05,182 - logger.py:50 - Starting model forward pass.
2024-12-12 23:15:05,182 - logger.py:50 - Starting gradient computation.
2024-12-12 23:15:05,182 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:15:51,410 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:15:58,744 - logger.py:50 - Processing step 0/9
2024-12-12 23:15:58,819 - logger.py:50 - Starting model forward pass.
2024-12-12 23:15:58,819 - logger.py:50 - Starting gradient computation.
2024-12-12 23:15:58,819 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:17:00,932 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:17:08,314 - logger.py:50 - Processing step 0/9
2024-12-12 23:17:08,377 - logger.py:50 - Starting model forward pass.
2024-12-12 23:17:08,377 - logger.py:50 - Starting gradient computation.
2024-12-12 23:17:08,377 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:21:05,000 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:21:12,415 - logger.py:50 - Processing step 0/9
2024-12-12 23:21:12,658 - logger.py:50 - Starting model forward pass.
2024-12-12 23:21:12,659 - logger.py:50 - Starting gradient computation.
2024-12-12 23:21:12,659 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:21:54,556 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:22:01,880 - logger.py:50 - Processing step 0/9
2024-12-12 23:22:01,954 - logger.py:50 - Starting model forward pass.
2024-12-12 23:22:01,955 - logger.py:50 - Starting gradient computation.
2024-12-12 23:22:01,955 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:23:49,262 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:23:56,696 - logger.py:50 - Processing step 0/9
2024-12-12 23:23:56,956 - logger.py:50 - Starting model forward pass.
2024-12-12 23:23:56,957 - logger.py:50 - Starting gradient computation.
2024-12-12 23:23:56,957 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:28:31,099 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:28:38,784 - logger.py:50 - Processing step 0/9
2024-12-12 23:28:39,100 - logger.py:50 - Starting model forward pass.
2024-12-12 23:28:39,100 - logger.py:50 - Starting gradient computation.
2024-12-12 23:28:39,100 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:29:26,393 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:29:33,922 - logger.py:50 - Processing step 0/9
2024-12-12 23:29:33,992 - logger.py:50 - Starting model forward pass.
2024-12-12 23:29:33,993 - logger.py:50 - Starting gradient computation.
2024-12-12 23:29:33,993 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:31:33,789 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:31:41,227 - logger.py:50 - Processing step 0/9
2024-12-12 23:31:41,293 - logger.py:50 - Starting model forward pass.
2024-12-12 23:31:41,293 - logger.py:50 - Starting gradient computation.
2024-12-12 23:31:41,294 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:35:00,535 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:35:08,174 - logger.py:50 - Processing step 0/9
2024-12-12 23:35:08,249 - logger.py:50 - Starting model forward pass.
2024-12-12 23:35:08,249 - logger.py:50 - Starting gradient computation.
2024-12-12 23:35:08,249 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:36:59,069 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:37:06,495 - logger.py:50 - Processing step 0/9
2024-12-12 23:37:06,582 - logger.py:50 - Starting model forward pass.
2024-12-12 23:37:06,582 - logger.py:50 - Starting gradient computation.
2024-12-12 23:37:06,582 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:40:57,099 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:41:04,770 - logger.py:50 - Processing step 0/9
2024-12-12 23:41:04,829 - logger.py:50 - Starting model forward pass.
2024-12-12 23:41:04,829 - logger.py:50 - Starting gradient computation.
2024-12-12 23:41:04,829 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:45:29,590 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:45:42,564 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:45:53,880 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:46:01,494 - logger.py:50 - Processing step 0/9
2024-12-12 23:46:01,565 - logger.py:50 - Starting model forward pass.
2024-12-12 23:46:01,565 - logger.py:50 - Starting gradient computation.
2024-12-12 23:46:01,565 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:50:02,551 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:50:10,003 - logger.py:50 - Processing step 0/9
2024-12-12 23:50:10,059 - logger.py:50 - Starting model forward pass.
2024-12-12 23:50:10,060 - logger.py:50 - Starting gradient computation.
2024-12-12 23:50:10,060 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-12 23:52:06,651 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-12 23:52:14,473 - logger.py:50 - Processing step 0/9
2024-12-12 23:52:14,542 - logger.py:50 - Starting model forward pass.
2024-12-12 23:52:14,542 - logger.py:50 - Starting gradient computation.
2024-12-12 23:52:14,542 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:08:13,486 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-13 00:19:43,897 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-13 00:20:01,660 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-13 00:20:09,436 - logger.py:50 - Processing step 0/9
2024-12-13 00:20:09,524 - logger.py:50 - Starting model forward pass.
2024-12-13 00:20:09,524 - logger.py:50 - Starting gradient computation.
2024-12-13 00:20:09,524 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:20:45,843 - logger.py:50 - Sampled Hessian values:
6.7858e-04
-2.5858e-08
1.5952e-08
-2.7677e-04
-2.3810e-03
4.0454e-07
-5.1962e-07
-4.8261e-07
-3.0683e-03
-1.1069e-04
2024-12-13 00:20:45,847 - logger.py:50 - Computing loss.
2024-12-13 00:20:45,875 - logger.py:50 - loss的值：0.9583429098129272
2024-12-13 00:20:45,875 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbe067cfd0>, requires_grad: True
2024-12-13 00:20:45,875 - logger.py:50 - Visualizing computation graph.
2024-12-13 00:20:45,877 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-13 00:20:45,943 - logger.py:50 - 参数未更新
2024-12-13 00:20:45,944 - logger.py:50 - Epoch [0], Step [0/9], Loss: 0.9583, MAE: 0.3219
2024-12-13 00:20:45,971 - logger.py:50 - Processing step 1/9
2024-12-13 00:20:45,994 - logger.py:50 - Starting model forward pass.
2024-12-13 00:20:45,994 - logger.py:50 - Starting gradient computation.
2024-12-13 00:20:45,994 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:21:43,938 - logger.py:50 - Sampled Hessian values:
1.5586e-10
-2.3127e-02
-1.9562e-09
3.0029e-10
8.6385e-04
-5.2199e-07
5.5935e-08
-1.0001e-10
-3.8780e-10
-4.5674e-08
2024-12-13 00:21:43,940 - logger.py:50 - Computing loss.
2024-12-13 00:21:43,942 - logger.py:50 - loss的值：0.026789026334881783
2024-12-13 00:21:43,942 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbc46430d0>, requires_grad: True
2024-12-13 00:21:43,942 - logger.py:50 - Visualizing computation graph.
2024-12-13 00:21:43,945 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-13 00:21:43,990 - logger.py:50 - 参数未更新
2024-12-13 00:21:44,027 - logger.py:50 - Processing step 2/9
2024-12-13 00:21:44,056 - logger.py:50 - Starting model forward pass.
2024-12-13 00:21:44,056 - logger.py:50 - Starting gradient computation.
2024-12-13 00:21:44,056 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:22:49,545 - logger.py:50 - Sampled Hessian values:
-1.6368e-14
1.2165e-08
2.3149e-07
3.8684e-08
-2.6852e-08
-3.1421e-04
1.4820e-11
2.2019e-09
-4.9782e-04
2.7098e-12
2024-12-13 00:22:49,546 - logger.py:50 - Computing loss.
2024-12-13 00:22:49,547 - logger.py:50 - loss的值：2.9639830589294434
2024-12-13 00:22:49,547 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbf4aa3a30>, requires_grad: True
2024-12-13 00:22:49,547 - logger.py:50 - Visualizing computation graph.
2024-12-13 00:22:49,550 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-13 00:22:49,580 - logger.py:50 - 参数未更新
2024-12-13 00:22:49,620 - logger.py:50 - Processing step 3/9
2024-12-13 00:22:49,654 - logger.py:50 - Starting model forward pass.
2024-12-13 00:22:49,654 - logger.py:50 - Starting gradient computation.
2024-12-13 00:22:49,654 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:23:54,595 - logger.py:50 - Sampled Hessian values:
-5.5563e-07
-6.8488e-13
1.1591e-08
-1.5780e-04
2.5409e-07
8.6743e-12
6.2080e-10
2.8287e-07
8.6670e-12
0.0000e+00
2024-12-13 00:23:54,597 - logger.py:50 - Computing loss.
2024-12-13 00:23:54,599 - logger.py:50 - loss的值：1.4129672050476074
2024-12-13 00:23:54,599 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbc4248970>, requires_grad: True
2024-12-13 00:23:54,599 - logger.py:50 - Visualizing computation graph.
2024-12-13 00:23:54,602 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-13 00:23:54,645 - logger.py:50 - 参数未更新
2024-12-13 00:23:54,674 - logger.py:50 - Processing step 4/9
2024-12-13 00:23:54,696 - logger.py:50 - Starting model forward pass.
2024-12-13 00:23:54,696 - logger.py:50 - Starting gradient computation.
2024-12-13 00:23:54,696 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:24:45,913 - logger.py:50 - Sampled Hessian values:
-2.5793e-08
6.0894e-10
8.9851e-10
-6.0745e-05
2.0448e-03
-6.2915e-13
1.2991e-02
1.1415e-07
-1.8414e-06
1.2798e-07
2024-12-13 00:24:45,915 - logger.py:50 - Computing loss.
2024-12-13 00:24:45,916 - logger.py:50 - loss的值：0.3364669382572174
2024-12-13 00:24:45,916 - logger.py:50 - Loss grad_fn: <MseLossBackward0 object at 0x7fdbf4aa3a30>, requires_grad: True
2024-12-13 00:24:45,916 - logger.py:50 - Visualizing computation graph.
2024-12-13 00:24:45,918 - logger.py:50 - Performing backward pass and optimizer step.
2024-12-13 00:24:45,956 - logger.py:50 - 参数未更新
2024-12-13 00:24:45,990 - logger.py:50 - Processing step 5/9
2024-12-13 00:24:46,014 - logger.py:50 - Starting model forward pass.
2024-12-13 00:24:46,014 - logger.py:50 - Starting gradient computation.
2024-12-13 00:24:46,014 - logger.py:50 - Starting Hessian computation using functional API.
2024-12-13 00:25:33,860 - logger.py:50 - Namespace(output_dir='models/ForceCon2/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=20, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='../datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-12-13 00:25:41,887 - logger.py:50 - Processing step 0/9
2024-12-13 00:25:41,950 - logger.py:50 - Starting model forward pass.
2024-12-13 00:25:41,950 - logger.py:50 - Starting gradient computation.
2024-12-13 00:25:41,950 - logger.py:50 - Starting Hessian computation using functional API.
