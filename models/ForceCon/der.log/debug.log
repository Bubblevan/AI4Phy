2024-05-08 22:25:42,488 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-08 22:26:06,588 - logger.py:50 - GraphAttentionTransformer(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-08 22:26:08,611 - logger.py:50 - Number of params: 2978805
2024-05-08 22:32:26,736 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-08 22:32:50,760 - logger.py:50 - GraphAttentionTransformer(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-08 22:32:52,834 - logger.py:50 - Number of params: 2978805
2024-05-08 23:29:45,279 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-09 15:12:03,054 - logger.py:50 - GraphAttentionTransformer(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-09 15:12:09,944 - logger.py:50 - Number of params: 2978805
2024-05-11 23:36:07,055 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-11 23:36:32,311 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-11 23:36:34,495 - logger.py:50 - Number of params: 2978805
2024-05-11 23:42:26,440 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-11 23:42:50,111 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-11 23:42:52,136 - logger.py:50 - Number of params: 2978805
2024-05-11 23:53:32,220 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-11 23:53:55,575 - logger.py:50 - GraphAttentionTransformer(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e+1x1o+1x2e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-11 23:53:57,532 - logger.py:50 - Number of params: 2978805
2024-05-11 23:56:20,232 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-11 23:56:44,647 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-11 23:56:46,707 - logger.py:50 - Number of params: 2978805
2024-05-13 22:09:26,529 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-13 22:09:50,490 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-13 22:09:52,575 - logger.py:50 - Number of params: 2978805
2024-05-13 22:10:56,578 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-13 22:11:21,465 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-13 22:11:23,783 - logger.py:50 - Number of params: 2978805
2024-05-13 22:15:48,290 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-13 22:16:11,988 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-13 22:16:13,949 - logger.py:50 - Number of params: 2978805
2024-05-15 22:06:27,724 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-15 22:08:18,414 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-05-15 22:08:42,831 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-05-15 22:08:44,786 - logger.py:50 - Number of params: 2978805
2024-06-05 12:51:36,610 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 12:52:00,392 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 12:52:02,664 - logger.py:50 - Number of params: 2978805
2024-06-05 20:25:01,220 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 20:25:24,658 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 20:25:26,629 - logger.py:50 - Number of params: 2978805
2024-06-05 20:27:14,357 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 20:27:37,704 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 20:27:39,706 - logger.py:50 - Number of params: 2978805
2024-06-05 20:28:56,644 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 20:29:20,075 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 20:29:22,078 - logger.py:50 - Number of params: 2978805
2024-06-05 20:43:40,544 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 20:44:03,800 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 20:44:05,747 - logger.py:50 - Number of params: 2978805
2024-06-05 21:03:52,752 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:04:16,816 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:04:19,019 - logger.py:50 - Number of params: 2978805
2024-06-05 21:14:19,822 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:14:43,294 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:14:45,165 - logger.py:50 - Number of params: 2978805
2024-06-05 21:29:39,796 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:30:03,049 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:30:04,891 - logger.py:50 - Number of params: 2978805
2024-06-05 21:35:42,961 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:35:50,057 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:36:13,614 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:36:15,379 - logger.py:50 - Number of params: 2978805
2024-06-05 21:39:01,165 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:39:24,357 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:39:26,282 - logger.py:50 - Number of params: 2978805
2024-06-05 21:48:45,087 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:49:08,653 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:49:10,636 - logger.py:50 - Number of params: 2978805
2024-06-05 21:54:18,555 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:54:41,749 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:54:43,646 - logger.py:50 - Number of params: 2978805
2024-06-05 21:55:44,285 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:56:07,736 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:56:09,573 - logger.py:50 - Number of params: 2978805
2024-06-05 21:57:59,910 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 21:58:23,479 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 21:58:25,285 - logger.py:50 - Number of params: 2978805
2024-06-05 22:00:27,623 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 22:00:51,054 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 22:00:52,943 - logger.py:50 - Number of params: 2978805
2024-06-05 22:05:09,241 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 22:05:32,682 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 22:05:34,597 - logger.py:50 - Number of params: 2978805
2024-06-05 22:07:49,481 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 22:08:12,939 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 22:08:14,848 - logger.py:50 - Number of params: 2978805
2024-06-05 22:10:55,762 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-05 22:11:19,002 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-05 22:11:20,859 - logger.py:50 - Number of params: 2978805
2024-06-06 00:34:28,832 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-06 20:56:19,767 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-07 20:24:50,084 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-07 21:29:26,760 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:10:49,822 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:17:39,259 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:20:05,498 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:21:12,390 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:21:13,648 - logger.py:50 - Number of params: 2978805
2024-06-09 12:23:55,913 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:24:07,236 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:24:08,513 - logger.py:50 - Number of params: 2978805
2024-06-09 12:41:36,219 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:41:47,568 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:41:49,035 - logger.py:50 - Number of params: 2978805
2024-06-09 12:53:03,709 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:53:14,346 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:53:15,626 - logger.py:50 - Number of params: 2978805
2024-06-09 12:55:27,027 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:55:37,640 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:55:38,930 - logger.py:50 - Number of params: 2978805
2024-06-09 12:58:46,821 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 12:58:58,075 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 12:58:59,524 - logger.py:50 - Number of params: 2978805
2024-06-09 13:01:48,479 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:01:59,222 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:02:00,710 - logger.py:50 - Number of params: 2978805
2024-06-09 13:07:55,748 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:08:06,238 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:08:07,760 - logger.py:50 - Number of params: 2978805
2024-06-09 13:11:51,410 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:12:02,960 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:12:04,553 - logger.py:50 - Number of params: 2978805
2024-06-09 13:12:55,269 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:13:06,036 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:13:07,318 - logger.py:50 - Number of params: 2978805
2024-06-09 13:14:05,631 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:14:16,513 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:14:17,967 - logger.py:50 - Number of params: 2978805
2024-06-09 13:15:33,301 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:15:44,164 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:15:45,618 - logger.py:50 - Number of params: 2978805
2024-06-09 13:19:02,252 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:19:13,251 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:19:14,743 - logger.py:50 - Number of params: 2978805
2024-06-09 13:23:03,584 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:23:14,917 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:23:16,401 - logger.py:50 - Number of params: 2978805
2024-06-09 13:23:54,825 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:24:05,474 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:24:06,757 - logger.py:50 - Number of params: 2978805
2024-06-09 13:46:26,519 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 13:46:37,445 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 13:46:38,783 - logger.py:50 - Number of params: 2978805
2024-06-09 14:02:53,935 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:03:04,898 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:03:06,422 - logger.py:50 - Number of params: 2978805
2024-06-09 14:24:33,201 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:24:43,763 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:24:45,062 - logger.py:50 - Number of params: 2978805
2024-06-09 14:26:22,037 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:26:33,010 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:26:34,276 - logger.py:50 - Number of params: 2978805
2024-06-09 14:27:34,585 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:27:44,840 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:27:46,111 - logger.py:50 - Number of params: 2978805
2024-06-09 14:35:33,284 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:35:44,601 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:35:45,865 - logger.py:50 - Number of params: 2978805
2024-06-09 14:36:34,497 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:36:46,085 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:36:47,583 - logger.py:50 - Number of params: 2978805
2024-06-09 14:51:21,903 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:51:39,226 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:51:50,314 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:51:51,542 - logger.py:50 - Number of params: 2978805
2024-06-09 14:55:49,332 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 14:55:59,820 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 14:56:01,169 - logger.py:50 - Number of params: 2978805
2024-06-09 15:02:59,087 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:04:04,296 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:04:05,564 - logger.py:50 - Number of params: 2978805
2024-06-09 15:04:29,222 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:04:40,017 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:04:41,410 - logger.py:50 - Number of params: 2978805
2024-06-09 15:23:13,383 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:23:24,344 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:23:25,730 - logger.py:50 - Number of params: 2978805
2024-06-09 15:24:10,850 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:24:21,708 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:24:23,130 - logger.py:50 - Number of params: 2978805
2024-06-09 15:35:18,021 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:35:29,316 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:35:30,999 - logger.py:50 - Number of params: 2978805
2024-06-09 15:50:05,736 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=True, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 15:50:16,133 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 15:50:17,488 - logger.py:50 - Number of params: 2978805
2024-06-09 16:02:52,094 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 16:03:03,230 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 16:03:03,494 - logger.py:50 - Number of params: 2978805
2024-06-09 16:14:51,101 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 16:15:02,287 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 16:15:02,521 - logger.py:50 - Number of params: 2978805
2024-06-09 16:23:36,883 - logger.py:50 - Epoch: [0][0/9] loss: 1.14258, MAE: 0.19084, time/step=514354ms, lr=1.00e-06
2024-06-09 16:54:45,241 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-09 16:54:56,446 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-09 16:54:56,687 - logger.py:50 - Number of params: 2978805
2024-06-09 16:57:05,337 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-10 10:29:36,964 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-10 10:29:45,620 - logger.py:50 - Number of params: 2978805
2024-06-10 10:33:39,077 - logger.py:50 - Epoch: [0][0/8002] loss: 11.18626, MAE: 0.69803, time/step=233447ms, lr=1.00e-06
2024-06-10 16:15:50,878 - logger.py:50 - Epoch: [0][50/8002] loss: 2.04430, MAE: 0.20418, time/step=407162ms, lr=1.00e-06
2024-06-10 22:34:09,412 - logger.py:50 - Epoch: [0][100/8002] loss: 1.67463, MAE: 0.18818, time/step=430334ms, lr=1.00e-06
2024-06-11 04:15:43,057 - logger.py:50 - Epoch: [0][150/8002] loss: 1.60851, MAE: 0.18290, time/step=423559ms, lr=1.00e-06
2024-06-11 09:14:52,729 - logger.py:50 - Epoch: [0][200/8002] loss: 1.65770, MAE: 0.18928, time/step=407498ms, lr=1.00e-06
2024-06-11 09:20:49,680 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 09:21:35,578 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 09:22:05,913 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 09:22:46,133 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 09:22:47,442 - logger.py:50 - Number of params: 2978805
2024-06-11 10:30:00,535 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 10:30:41,095 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 10:30:42,455 - logger.py:50 - Number of params: 2978805
2024-06-11 10:43:22,047 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 10:44:02,272 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 10:44:03,682 - logger.py:50 - Number of params: 2978805
2024-06-11 10:51:00,543 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 10:51:38,001 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 10:51:39,324 - logger.py:50 - Number of params: 2978805
2024-06-11 11:16:49,993 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 11:17:28,663 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 11:17:30,129 - logger.py:50 - Number of params: 2978805
2024-06-11 14:03:25,667 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 14:04:05,048 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 14:04:06,433 - logger.py:50 - Number of params: 2978805
2024-06-11 14:58:56,373 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 14:59:34,186 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 14:59:35,526 - logger.py:50 - Number of params: 2978805
2024-06-11 15:02:28,591 - logger.py:50 - Epoch: [0][250/8002] loss: 1.61704, MAE: 0.18511, time/step=409414ms, lr=1.00e-06
2024-06-11 18:01:07,430 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 18:01:46,220 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 18:01:47,519 - logger.py:50 - Number of params: 2978805
2024-06-11 19:45:17,351 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 19:45:57,222 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 19:45:58,544 - logger.py:50 - Number of params: 2978805
2024-06-11 19:47:42,206 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 19:48:20,285 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 19:48:21,667 - logger.py:50 - Number of params: 2978805
2024-06-11 19:49:09,149 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 19:49:47,358 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 19:49:48,643 - logger.py:50 - Number of params: 2978805
2024-06-11 20:56:25,738 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 20:57:05,882 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 20:57:07,149 - logger.py:50 - Number of params: 2978805
2024-06-11 20:57:54,773 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 20:58:38,620 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 20:58:39,956 - logger.py:50 - Number of params: 2978805
2024-06-11 21:01:07,861 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 21:01:56,000 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 21:02:36,590 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 21:02:37,916 - logger.py:50 - Number of params: 2978805
2024-06-11 21:08:54,559 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 21:09:31,655 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 21:09:32,767 - logger.py:50 - Number of params: 2978805
2024-06-11 21:17:50,440 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 21:18:29,901 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 21:18:31,278 - logger.py:50 - Number of params: 2978805
2024-06-11 21:53:25,375 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-11 21:54:03,393 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-11 21:54:05,037 - logger.py:50 - Number of params: 2978805
2024-06-12 10:35:21,459 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=5, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 10:36:02,517 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 10:36:03,859 - logger.py:50 - Number of params: 2978805
2024-06-12 10:55:14,045 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=8, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 10:56:13,605 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 10:56:13,859 - logger.py:50 - Number of params: 2978805
2024-06-12 12:01:42,496 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:01:54,516 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:01:54,799 - logger.py:50 - Number of params: 2978805
2024-06-12 12:33:55,953 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:34:08,055 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:34:08,338 - logger.py:50 - Number of params: 2978805
2024-06-12 12:34:30,575 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:34:42,146 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:34:42,410 - logger.py:50 - Number of params: 2978805
2024-06-12 12:35:28,119 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:35:39,932 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:35:40,193 - logger.py:50 - Number of params: 2978805
2024-06-12 12:37:42,251 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:37:54,608 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:37:54,853 - logger.py:50 - Number of params: 2978805
2024-06-12 12:42:28,670 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:42:40,495 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:42:40,744 - logger.py:50 - Number of params: 2978805
2024-06-12 12:43:21,864 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:43:34,397 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:43:34,717 - logger.py:50 - Number of params: 2978805
2024-06-12 12:46:52,647 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:47:04,585 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:47:04,862 - logger.py:50 - Number of params: 2978805
2024-06-12 12:57:00,033 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:57:12,209 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:57:12,481 - logger.py:50 - Number of params: 2978805
2024-06-12 12:59:10,674 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 12:59:22,760 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 12:59:23,033 - logger.py:50 - Number of params: 2978805
2024-06-12 13:05:20,441 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 13:05:32,503 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 13:05:32,758 - logger.py:50 - Number of params: 2978805
2024-06-12 15:22:15,623 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:22:27,837 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:22:28,094 - logger.py:50 - Number of params: 2978805
2024-06-12 15:26:04,713 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:26:16,788 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:26:17,078 - logger.py:50 - Number of params: 2978805
2024-06-12 15:49:49,536 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:50:01,451 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:50:01,733 - logger.py:50 - Number of params: 2978805
2024-06-12 15:52:51,637 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:53:04,140 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:53:04,434 - logger.py:50 - Number of params: 2978805
2024-06-12 15:54:02,233 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=4, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:54:12,954 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:54:24,563 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:54:24,839 - logger.py:50 - Number of params: 2978805
2024-06-12 15:55:32,932 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:55:45,172 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:55:45,472 - logger.py:50 - Number of params: 2978805
2024-06-12 15:56:08,054 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 15:56:19,140 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 15:56:19,404 - logger.py:50 - Number of params: 2978805
2024-06-12 16:25:52,090 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 16:26:04,136 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 16:26:04,398 - logger.py:50 - Number of params: 2978805
2024-06-12 16:28:09,272 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 16:28:21,461 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 16:28:21,743 - logger.py:50 - Number of params: 2978805
2024-06-12 16:29:28,861 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 16:29:40,890 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 16:29:41,171 - logger.py:50 - Number of params: 2978805
2024-06-12 16:34:50,865 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 16:35:02,435 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 16:35:02,722 - logger.py:50 - Number of params: 2978805
2024-06-12 16:37:24,289 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-06-12 16:37:36,539 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-06-12 16:37:36,814 - logger.py:50 - Number of params: 2978805
2024-07-10 20:19:39,899 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-07-10 20:19:47,612 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-07-10 20:19:47,834 - logger.py:50 - Number of params: 2978805
2024-07-10 20:21:33,705 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=2, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-07-10 20:21:40,834 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-07-10 20:21:41,032 - logger.py:50 - Number of params: 2978805
2024-07-10 20:23:48,186 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-07-10 20:23:55,462 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-07-10 20:23:55,667 - logger.py:50 - Number of params: 2978805
2024-07-10 20:29:24,849 - logger.py:50 - Epoch: [0][0/9] loss: 1.14258, MAE: 0.19084, time/step=329177ms, lr=1.00e-06
2024-07-10 21:06:21,002 - logger.py:50 - Epoch: [0][8/9] loss: 1.17173, MAE: 0.20074, time/step=282814ms, lr=1.00e-06
2024-07-12 14:57:26,547 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-07-12 14:57:34,613 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-07-12 14:57:34,817 - logger.py:50 - Number of params: 2978805
2024-07-12 15:03:47,183 - logger.py:50 - Epoch: [0][0/9] loss: 1.14258, MAE: 0.19084, time/step=372360ms, lr=1.00e-06
2024-07-12 15:47:08,358 - logger.py:50 - Epoch: [0][8/9] loss: 1.17173, MAE: 0.20074, time/step=330393ms, lr=1.00e-06
2024-07-12 19:49:16,415 - logger.py:50 - Namespace(output_dir='models/ForceCon/der.log', model_name='graph_attention_transformer_nonlinear_l2_e3_noNorm_dx', input_irreps='86x0e', radius=8.0, num_basis=86, output_channels=1, epochs=150, batch_size=1, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, drop_path=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='cosine', lr=5e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, print_freq=50, target=0, data_path='datasets', run_fold=4, order_type='all', feature_type='one_hot', compute_stats=False, standardize=True, loss='l1', seed=0, workers=0, pin_mem=True, amp=False, world_size=1, dist_url='env://', distributed=False, rank=0, local_rank=0)
2024-07-12 19:49:24,243 - logger.py:50 - GraphAttentionTransformer_dx(
  (atom_embed): NodeEmbeddingNetwork(
    (atom_type_lin): LinearRS(
      (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
  )
  (rbf): GaussianRadialBasisLayer(mean_init_max=1.0, mean_init_min=0, std_init_max=1.0, std_init_min=0.011627906976744186)
  (edge_deg_embed): EdgeDegreeEmbeddingNetwork(
    (exp): LinearRS(
      (tp): TensorProduct(1x0e x 1x0e -> 128x0e+64x1e+32x2e | 128 paths | 128 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (dw): TensorProductRescale(
      (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
      (bias): ParameterList()
    )
    (rad): RadialProfile(
      (net): Sequential(
        (0): Linear(in_features=86, out_features=64, bias=True)
        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (2): SiLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (5): SiLU()
        (6): Linear(in_features=64, out_features=576, bias=False)
      )
    )
    (proj): LinearRS(
      (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
    )
    (scale_scatter): ScaledScatter(avg_aggregate_num=34.29242574467496)
  )
  (blocks): ModuleList(
    (0-4): 5 x TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 128x0e+64x1e+32x2e | 64512 paths | 64512 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
    )
    (5): TransBlock(
      (ga): GraphAttention(
        rescale_degree=False, 
        (merge_src): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (merge_dst): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList()
        )
        (sep_act): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (dtp_rad): RadialProfile(
            (net): Sequential(
              (0): Linear(in_features=86, out_features=64, bias=True)
              (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (2): SiLU()
              (3): Linear(in_features=64, out_features=64, bias=True)
              (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
              (5): SiLU()
              (6): Linear(in_features=64, out_features=576, bias=False)
            )
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 224x0e+64x1e+32x2e | 54272 paths | 54272 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 224])
          )
          (gate): Gate (224x0e+64x1e+32x2e -> 128x0e+64x1e+32x2e)
        )
        (sep_alpha): LinearRS(
          (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e | 20480 paths | 20480 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
        (sep_value): SeparableFCTP(
          (dtp): TensorProductRescale(
            (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e+1x1o+1x2e -> 160x0e+160x1e+256x2e | 576 paths | 576 weights)
            (bias): ParameterList()
          )
          (lin): LinearRS(
            (tp): TensorProduct(160x0e+160x1e+256x2e x 1x0e -> 128x0e+64x1e+32x2e | 38912 paths | 38912 weights)
            (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
          )
        )
        (vec2heads_alpha): Vec2AttnHeads(irreps_head=16x0e, num_heads=8)
        (vec2heads_value): Vec2AttnHeads(irreps_head=16x0e+8x1e+4x2e, num_heads=8)
        (alpha_act): Activation(
          16x0e -> 16x0e, 
          (acts): ModuleList(
            (0): normalize2mom(
              (f): SmoothLeakyReLU(negative_slope=0.2)
            )
          )
        )
        (heads2vec): AttnHeads2Vec(irreps_head=16x0e+8x1e+4x2e)
        (alpha_dropout): Dropout(p=0.2, inplace=False)
        (proj): LinearRS(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 128x0e+64x1e+32x2e | 21504 paths | 21504 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
        )
      )
      (ffn): FeedForwardNetwork(
        (fctp_1): FullyConnectedTensorProductRescaleSwishGate(
          (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 672x0e+192x1e+96x2e | 101376 paths | 101376 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 672])
          (gate): Gate (672x0e+192x1e+96x2e -> 384x0e+192x1e+96x2e)
        )
        (fctp_2): FullyConnectedTensorProductRescale(
          (tp): TensorProduct(384x0e+192x1e+96x2e x 1x0e -> 512x0e | 196608 paths | 196608 weights)
          (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
        )
      )
      (ffn_shortcut): FullyConnectedTensorProductRescale(
        (tp): TensorProduct(128x0e+64x1e+32x2e x 1x0e -> 512x0e | 65536 paths | 65536 weights)
        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
      )
    )
  )
  (norm): EquivariantLayerNormV2(512x0e, eps=1e-05)
  (head): Sequential(
    (0): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 512x0e | 262144 paths | 262144 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
    )
    (1): Activation(
      512x0e -> 512x0e, 
      (acts): ModuleList(
        (0): normalize2mom(
          (f): SiLU()
        )
      )
    )
    (2): LinearRS(
      (tp): TensorProduct(512x0e x 1x0e -> 1x0e | 512 paths | 512 weights)
      (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])
    )
  )
  (lrs): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 512x0e | 44032 paths | 44032 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])
  )
  (scale_scatter): ScaledScatter(avg_aggregate_num=29.891087392943284)
  (atom_expand): LinearRS(
    (tp): TensorProduct(86x0e x 1x0e -> 128x0e+64x1e+32x2e | 11008 paths | 11008 weights)
    (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 128])
  )
)
2024-07-12 19:49:24,471 - logger.py:50 - Number of params: 2978805
2024-07-12 19:56:05,850 - logger.py:50 - Epoch: [0][0/9] loss: 1.14258, MAE: 0.19084, time/step=401373ms, lr=1.00e-06
2024-07-12 20:39:41,085 - logger.py:50 - Epoch: [0][8/9] loss: 1.17173, MAE: 0.20074, time/step=335179ms, lr=1.00e-06
2024-07-12 20:48:06,654 - logger.py:50 - Epoch: [0] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3522.18s
2024-07-12 20:48:06,655 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-12 20:53:06,629 - logger.py:50 - Epoch: [1][0/9] loss: 1.35237, MAE: 0.27714, time/step=299970ms, lr=1.08e-05
2024-07-12 21:37:59,130 - logger.py:50 - Epoch: [1][8/9] loss: 1.17173, MAE: 0.20074, time/step=332497ms, lr=1.08e-05
2024-07-12 21:46:18,398 - logger.py:50 - Epoch: [1] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3491.74s
2024-07-12 21:46:18,400 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-12 21:49:10,299 - logger.py:50 - Epoch: [2][0/9] loss: 1.37256, MAE: 0.36221, time/step=171886ms, lr=2.06e-05
2024-07-12 22:36:27,655 - logger.py:50 - Epoch: [2][8/9] loss: 1.17173, MAE: 0.20074, time/step=334360ms, lr=2.06e-05
2024-07-12 22:44:46,233 - logger.py:50 - Epoch: [2] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3507.83s
2024-07-12 22:44:46,234 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-12 22:47:29,498 - logger.py:50 - Epoch: [3][0/9] loss: 0.95472, MAE: 0.32155, time/step=163249ms, lr=3.04e-05
2024-07-12 23:34:52,081 - logger.py:50 - Epoch: [3][8/9] loss: 1.17173, MAE: 0.20074, time/step=333981ms, lr=3.04e-05
2024-07-12 23:43:12,247 - logger.py:50 - Epoch: [3] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3506.00s
2024-07-12 23:43:12,248 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-12 23:45:54,846 - logger.py:50 - Epoch: [4][0/9] loss: 1.37256, MAE: 0.36221, time/step=162589ms, lr=4.02e-05
2024-07-13 00:34:04,922 - logger.py:50 - Epoch: [4][8/9] loss: 1.17173, MAE: 0.20074, time/step=339185ms, lr=4.02e-05
2024-07-13 00:42:38,669 - logger.py:50 - Epoch: [4] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3566.42s
2024-07-13 00:42:38,671 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 00:48:32,271 - logger.py:50 - Epoch: [5][0/9] loss: 0.21769, MAE: 0.09627, time/step=353591ms, lr=4.99e-05
2024-07-13 01:33:16,299 - logger.py:50 - Epoch: [5][8/9] loss: 1.17173, MAE: 0.20074, time/step=337513ms, lr=4.99e-05
2024-07-13 01:41:41,848 - logger.py:50 - Epoch: [5] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3543.18s
2024-07-13 01:41:41,849 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 01:50:00,142 - logger.py:50 - Epoch: [6][0/9] loss: 2.96458, MAE: 0.25491, time/step=498279ms, lr=4.98e-05
2024-07-13 02:33:08,409 - logger.py:50 - Epoch: [6][8/9] loss: 1.17173, MAE: 0.20074, time/step=342950ms, lr=4.98e-05
2024-07-13 02:42:11,118 - logger.py:50 - Epoch: [6] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3629.26s
2024-07-13 02:42:11,119 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 02:44:54,677 - logger.py:50 - Epoch: [7][0/9] loss: 1.37256, MAE: 0.36221, time/step=163542ms, lr=4.97e-05
2024-07-13 03:32:07,238 - logger.py:50 - Epoch: [7][8/9] loss: 1.17173, MAE: 0.20074, time/step=332900ms, lr=4.97e-05
2024-07-13 03:40:32,970 - logger.py:50 - Epoch: [7] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3501.84s
2024-07-13 03:40:32,971 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 03:43:24,381 - logger.py:50 - Epoch: [8][0/9] loss: 1.37256, MAE: 0.36221, time/step=171401ms, lr=4.97e-05
2024-07-13 04:31:03,634 - logger.py:50 - Epoch: [8][8/9] loss: 1.17173, MAE: 0.20074, time/step=336739ms, lr=4.97e-05
2024-07-13 04:39:30,941 - logger.py:50 - Epoch: [8] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3537.97s
2024-07-13 04:39:30,942 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 04:43:18,479 - logger.py:50 - Epoch: [9][0/9] loss: 0.09148, MAE: 0.12241, time/step=227523ms, lr=4.96e-05
2024-07-13 05:30:43,423 - logger.py:50 - Epoch: [9][8/9] loss: 1.17173, MAE: 0.20074, time/step=341385ms, lr=4.96e-05
2024-07-13 05:39:12,048 - logger.py:50 - Epoch: [9] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3581.10s
2024-07-13 05:39:12,050 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 05:44:15,625 - logger.py:50 - Epoch: [10][0/9] loss: 1.35237, MAE: 0.27714, time/step=303559ms, lr=4.95e-05
2024-07-13 06:29:36,421 - logger.py:50 - Epoch: [10][8/9] loss: 1.17173, MAE: 0.20074, time/step=336040ms, lr=4.95e-05
2024-07-13 06:38:07,908 - logger.py:50 - Epoch: [10] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3535.85s
2024-07-13 06:38:07,909 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 06:43:19,188 - logger.py:50 - Epoch: [11][0/9] loss: 1.35237, MAE: 0.27714, time/step=311266ms, lr=4.94e-05
2024-07-13 07:30:46,020 - logger.py:50 - Epoch: [11][8/9] loss: 1.17173, MAE: 0.20074, time/step=350900ms, lr=4.94e-05
2024-07-13 07:39:16,477 - logger.py:50 - Epoch: [11] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3668.56s
2024-07-13 07:39:16,478 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 07:47:23,468 - logger.py:50 - Epoch: [12][0/9] loss: 2.96458, MAE: 0.25491, time/step=486981ms, lr=4.92e-05
2024-07-13 08:31:03,171 - logger.py:50 - Epoch: [12][8/9] loss: 1.17173, MAE: 0.20074, time/step=345187ms, lr=4.92e-05
2024-07-13 08:39:36,892 - logger.py:50 - Epoch: [12] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3620.41s
2024-07-13 08:39:36,893 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 08:43:30,032 - logger.py:50 - Epoch: [13][0/9] loss: 0.09148, MAE: 0.12241, time/step=233127ms, lr=4.91e-05
2024-07-13 09:31:01,240 - logger.py:50 - Epoch: [13][8/9] loss: 1.17173, MAE: 0.20074, time/step=342704ms, lr=4.91e-05
2024-07-13 09:39:34,323 - logger.py:50 - Epoch: [13] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3597.43s
2024-07-13 09:39:34,325 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 09:43:27,853 - logger.py:50 - Epoch: [14][0/9] loss: 0.09148, MAE: 0.12241, time/step=233516ms, lr=4.90e-05
2024-07-13 10:31:05,209 - logger.py:50 - Epoch: [14][8/9] loss: 1.17173, MAE: 0.20074, time/step=343430ms, lr=4.90e-05
2024-07-13 10:39:54,149 - logger.py:50 - Epoch: [14] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3619.82s
2024-07-13 10:39:54,151 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 10:45:55,486 - logger.py:50 - Epoch: [15][0/9] loss: 0.21769, MAE: 0.09627, time/step=361328ms, lr=4.88e-05
2024-07-13 11:32:04,642 - logger.py:50 - Epoch: [15][8/9] loss: 1.17173, MAE: 0.20074, time/step=347832ms, lr=4.88e-05
2024-07-13 11:40:47,275 - logger.py:50 - Epoch: [15] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3653.12s
2024-07-13 11:40:47,276 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 11:47:33,424 - logger.py:50 - Epoch: [16][0/9] loss: 1.14258, MAE: 0.19084, time/step=406134ms, lr=4.86e-05
2024-07-13 12:32:18,866 - logger.py:50 - Epoch: [16][8/9] loss: 1.17173, MAE: 0.20074, time/step=343508ms, lr=4.86e-05
2024-07-13 12:40:56,922 - logger.py:50 - Epoch: [16] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3609.64s
2024-07-13 12:40:56,924 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 12:43:45,970 - logger.py:50 - Epoch: [17][0/9] loss: 1.37256, MAE: 0.36221, time/step=169033ms, lr=4.85e-05
2024-07-13 13:34:28,451 - logger.py:50 - Epoch: [17][8/9] loss: 1.17173, MAE: 0.20074, time/step=356835ms, lr=4.85e-05
2024-07-13 13:43:21,136 - logger.py:50 - Epoch: [17] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3744.21s
2024-07-13 13:43:21,138 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 13:50:15,957 - logger.py:50 - Epoch: [18][0/9] loss: 1.14258, MAE: 0.19084, time/step=414809ms, lr=4.83e-05
2024-07-13 14:36:47,132 - logger.py:50 - Epoch: [18][8/9] loss: 1.17173, MAE: 0.20074, time/step=356221ms, lr=4.83e-05
2024-07-13 14:45:30,836 - logger.py:50 - Epoch: [18] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3729.70s
2024-07-13 14:45:30,837 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 14:49:25,888 - logger.py:50 - Epoch: [19][0/9] loss: 0.09148, MAE: 0.12241, time/step=235041ms, lr=4.81e-05
2024-07-13 15:38:28,445 - logger.py:50 - Epoch: [19][8/9] loss: 1.17173, MAE: 0.20074, time/step=353067ms, lr=4.81e-05
2024-07-13 15:47:23,765 - logger.py:50 - Epoch: [19] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3712.93s
2024-07-13 15:47:23,766 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 15:56:25,883 - logger.py:50 - Epoch: [20][0/9] loss: 2.96458, MAE: 0.25491, time/step=542106ms, lr=4.79e-05
2024-07-13 16:40:38,599 - logger.py:50 - Epoch: [20][8/9] loss: 1.17173, MAE: 0.20074, time/step=354980ms, lr=4.79e-05
2024-07-13 16:50:03,966 - logger.py:50 - Epoch: [20] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3760.20s
2024-07-13 16:50:03,967 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 16:56:10,574 - logger.py:50 - Epoch: [21][0/9] loss: 0.02596, MAE: 0.04593, time/step=366593ms, lr=4.77e-05
2024-07-13 17:42:31,182 - logger.py:50 - Epoch: [21][8/9] loss: 1.17173, MAE: 0.20074, time/step=349689ms, lr=4.77e-05
2024-07-13 17:51:19,483 - logger.py:50 - Epoch: [21] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3675.51s
2024-07-13 17:51:19,484 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 17:54:13,316 - logger.py:50 - Epoch: [22][0/9] loss: 0.95472, MAE: 0.32155, time/step=173815ms, lr=4.74e-05
2024-07-13 18:44:18,365 - logger.py:50 - Epoch: [22][8/9] loss: 1.17173, MAE: 0.20074, time/step=353207ms, lr=4.74e-05
2024-07-13 18:53:13,538 - logger.py:50 - Epoch: [22] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3714.05s
2024-07-13 18:53:13,539 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 18:57:09,418 - logger.py:50 - Epoch: [23][0/9] loss: 0.09148, MAE: 0.12241, time/step=235866ms, lr=4.72e-05
2024-07-13 19:46:09,297 - logger.py:50 - Epoch: [23][8/9] loss: 1.17173, MAE: 0.20074, time/step=352861ms, lr=4.72e-05
2024-07-13 19:55:18,781 - logger.py:50 - Epoch: [23] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3725.24s
2024-07-13 19:55:18,783 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 19:59:16,676 - logger.py:50 - Epoch: [24][0/9] loss: 0.09148, MAE: 0.12241, time/step=237878ms, lr=4.70e-05
2024-07-13 20:48:50,397 - logger.py:50 - Epoch: [24][8/9] loss: 1.17173, MAE: 0.20074, time/step=356844ms, lr=4.70e-05
2024-07-13 20:58:07,169 - logger.py:50 - Epoch: [24] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3768.38s
2024-07-13 20:58:07,170 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 21:03:26,974 - logger.py:50 - Epoch: [25][0/9] loss: 1.35237, MAE: 0.27714, time/step=319790ms, lr=4.67e-05
2024-07-13 21:52:18,462 - logger.py:50 - Epoch: [25][8/9] loss: 1.17173, MAE: 0.20074, time/step=361253ms, lr=4.67e-05
2024-07-13 22:01:19,129 - logger.py:50 - Epoch: [25] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3791.95s
2024-07-13 22:01:19,131 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 22:08:08,679 - logger.py:50 - Epoch: [26][0/9] loss: 1.14258, MAE: 0.19084, time/step=409533ms, lr=4.65e-05
2024-07-13 22:54:25,120 - logger.py:50 - Epoch: [26][8/9] loss: 1.17173, MAE: 0.20074, time/step=353997ms, lr=4.65e-05
2024-07-13 23:03:36,211 - logger.py:50 - Epoch: [26] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3737.08s
2024-07-13 23:03:36,213 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-13 23:06:34,851 - logger.py:50 - Epoch: [27][0/9] loss: 0.95472, MAE: 0.32155, time/step=178628ms, lr=4.62e-05
2024-07-13 23:58:07,456 - logger.py:50 - Epoch: [27][8/9] loss: 1.17173, MAE: 0.20074, time/step=363470ms, lr=4.62e-05
2024-07-14 00:06:59,021 - logger.py:50 - Epoch: [27] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3802.81s
2024-07-14 00:06:59,023 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 00:15:22,763 - logger.py:50 - Epoch: [28][0/9] loss: 2.96458, MAE: 0.25491, time/step=503731ms, lr=4.59e-05
2024-07-14 01:00:15,447 - logger.py:50 - Epoch: [28][8/9] loss: 1.17173, MAE: 0.20074, time/step=355157ms, lr=4.59e-05
2024-07-14 01:09:32,311 - logger.py:50 - Epoch: [28] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3753.29s
2024-07-14 01:09:32,313 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 01:18:37,466 - logger.py:50 - Epoch: [29][0/9] loss: 2.96458, MAE: 0.25491, time/step=545144ms, lr=4.56e-05
2024-07-14 02:04:12,003 - logger.py:50 - Epoch: [29][8/9] loss: 1.17173, MAE: 0.20074, time/step=364409ms, lr=4.56e-05
2024-07-14 02:13:10,522 - logger.py:50 - Epoch: [29] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3818.21s
2024-07-14 02:13:10,523 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 02:19:29,415 - logger.py:50 - Epoch: [30][0/9] loss: 0.21769, MAE: 0.09627, time/step=378882ms, lr=4.53e-05
2024-07-14 03:07:29,603 - logger.py:50 - Epoch: [30][8/9] loss: 1.17173, MAE: 0.20074, time/step=362119ms, lr=4.53e-05
2024-07-14 03:16:38,907 - logger.py:50 - Epoch: [30] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3808.38s
2024-07-14 03:16:38,908 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 03:23:33,912 - logger.py:50 - Epoch: [31][0/9] loss: 0.21769, MAE: 0.09627, time/step=414996ms, lr=4.50e-05
2024-07-14 04:11:23,183 - logger.py:50 - Epoch: [31][8/9] loss: 1.17173, MAE: 0.20074, time/step=364918ms, lr=4.50e-05
2024-07-14 04:20:48,149 - logger.py:50 - Epoch: [31] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3849.24s
2024-07-14 04:20:48,150 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 04:29:29,051 - logger.py:50 - Epoch: [32][0/9] loss: 1.41347, MAE: 0.25817, time/step=520891ms, lr=4.47e-05
2024-07-14 05:15:24,228 - logger.py:50 - Epoch: [32][8/9] loss: 1.17173, MAE: 0.20074, time/step=364008ms, lr=4.47e-05
2024-07-14 05:24:48,441 - logger.py:50 - Epoch: [32] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3840.29s
2024-07-14 05:24:48,442 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 05:27:46,057 - logger.py:50 - Epoch: [33][0/9] loss: 0.95472, MAE: 0.32155, time/step=177605ms, lr=4.44e-05
2024-07-14 06:19:48,104 - logger.py:50 - Epoch: [33][8/9] loss: 1.17173, MAE: 0.20074, time/step=366628ms, lr=4.44e-05
2024-07-14 06:29:06,010 - logger.py:50 - Epoch: [33] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3857.57s
2024-07-14 06:29:06,011 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 06:35:26,541 - logger.py:50 - Epoch: [34][0/9] loss: 0.21769, MAE: 0.09627, time/step=380521ms, lr=4.40e-05
2024-07-14 07:23:27,993 - logger.py:50 - Epoch: [34][8/9] loss: 1.17173, MAE: 0.20074, time/step=362441ms, lr=4.40e-05
2024-07-14 07:32:31,124 - logger.py:50 - Epoch: [34] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3805.11s
2024-07-14 07:32:31,126 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 07:39:33,192 - logger.py:50 - Epoch: [35][0/9] loss: 1.14258, MAE: 0.19084, time/step=422051ms, lr=4.37e-05
2024-07-14 08:27:29,427 - logger.py:50 - Epoch: [35][8/9] loss: 1.17173, MAE: 0.20074, time/step=366476ms, lr=4.37e-05
2024-07-14 08:36:35,939 - logger.py:50 - Epoch: [35] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3844.81s
2024-07-14 08:36:35,941 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 08:46:01,313 - logger.py:50 - Epoch: [36][0/9] loss: 2.96458, MAE: 0.25491, time/step=565358ms, lr=4.34e-05
2024-07-14 09:32:50,159 - logger.py:50 - Epoch: [36][8/9] loss: 1.17173, MAE: 0.20074, time/step=374912ms, lr=4.34e-05
2024-07-14 09:41:57,847 - logger.py:50 - Epoch: [36] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3921.90s
2024-07-14 09:41:57,849 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 09:50:45,859 - logger.py:50 - Epoch: [37][0/9] loss: 2.96458, MAE: 0.25491, time/step=528000ms, lr=4.30e-05
2024-07-14 10:37:28,553 - logger.py:50 - Epoch: [37][8/9] loss: 1.17173, MAE: 0.20074, time/step=370077ms, lr=4.30e-05
2024-07-14 10:46:44,044 - logger.py:50 - Epoch: [37] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3886.19s
2024-07-14 10:46:44,046 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 10:55:26,370 - logger.py:50 - Epoch: [38][0/9] loss: 2.96458, MAE: 0.25491, time/step=522315ms, lr=4.26e-05
2024-07-14 11:42:13,092 - logger.py:50 - Epoch: [38][8/9] loss: 1.17173, MAE: 0.20074, time/step=369893ms, lr=4.26e-05
2024-07-14 11:51:33,274 - logger.py:50 - Epoch: [38] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3889.23s
2024-07-14 11:51:33,276 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 11:57:59,225 - logger.py:50 - Epoch: [39][0/9] loss: 0.21769, MAE: 0.09627, time/step=385939ms, lr=4.23e-05
2024-07-14 12:46:18,546 - logger.py:50 - Epoch: [39][8/9] loss: 1.17173, MAE: 0.20074, time/step=365029ms, lr=4.23e-05
2024-07-14 12:55:40,766 - logger.py:50 - Epoch: [39] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3847.49s
2024-07-14 12:55:40,767 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 13:04:42,425 - logger.py:50 - Epoch: [40][0/9] loss: 2.96458, MAE: 0.25491, time/step=541648ms, lr=4.19e-05
2024-07-14 13:52:11,597 - logger.py:50 - Epoch: [40][8/9] loss: 1.17173, MAE: 0.20074, time/step=376758ms, lr=4.19e-05
2024-07-14 14:01:36,213 - logger.py:50 - Epoch: [40] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3955.44s
2024-07-14 14:01:36,214 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 14:08:49,899 - logger.py:50 - Epoch: [41][0/9] loss: 1.14258, MAE: 0.19084, time/step=433675ms, lr=4.15e-05
2024-07-14 14:57:27,215 - logger.py:50 - Epoch: [41][8/9] loss: 1.17173, MAE: 0.20074, time/step=372332ms, lr=4.15e-05
2024-07-14 15:06:50,340 - logger.py:50 - Epoch: [41] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3914.12s
2024-07-14 15:06:50,342 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 15:14:07,434 - logger.py:50 - Epoch: [42][0/9] loss: 1.14258, MAE: 0.19084, time/step=437079ms, lr=4.11e-05
2024-07-14 16:03:50,302 - logger.py:50 - Epoch: [42][8/9] loss: 1.17173, MAE: 0.20074, time/step=379994ms, lr=4.11e-05
2024-07-14 16:13:25,684 - logger.py:50 - Epoch: [42] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3995.34s
2024-07-14 16:13:25,686 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 16:19:09,491 - logger.py:50 - Epoch: [43][0/9] loss: 1.35237, MAE: 0.27714, time/step=343795ms, lr=4.07e-05
2024-07-14 17:11:39,884 - logger.py:50 - Epoch: [43][8/9] loss: 1.17173, MAE: 0.20074, time/step=388243ms, lr=4.07e-05
2024-07-14 17:21:15,907 - logger.py:50 - Epoch: [43] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4070.22s
2024-07-14 17:21:15,909 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 17:24:23,924 - logger.py:50 - Epoch: [44][0/9] loss: 0.95472, MAE: 0.32155, time/step=188006ms, lr=4.03e-05
2024-07-14 18:18:10,760 - logger.py:50 - Epoch: [44][8/9] loss: 1.17173, MAE: 0.20074, time/step=379427ms, lr=4.03e-05
2024-07-14 18:27:42,930 - logger.py:50 - Epoch: [44] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 3987.02s
2024-07-14 18:27:42,932 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 18:36:42,935 - logger.py:50 - Epoch: [45][0/9] loss: 2.96458, MAE: 0.25491, time/step=539991ms, lr=3.99e-05
2024-07-14 19:24:47,736 - logger.py:50 - Epoch: [45][8/9] loss: 1.17173, MAE: 0.20074, time/step=380532ms, lr=3.99e-05
2024-07-14 19:34:24,012 - logger.py:50 - Epoch: [45] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4001.07s
2024-07-14 19:34:24,013 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 19:41:03,067 - logger.py:50 - Epoch: [46][0/9] loss: 0.02596, MAE: 0.04593, time/step=399043ms, lr=3.95e-05
2024-07-14 20:32:04,191 - logger.py:50 - Epoch: [46][8/9] loss: 1.17173, MAE: 0.20074, time/step=384463ms, lr=3.95e-05
2024-07-14 20:42:01,356 - logger.py:50 - Epoch: [46] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4057.34s
2024-07-14 20:42:01,357 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 20:51:21,332 - logger.py:50 - Epoch: [47][0/9] loss: 1.41347, MAE: 0.25817, time/step=559963ms, lr=3.91e-05
2024-07-14 21:40:41,020 - logger.py:50 - Epoch: [47][8/9] loss: 1.17173, MAE: 0.20074, time/step=391072ms, lr=3.91e-05
2024-07-14 21:50:26,550 - logger.py:50 - Epoch: [47] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4105.19s
2024-07-14 21:50:26,552 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 21:53:38,986 - logger.py:50 - Epoch: [48][0/9] loss: 0.95472, MAE: 0.32155, time/step=192421ms, lr=3.86e-05
2024-07-14 22:48:37,575 - logger.py:50 - Epoch: [48][8/9] loss: 1.17173, MAE: 0.20074, time/step=387890ms, lr=3.86e-05
2024-07-14 22:58:40,734 - logger.py:50 - Epoch: [48] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4094.18s
2024-07-14 22:58:40,735 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-14 23:02:59,854 - logger.py:50 - Epoch: [49][0/9] loss: 0.09148, MAE: 0.12241, time/step=259105ms, lr=3.82e-05
2024-07-14 23:56:39,098 - logger.py:50 - Epoch: [49][8/9] loss: 1.17173, MAE: 0.20074, time/step=386483ms, lr=3.82e-05
2024-07-15 00:06:31,304 - logger.py:50 - Epoch: [49] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4070.56s
2024-07-15 00:06:31,306 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 00:12:24,675 - logger.py:50 - Epoch: [50][0/9] loss: 1.35237, MAE: 0.27714, time/step=353359ms, lr=3.78e-05
2024-07-15 01:04:11,450 - logger.py:50 - Epoch: [50][8/9] loss: 1.17173, MAE: 0.20074, time/step=384459ms, lr=3.78e-05
2024-07-15 01:13:54,578 - logger.py:50 - Epoch: [50] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4043.27s
2024-07-15 01:13:54,579 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 01:17:03,460 - logger.py:50 - Epoch: [51][0/9] loss: 1.37256, MAE: 0.36221, time/step=188865ms, lr=3.73e-05
2024-07-15 02:12:25,073 - logger.py:50 - Epoch: [51][8/9] loss: 1.17173, MAE: 0.20074, time/step=390053ms, lr=3.73e-05
2024-07-15 02:22:13,479 - logger.py:50 - Epoch: [51] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4098.89s
2024-07-15 02:22:13,480 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 02:31:46,416 - logger.py:50 - Epoch: [52][0/9] loss: 2.96458, MAE: 0.25491, time/step=572925ms, lr=3.69e-05
2024-07-15 03:21:19,929 - logger.py:50 - Epoch: [52][8/9] loss: 1.17173, MAE: 0.20074, time/step=394049ms, lr=3.69e-05
2024-07-15 03:31:23,165 - logger.py:50 - Epoch: [52] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4149.68s
2024-07-15 03:31:23,167 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 03:40:59,213 - logger.py:50 - Epoch: [53][0/9] loss: 1.41347, MAE: 0.25817, time/step=576034ms, lr=3.64e-05
2024-07-15 04:30:25,914 - logger.py:50 - Epoch: [53][8/9] loss: 1.17173, MAE: 0.20074, time/step=393637ms, lr=3.64e-05
2024-07-15 04:40:16,249 - logger.py:50 - Epoch: [53] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4133.08s
2024-07-15 04:40:16,251 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 04:49:47,333 - logger.py:50 - Epoch: [54][0/9] loss: 2.96458, MAE: 0.25491, time/step=571068ms, lr=3.59e-05
2024-07-15 05:39:35,481 - logger.py:50 - Epoch: [54][8/9] loss: 1.17173, MAE: 0.20074, time/step=395469ms, lr=3.59e-05
2024-07-15 05:49:31,358 - logger.py:50 - Epoch: [54] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4155.10s
2024-07-15 05:49:31,359 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 05:55:30,402 - logger.py:50 - Epoch: [55][0/9] loss: 1.35237, MAE: 0.27714, time/step=359028ms, lr=3.55e-05
2024-07-15 06:49:16,636 - logger.py:50 - Epoch: [55][8/9] loss: 1.17173, MAE: 0.20074, time/step=398363ms, lr=3.55e-05
2024-07-15 06:59:26,328 - logger.py:50 - Epoch: [55] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4194.96s
2024-07-15 06:59:26,329 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 07:03:58,113 - logger.py:50 - Epoch: [56][0/9] loss: 0.09148, MAE: 0.12241, time/step=271774ms, lr=3.50e-05
2024-07-15 08:00:33,208 - logger.py:50 - Epoch: [56][8/9] loss: 1.17173, MAE: 0.20074, time/step=407430ms, lr=3.50e-05
2024-07-15 08:10:50,015 - logger.py:50 - Epoch: [56] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4283.68s
2024-07-15 08:10:50,017 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 08:15:25,777 - logger.py:50 - Epoch: [57][0/9] loss: 0.09148, MAE: 0.12241, time/step=275750ms, lr=3.45e-05
2024-07-15 09:12:05,508 - logger.py:50 - Epoch: [57][8/9] loss: 1.17173, MAE: 0.20074, time/step=408387ms, lr=3.45e-05
2024-07-15 09:22:16,938 - logger.py:50 - Epoch: [57] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4286.92s
2024-07-15 09:22:16,940 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 09:26:54,012 - logger.py:50 - Epoch: [58][0/9] loss: 0.09148, MAE: 0.12241, time/step=277060ms, lr=3.40e-05
2024-07-15 10:23:12,569 - logger.py:50 - Epoch: [58][8/9] loss: 1.17173, MAE: 0.20074, time/step=406180ms, lr=3.40e-05
2024-07-15 10:33:14,993 - logger.py:50 - Epoch: [58] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4258.05s
2024-07-15 10:33:14,994 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 10:40:59,444 - logger.py:50 - Epoch: [59][0/9] loss: 1.14258, MAE: 0.19084, time/step=464440ms, lr=3.36e-05
2024-07-15 11:33:18,289 - logger.py:50 - Epoch: [59][8/9] loss: 1.17173, MAE: 0.20074, time/step=400365ms, lr=3.36e-05
2024-07-15 11:43:23,224 - logger.py:50 - Epoch: [59] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4208.23s
2024-07-15 11:43:23,225 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 11:50:34,549 - logger.py:50 - Epoch: [60][0/9] loss: 0.21769, MAE: 0.09627, time/step=431314ms, lr=3.31e-05
2024-07-15 12:44:34,349 - logger.py:50 - Epoch: [60][8/9] loss: 1.17173, MAE: 0.20074, time/step=407902ms, lr=3.31e-05
2024-07-15 12:55:11,076 - logger.py:50 - Epoch: [60] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4307.85s
2024-07-15 12:55:11,077 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 13:01:30,897 - logger.py:50 - Epoch: [61][0/9] loss: 1.35237, MAE: 0.27714, time/step=379807ms, lr=3.26e-05
2024-07-15 13:57:02,739 - logger.py:50 - Epoch: [61][8/9] loss: 1.17173, MAE: 0.20074, time/step=412406ms, lr=3.26e-05
2024-07-15 14:07:21,262 - logger.py:50 - Epoch: [61] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4330.18s
2024-07-15 14:07:21,263 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 14:17:31,935 - logger.py:50 - Epoch: [62][0/9] loss: 1.41347, MAE: 0.25817, time/step=610658ms, lr=3.21e-05
2024-07-15 15:09:33,432 - logger.py:50 - Epoch: [62][8/9] loss: 1.17173, MAE: 0.20074, time/step=414684ms, lr=3.21e-05
2024-07-15 15:19:59,483 - logger.py:50 - Epoch: [62] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4358.21s
2024-07-15 15:19:59,485 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 15:29:56,949 - logger.py:50 - Epoch: [63][0/9] loss: 1.41347, MAE: 0.25817, time/step=597454ms, lr=3.16e-05
2024-07-15 16:22:14,018 - logger.py:50 - Epoch: [63][8/9] loss: 1.17173, MAE: 0.20074, time/step=414947ms, lr=3.16e-05
2024-07-15 16:32:46,409 - logger.py:50 - Epoch: [63] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4366.92s
2024-07-15 16:32:46,410 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 16:42:38,098 - logger.py:50 - Epoch: [64][0/9] loss: 1.41347, MAE: 0.25817, time/step=591673ms, lr=3.11e-05
2024-07-15 17:35:31,807 - logger.py:50 - Epoch: [64][8/9] loss: 1.17173, MAE: 0.20074, time/step=418376ms, lr=3.11e-05
2024-07-15 17:46:05,474 - logger.py:50 - Epoch: [64] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4399.06s
2024-07-15 17:46:05,476 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 17:53:14,001 - logger.py:50 - Epoch: [65][0/9] loss: 0.02596, MAE: 0.04593, time/step=428515ms, lr=3.06e-05
2024-07-15 18:48:14,804 - logger.py:50 - Epoch: [65][8/9] loss: 1.17173, MAE: 0.20074, time/step=414369ms, lr=3.06e-05
2024-07-15 18:58:49,828 - logger.py:50 - Epoch: [65] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4364.35s
2024-07-15 18:58:49,829 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 19:03:31,743 - logger.py:50 - Epoch: [66][0/9] loss: 0.09148, MAE: 0.12241, time/step=281905ms, lr=3.01e-05
2024-07-15 20:01:59,242 - logger.py:50 - Epoch: [66][8/9] loss: 1.17173, MAE: 0.20074, time/step=421045ms, lr=3.01e-05
2024-07-15 20:12:37,687 - logger.py:50 - Epoch: [66] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4427.86s
2024-07-15 20:12:37,689 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 20:20:43,988 - logger.py:50 - Epoch: [67][0/9] loss: 1.14258, MAE: 0.19084, time/step=486284ms, lr=2.96e-05
2024-07-15 21:15:33,615 - logger.py:50 - Epoch: [67][8/9] loss: 1.17173, MAE: 0.20074, time/step=419546ms, lr=2.96e-05
2024-07-15 21:26:11,908 - logger.py:50 - Epoch: [67] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4414.21s
2024-07-15 21:26:11,910 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 21:31:04,134 - logger.py:50 - Epoch: [68][0/9] loss: 0.09148, MAE: 0.12241, time/step=292210ms, lr=2.91e-05
2024-07-15 22:29:13,039 - logger.py:50 - Epoch: [68][8/9] loss: 1.17173, MAE: 0.20074, time/step=420124ms, lr=2.91e-05
2024-07-15 22:39:58,223 - logger.py:50 - Epoch: [68] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4426.31s
2024-07-15 22:39:58,224 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-15 22:48:07,925 - logger.py:50 - Epoch: [69][0/9] loss: 1.14258, MAE: 0.19084, time/step=489687ms, lr=2.86e-05
2024-07-15 23:43:07,780 - logger.py:50 - Epoch: [69][8/9] loss: 1.17173, MAE: 0.20074, time/step=421060ms, lr=2.86e-05
2024-07-15 23:53:55,101 - logger.py:50 - Epoch: [69] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4436.87s
2024-07-15 23:53:55,103 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 00:01:24,596 - logger.py:50 - Epoch: [70][0/9] loss: 0.21769, MAE: 0.09627, time/step=449484ms, lr=2.81e-05
2024-07-16 00:57:34,772 - logger.py:50 - Epoch: [70][8/9] loss: 1.17173, MAE: 0.20074, time/step=424407ms, lr=2.81e-05
2024-07-16 01:08:25,084 - logger.py:50 - Epoch: [70] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4469.98s
2024-07-16 01:08:25,085 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 01:15:49,685 - logger.py:50 - Epoch: [71][0/9] loss: 0.02596, MAE: 0.04593, time/step=444589ms, lr=2.76e-05
2024-07-16 02:12:25,909 - logger.py:50 - Epoch: [71][8/9] loss: 1.17173, MAE: 0.20074, time/step=426757ms, lr=2.76e-05
2024-07-16 02:23:18,503 - logger.py:50 - Epoch: [71] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4493.42s
2024-07-16 02:23:18,504 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 02:31:36,751 - logger.py:50 - Epoch: [72][0/9] loss: 1.14258, MAE: 0.19084, time/step=498235ms, lr=2.70e-05
2024-07-16 03:27:26,482 - logger.py:50 - Epoch: [72][8/9] loss: 1.17173, MAE: 0.20074, time/step=427552ms, lr=2.70e-05
2024-07-16 03:38:32,085 - logger.py:50 - Epoch: [72] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4513.58s
2024-07-16 03:38:32,087 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 03:46:43,844 - logger.py:50 - Epoch: [73][0/9] loss: 1.14258, MAE: 0.19084, time/step=491745ms, lr=2.65e-05
2024-07-16 04:42:47,426 - logger.py:50 - Epoch: [73][8/9] loss: 1.17173, MAE: 0.20074, time/step=428370ms, lr=2.65e-05
2024-07-16 04:53:52,476 - logger.py:50 - Epoch: [73] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4520.39s
2024-07-16 04:53:52,478 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 05:04:06,401 - logger.py:50 - Epoch: [74][0/9] loss: 2.96458, MAE: 0.25491, time/step=613911ms, lr=2.60e-05
2024-07-16 05:57:37,754 - logger.py:50 - Epoch: [74][8/9] loss: 1.17173, MAE: 0.20074, time/step=425029ms, lr=2.60e-05
2024-07-16 06:08:28,556 - logger.py:50 - Epoch: [74] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4476.08s
2024-07-16 06:08:28,558 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 06:11:57,344 - logger.py:50 - Epoch: [75][0/9] loss: 1.37256, MAE: 0.36221, time/step=208775ms, lr=2.55e-05
2024-07-16 07:13:11,905 - logger.py:50 - Epoch: [75][8/9] loss: 1.17173, MAE: 0.20074, time/step=431482ms, lr=2.55e-05
2024-07-16 07:24:05,098 - logger.py:50 - Epoch: [75] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4536.54s
2024-07-16 07:24:05,099 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 07:31:35,904 - logger.py:50 - Epoch: [76][0/9] loss: 0.02596, MAE: 0.04593, time/step=450791ms, lr=2.50e-05
2024-07-16 08:29:05,645 - logger.py:50 - Epoch: [76][8/9] loss: 1.17173, MAE: 0.20074, time/step=433393ms, lr=2.50e-05
2024-07-16 08:40:07,631 - logger.py:50 - Epoch: [76] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4562.53s
2024-07-16 08:40:07,633 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 08:43:48,678 - logger.py:50 - Epoch: [77][0/9] loss: 0.95472, MAE: 0.32155, time/step=221030ms, lr=2.45e-05
2024-07-16 09:45:22,734 - logger.py:50 - Epoch: [77][8/9] loss: 1.17173, MAE: 0.20074, time/step=435010ms, lr=2.45e-05
2024-07-16 09:56:18,499 - logger.py:50 - Epoch: [77] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4570.86s
2024-07-16 09:56:18,501 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 10:03:59,021 - logger.py:50 - Epoch: [78][0/9] loss: 0.21769, MAE: 0.09627, time/step=460509ms, lr=2.40e-05
2024-07-16 11:01:24,953 - logger.py:50 - Epoch: [78][8/9] loss: 1.17173, MAE: 0.20074, time/step=434049ms, lr=2.40e-05
2024-07-16 11:12:38,061 - logger.py:50 - Epoch: [78] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4579.56s
2024-07-16 11:12:38,063 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 11:20:16,515 - logger.py:50 - Epoch: [79][0/9] loss: 0.02596, MAE: 0.04593, time/step=458441ms, lr=2.34e-05
2024-07-16 12:18:13,957 - logger.py:50 - Epoch: [79][8/9] loss: 1.17173, MAE: 0.20074, time/step=437320ms, lr=2.34e-05
2024-07-16 12:29:19,856 - logger.py:50 - Epoch: [79] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4601.79s
2024-07-16 12:29:19,858 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 12:34:18,005 - logger.py:50 - Epoch: [80][0/9] loss: 0.09148, MAE: 0.12241, time/step=298138ms, lr=2.29e-05
2024-07-16 13:35:30,537 - logger.py:50 - Epoch: [80][8/9] loss: 1.17173, MAE: 0.20074, time/step=441186ms, lr=2.29e-05
2024-07-16 13:46:41,529 - logger.py:50 - Epoch: [80] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4641.67s
2024-07-16 13:46:41,530 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 13:51:37,034 - logger.py:50 - Epoch: [81][0/9] loss: 0.09148, MAE: 0.12241, time/step=295488ms, lr=2.24e-05
2024-07-16 14:52:08,964 - logger.py:50 - Epoch: [81][8/9] loss: 1.17173, MAE: 0.20074, time/step=436380ms, lr=2.24e-05
2024-07-16 15:03:25,152 - logger.py:50 - Epoch: [81] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4603.62s
2024-07-16 15:03:25,153 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 15:07:12,038 - logger.py:50 - Epoch: [82][0/9] loss: 0.95472, MAE: 0.32155, time/step=226871ms, lr=2.19e-05
2024-07-16 16:09:42,551 - logger.py:50 - Epoch: [82][8/9] loss: 1.17173, MAE: 0.20074, time/step=441932ms, lr=2.19e-05
2024-07-16 16:20:50,136 - logger.py:50 - Epoch: [82] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4644.98s
2024-07-16 16:20:50,137 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 16:28:31,244 - logger.py:50 - Epoch: [83][0/9] loss: 0.02596, MAE: 0.04593, time/step=461092ms, lr=2.14e-05
2024-07-16 17:27:08,870 - logger.py:50 - Epoch: [83][8/9] loss: 1.17173, MAE: 0.20074, time/step=442080ms, lr=2.14e-05
2024-07-16 17:38:23,699 - logger.py:50 - Epoch: [83] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4653.56s
2024-07-16 17:38:23,700 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 17:42:07,779 - logger.py:50 - Epoch: [84][0/9] loss: 1.37256, MAE: 0.36221, time/step=224067ms, lr=2.09e-05
2024-07-16 18:45:14,921 - logger.py:50 - Epoch: [84][8/9] loss: 1.17173, MAE: 0.20074, time/step=445690ms, lr=2.09e-05
2024-07-16 18:56:36,677 - logger.py:50 - Epoch: [84] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4692.98s
2024-07-16 18:56:36,678 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 19:01:43,843 - logger.py:50 - Epoch: [85][0/9] loss: 0.09148, MAE: 0.12241, time/step=307147ms, lr=2.04e-05
2024-07-16 20:03:52,913 - logger.py:50 - Epoch: [85][8/9] loss: 1.17173, MAE: 0.20074, time/step=448469ms, lr=2.04e-05
2024-07-16 20:15:17,791 - logger.py:50 - Epoch: [85] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4721.10s
2024-07-16 20:15:17,793 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 20:18:59,868 - logger.py:50 - Epoch: [86][0/9] loss: 1.37256, MAE: 0.36221, time/step=222058ms, lr=1.99e-05
2024-07-16 21:22:37,307 - logger.py:50 - Epoch: [86][8/9] loss: 1.17173, MAE: 0.20074, time/step=448833ms, lr=1.99e-05
2024-07-16 21:34:12,839 - logger.py:50 - Epoch: [86] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4735.04s
2024-07-16 21:34:12,840 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 21:43:00,713 - logger.py:50 - Epoch: [87][0/9] loss: 1.14258, MAE: 0.19084, time/step=527856ms, lr=1.94e-05
2024-07-16 22:42:22,610 - logger.py:50 - Epoch: [87][8/9] loss: 1.17173, MAE: 0.20074, time/step=454417ms, lr=1.94e-05
2024-07-16 22:53:57,375 - logger.py:50 - Epoch: [87] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4784.53s
2024-07-16 22:53:57,377 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-16 23:01:53,769 - logger.py:50 - Epoch: [88][0/9] loss: 0.02596, MAE: 0.04593, time/step=476381ms, lr=1.89e-05
2024-07-17 00:02:07,782 - logger.py:50 - Epoch: [88][8/9] loss: 1.17173, MAE: 0.20074, time/step=454488ms, lr=1.89e-05
2024-07-17 00:13:44,422 - logger.py:50 - Epoch: [88] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4787.04s
2024-07-17 00:13:44,424 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 00:24:40,292 - logger.py:50 - Epoch: [89][0/9] loss: 1.41347, MAE: 0.25817, time/step=655859ms, lr=1.84e-05
2024-07-17 01:22:04,154 - logger.py:50 - Epoch: [89][8/9] loss: 1.17173, MAE: 0.20074, time/step=455525ms, lr=1.84e-05
2024-07-17 01:33:46,786 - logger.py:50 - Epoch: [89] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4802.36s
2024-07-17 01:33:46,788 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 01:44:29,460 - logger.py:50 - Epoch: [90][0/9] loss: 2.96458, MAE: 0.25491, time/step=642661ms, lr=1.79e-05
2024-07-17 02:42:34,174 - logger.py:50 - Epoch: [90][8/9] loss: 1.17173, MAE: 0.20074, time/step=458597ms, lr=1.79e-05
2024-07-17 02:54:17,680 - logger.py:50 - Epoch: [90] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4830.89s
2024-07-17 02:54:17,681 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 03:02:20,742 - logger.py:50 - Epoch: [91][0/9] loss: 0.21769, MAE: 0.09627, time/step=483046ms, lr=1.74e-05
2024-07-17 04:03:11,112 - logger.py:50 - Epoch: [91][8/9] loss: 1.17173, MAE: 0.20074, time/step=459269ms, lr=1.74e-05
2024-07-17 04:14:54,993 - logger.py:50 - Epoch: [91] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4837.31s
2024-07-17 04:14:54,994 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 04:20:08,901 - logger.py:50 - Epoch: [92][0/9] loss: 0.09148, MAE: 0.12241, time/step=313897ms, lr=1.70e-05
2024-07-17 05:24:15,662 - logger.py:50 - Epoch: [92][8/9] loss: 1.17173, MAE: 0.20074, time/step=462295ms, lr=1.70e-05
2024-07-17 05:35:46,483 - logger.py:50 - Epoch: [92] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4851.49s
2024-07-17 05:35:46,484 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 05:39:36,889 - logger.py:50 - Epoch: [93][0/9] loss: 0.95472, MAE: 0.32155, time/step=230394ms, lr=1.65e-05
2024-07-17 06:44:14,675 - logger.py:50 - Epoch: [93][8/9] loss: 1.17173, MAE: 0.20074, time/step=456465ms, lr=1.65e-05
2024-07-17 06:55:58,151 - logger.py:50 - Epoch: [93] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4811.67s
2024-07-17 06:55:58,152 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 06:59:46,094 - logger.py:50 - Epoch: [94][0/9] loss: 1.37256, MAE: 0.36221, time/step=227922ms, lr=1.60e-05
2024-07-17 08:05:55,703 - logger.py:50 - Epoch: [94][8/9] loss: 1.17173, MAE: 0.20074, time/step=466392ms, lr=1.60e-05
2024-07-17 08:17:56,504 - logger.py:50 - Epoch: [94] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4918.34s
2024-07-17 08:17:56,505 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 08:21:57,269 - logger.py:50 - Epoch: [95][0/9] loss: 0.95472, MAE: 0.32155, time/step=240751ms, lr=1.55e-05
2024-07-17 09:29:18,583 - logger.py:50 - Epoch: [95][8/9] loss: 1.17173, MAE: 0.20074, time/step=475785ms, lr=1.55e-05
2024-07-17 09:41:16,987 - logger.py:50 - Epoch: [95] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5000.48s
2024-07-17 09:41:16,988 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 09:50:31,040 - logger.py:50 - Epoch: [96][0/9] loss: 1.14258, MAE: 0.19084, time/step=554041ms, lr=1.51e-05
2024-07-17 10:52:04,980 - logger.py:50 - Epoch: [96][8/9] loss: 1.17173, MAE: 0.20074, time/step=471998ms, lr=1.51e-05
2024-07-17 11:04:05,075 - logger.py:50 - Epoch: [96] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4968.09s
2024-07-17 11:04:05,076 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 11:15:21,682 - logger.py:50 - Epoch: [97][0/9] loss: 1.41347, MAE: 0.25817, time/step=676592ms, lr=1.46e-05
2024-07-17 12:14:53,081 - logger.py:50 - Epoch: [97][8/9] loss: 1.17173, MAE: 0.20074, time/step=471999ms, lr=1.46e-05
2024-07-17 12:26:53,616 - logger.py:50 - Epoch: [97] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4968.54s
2024-07-17 12:26:53,618 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 12:30:54,382 - logger.py:50 - Epoch: [98][0/9] loss: 1.37256, MAE: 0.36221, time/step=240753ms, lr=1.41e-05
2024-07-17 13:37:18,899 - logger.py:50 - Epoch: [98][8/9] loss: 1.17173, MAE: 0.20074, time/step=469474ms, lr=1.41e-05
2024-07-17 13:49:28,066 - logger.py:50 - Epoch: [98] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4954.45s
2024-07-17 13:49:28,068 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 13:58:29,561 - logger.py:50 - Epoch: [99][0/9] loss: 1.14258, MAE: 0.19084, time/step=541479ms, lr=1.37e-05
2024-07-17 15:00:16,064 - logger.py:50 - Epoch: [99][8/9] loss: 1.17173, MAE: 0.20074, time/step=471998ms, lr=1.37e-05
2024-07-17 15:12:32,487 - logger.py:50 - Epoch: [99] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 4984.41s
2024-07-17 15:12:32,489 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 15:18:04,825 - logger.py:50 - Epoch: [100][0/9] loss: 0.09148, MAE: 0.12241, time/step=332326ms, lr=1.32e-05
2024-07-17 16:24:22,394 - logger.py:50 - Epoch: [100][8/9] loss: 1.17173, MAE: 0.20074, time/step=478877ms, lr=1.32e-05
2024-07-17 16:36:34,702 - logger.py:50 - Epoch: [100] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5042.21s
2024-07-17 16:36:34,703 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 16:40:33,437 - logger.py:50 - Epoch: [101][0/9] loss: 0.95472, MAE: 0.32155, time/step=238717ms, lr=1.28e-05
2024-07-17 17:48:21,170 - logger.py:50 - Epoch: [101][8/9] loss: 1.17173, MAE: 0.20074, time/step=478494ms, lr=1.28e-05
2024-07-17 18:00:36,078 - logger.py:50 - Epoch: [101] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5041.37s
2024-07-17 18:00:36,080 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 18:09:04,290 - logger.py:50 - Epoch: [102][0/9] loss: 0.21769, MAE: 0.09627, time/step=508198ms, lr=1.24e-05
2024-07-17 19:12:41,528 - logger.py:50 - Epoch: [102][8/9] loss: 1.17173, MAE: 0.20074, time/step=480604ms, lr=1.24e-05
2024-07-17 19:25:01,074 - logger.py:50 - Epoch: [102] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5064.99s
2024-07-17 19:25:01,076 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 19:33:43,797 - logger.py:50 - Epoch: [103][0/9] loss: 0.21769, MAE: 0.09627, time/step=522710ms, lr=1.19e-05
2024-07-17 20:37:57,362 - logger.py:50 - Epoch: [103][8/9] loss: 1.17173, MAE: 0.20074, time/step=486253ms, lr=1.19e-05
2024-07-17 20:50:27,243 - logger.py:50 - Epoch: [103] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5126.17s
2024-07-17 20:50:27,245 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 20:56:00,945 - logger.py:50 - Epoch: [104][0/9] loss: 0.09148, MAE: 0.12241, time/step=333689ms, lr=1.15e-05
2024-07-17 22:03:05,966 - logger.py:50 - Epoch: [104][8/9] loss: 1.17173, MAE: 0.20074, time/step=484301ms, lr=1.15e-05
2024-07-17 22:15:22,488 - logger.py:50 - Epoch: [104] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5095.24s
2024-07-17 22:15:22,490 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 22:19:27,137 - logger.py:50 - Epoch: [105][0/9] loss: 1.37256, MAE: 0.36221, time/step=244637ms, lr=1.11e-05
2024-07-17 23:28:38,911 - logger.py:50 - Epoch: [105][8/9] loss: 1.17173, MAE: 0.20074, time/step=488490ms, lr=1.11e-05
2024-07-17 23:41:16,842 - logger.py:50 - Epoch: [105] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5154.35s
2024-07-17 23:41:16,843 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-17 23:53:07,743 - logger.py:50 - Epoch: [106][0/9] loss: 2.96458, MAE: 0.25491, time/step=710889ms, lr=1.07e-05
2024-07-18 00:54:53,841 - logger.py:50 - Epoch: [106][8/9] loss: 1.17173, MAE: 0.20074, time/step=490776ms, lr=1.07e-05
2024-07-18 01:07:41,647 - logger.py:50 - Epoch: [106] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5184.80s
2024-07-18 01:07:41,649 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 01:17:08,768 - logger.py:50 - Epoch: [107][0/9] loss: 1.14258, MAE: 0.19084, time/step=567105ms, lr=1.03e-05
2024-07-18 02:21:26,364 - logger.py:50 - Epoch: [107][8/9] loss: 1.17173, MAE: 0.20074, time/step=491633ms, lr=1.03e-05
2024-07-18 02:34:12,552 - logger.py:50 - Epoch: [107] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5190.90s
2024-07-18 02:34:12,554 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 02:39:55,280 - logger.py:50 - Epoch: [108][0/9] loss: 0.09148, MAE: 0.12241, time/step=342712ms, lr=9.88e-06
2024-07-18 03:49:06,135 - logger.py:50 - Epoch: [108][8/9] loss: 1.17173, MAE: 0.20074, time/step=499285ms, lr=9.88e-06
2024-07-18 04:01:41,086 - logger.py:50 - Epoch: [108] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5248.53s
2024-07-18 04:01:41,088 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 04:11:11,254 - logger.py:50 - Epoch: [109][0/9] loss: 1.14258, MAE: 0.19084, time/step=570156ms, lr=9.49e-06
2024-07-18 05:15:58,225 - logger.py:50 - Epoch: [109][8/9] loss: 1.17173, MAE: 0.20074, time/step=495236ms, lr=9.49e-06
2024-07-18 05:28:49,581 - logger.py:50 - Epoch: [109] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5228.49s
2024-07-18 05:28:49,582 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 05:32:56,975 - logger.py:50 - Epoch: [110][0/9] loss: 0.95472, MAE: 0.32155, time/step=247378ms, lr=9.11e-06
2024-07-18 06:44:05,257 - logger.py:50 - Epoch: [110][8/9] loss: 1.17173, MAE: 0.20074, time/step=501740ms, lr=9.11e-06
2024-07-18 06:56:43,644 - logger.py:50 - Epoch: [110] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5274.06s
2024-07-18 06:56:43,646 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 07:06:15,930 - logger.py:50 - Epoch: [111][0/9] loss: 1.14258, MAE: 0.19084, time/step=572269ms, lr=8.73e-06
2024-07-18 08:11:13,998 - logger.py:50 - Epoch: [111][8/9] loss: 1.17173, MAE: 0.20074, time/step=496704ms, lr=8.73e-06
2024-07-18 08:23:57,940 - logger.py:50 - Epoch: [111] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5234.29s
2024-07-18 08:23:57,941 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 08:33:32,400 - logger.py:50 - Epoch: [112][0/9] loss: 1.14258, MAE: 0.19084, time/step=574443ms, lr=8.36e-06
2024-07-18 09:38:36,725 - logger.py:50 - Epoch: [112][8/9] loss: 1.17173, MAE: 0.20074, time/step=497641ms, lr=8.36e-06
2024-07-18 09:51:25,140 - logger.py:50 - Epoch: [112] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5247.19s
2024-07-18 09:51:25,143 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 10:03:19,939 - logger.py:50 - Epoch: [113][0/9] loss: 1.41347, MAE: 0.25817, time/step=714780ms, lr=8.00e-06
2024-07-18 11:06:50,542 - logger.py:50 - Epoch: [113][8/9] loss: 1.17173, MAE: 0.20074, time/step=502820ms, lr=8.00e-06
2024-07-18 11:19:42,814 - logger.py:50 - Epoch: [113] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5297.67s
2024-07-18 11:19:42,815 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 11:27:18,945 - logger.py:50 - Epoch: [114][0/9] loss: 1.35237, MAE: 0.27714, time/step=456117ms, lr=7.64e-06
2024-07-18 12:35:43,002 - logger.py:50 - Epoch: [114][8/9] loss: 1.17173, MAE: 0.20074, time/step=506686ms, lr=7.64e-06
2024-07-18 12:48:45,189 - logger.py:50 - Epoch: [114] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5342.37s
2024-07-18 12:48:45,191 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 12:53:02,432 - logger.py:50 - Epoch: [115][0/9] loss: 0.95472, MAE: 0.32155, time/step=257224ms, lr=7.29e-06
2024-07-18 14:04:50,820 - logger.py:50 - Epoch: [115][8/9] loss: 1.17173, MAE: 0.20074, time/step=507290ms, lr=7.29e-06
2024-07-18 14:17:50,390 - logger.py:50 - Epoch: [115] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5345.19s
2024-07-18 14:17:50,392 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 14:29:54,009 - logger.py:50 - Epoch: [116][0/9] loss: 2.96458, MAE: 0.25491, time/step=723603ms, lr=6.95e-06
2024-07-18 15:34:15,254 - logger.py:50 - Epoch: [116][8/9] loss: 1.17173, MAE: 0.20074, time/step=509428ms, lr=6.95e-06
2024-07-18 15:47:14,916 - logger.py:50 - Epoch: [116] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5364.52s
2024-07-18 15:47:14,918 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 15:59:20,766 - logger.py:50 - Epoch: [117][0/9] loss: 1.41347, MAE: 0.25817, time/step=725838ms, lr=6.62e-06
2024-07-18 17:02:59,747 - logger.py:50 - Epoch: [117][8/9] loss: 1.17173, MAE: 0.20074, time/step=504980ms, lr=6.62e-06
2024-07-18 17:15:55,477 - logger.py:50 - Epoch: [117] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5320.56s
2024-07-18 17:15:55,479 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 17:28:08,826 - logger.py:50 - Epoch: [118][0/9] loss: 1.41347, MAE: 0.25817, time/step=733333ms, lr=6.30e-06
2024-07-18 18:32:46,618 - logger.py:50 - Epoch: [118][8/9] loss: 1.17173, MAE: 0.20074, time/step=512347ms, lr=6.30e-06
2024-07-18 18:45:51,327 - logger.py:50 - Epoch: [118] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5395.85s
2024-07-18 18:45:51,328 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 18:53:46,043 - logger.py:50 - Epoch: [119][0/9] loss: 1.35237, MAE: 0.27714, time/step=474703ms, lr=5.99e-06
2024-07-18 20:03:06,949 - logger.py:50 - Epoch: [119][8/9] loss: 1.17173, MAE: 0.20074, time/step=515068ms, lr=5.99e-06
2024-07-18 20:16:20,824 - logger.py:50 - Epoch: [119] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5429.49s
2024-07-18 20:16:20,825 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 20:24:18,872 - logger.py:50 - Epoch: [120][0/9] loss: 1.35237, MAE: 0.27714, time/step=478035ms, lr=5.68e-06
2024-07-18 21:34:14,679 - logger.py:50 - Epoch: [120][8/9] loss: 1.17173, MAE: 0.20074, time/step=519316ms, lr=5.68e-06
2024-07-18 21:47:26,264 - logger.py:50 - Epoch: [120] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5465.44s
2024-07-18 21:47:26,266 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 21:56:53,623 - logger.py:50 - Epoch: [121][0/9] loss: 0.21769, MAE: 0.09627, time/step=567344ms, lr=5.38e-06
2024-07-18 23:05:21,082 - logger.py:50 - Epoch: [121][8/9] loss: 1.17173, MAE: 0.20074, time/step=519423ms, lr=5.38e-06
2024-07-18 23:18:42,493 - logger.py:50 - Epoch: [121] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5476.22s
2024-07-18 23:18:42,494 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-18 23:24:41,565 - logger.py:50 - Epoch: [122][0/9] loss: 0.09148, MAE: 0.12241, time/step=359060ms, lr=5.09e-06
2024-07-19 00:36:26,130 - logger.py:50 - Epoch: [122][8/9] loss: 1.17173, MAE: 0.20074, time/step=518181ms, lr=5.09e-06
2024-07-19 00:49:49,164 - logger.py:50 - Epoch: [122] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5466.67s
2024-07-19 00:49:49,166 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 00:59:53,326 - logger.py:50 - Epoch: [123][0/9] loss: 1.14258, MAE: 0.19084, time/step=604149ms, lr=4.81e-06
2024-07-19 02:08:06,358 - logger.py:50 - Epoch: [123][8/9] loss: 1.17173, MAE: 0.20074, time/step=521909ms, lr=4.81e-06
2024-07-19 02:21:27,769 - logger.py:50 - Epoch: [123] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5498.60s
2024-07-19 02:21:27,771 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 02:31:31,446 - logger.py:50 - Epoch: [124][0/9] loss: 1.14258, MAE: 0.19084, time/step=603664ms, lr=4.54e-06
2024-07-19 03:39:40,232 - logger.py:50 - Epoch: [124][8/9] loss: 1.17173, MAE: 0.20074, time/step=521383ms, lr=4.54e-06
2024-07-19 03:53:11,933 - logger.py:50 - Epoch: [124] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5504.16s
2024-07-19 03:53:11,935 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 04:02:23,789 - logger.py:50 - Epoch: [125][0/9] loss: 0.02596, MAE: 0.04593, time/step=551844ms, lr=4.28e-06
2024-07-19 05:11:27,171 - logger.py:50 - Epoch: [125][8/9] loss: 1.17173, MAE: 0.20074, time/step=521692ms, lr=4.28e-06
2024-07-19 05:24:51,153 - logger.py:50 - Epoch: [125] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5499.22s
2024-07-19 05:24:51,155 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 05:34:10,448 - logger.py:50 - Epoch: [126][0/9] loss: 0.02596, MAE: 0.04593, time/step=559271ms, lr=4.03e-06
2024-07-19 06:43:23,418 - logger.py:50 - Epoch: [126][8/9] loss: 1.17173, MAE: 0.20074, time/step=523582ms, lr=4.03e-06
2024-07-19 06:56:50,192 - logger.py:50 - Epoch: [126] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5519.03s
2024-07-19 06:56:50,193 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 07:09:24,167 - logger.py:50 - Epoch: [127][0/9] loss: 2.96458, MAE: 0.25491, time/step=753963ms, lr=3.79e-06
2024-07-19 08:15:39,548 - logger.py:50 - Epoch: [127][8/9] loss: 1.17173, MAE: 0.20074, time/step=525483ms, lr=3.79e-06
2024-07-19 08:28:54,037 - logger.py:50 - Epoch: [127] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5523.84s
2024-07-19 08:28:54,039 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 08:38:11,793 - logger.py:50 - Epoch: [128][0/9] loss: 0.21769, MAE: 0.09627, time/step=557744ms, lr=3.56e-06
2024-07-19 09:47:20,403 - logger.py:50 - Epoch: [128][8/9] loss: 1.17173, MAE: 0.20074, time/step=522928ms, lr=3.56e-06
2024-07-19 10:00:56,510 - logger.py:50 - Epoch: [128] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5522.47s
2024-07-19 10:00:56,512 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 10:05:25,603 - logger.py:50 - Epoch: [129][0/9] loss: 0.95472, MAE: 0.32155, time/step=269076ms, lr=3.33e-06
2024-07-19 11:19:06,126 - logger.py:50 - Epoch: [129][8/9] loss: 1.17173, MAE: 0.20074, time/step=521067ms, lr=3.33e-06
2024-07-19 11:32:40,939 - logger.py:50 - Epoch: [129] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5504.42s
2024-07-19 11:32:40,940 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 11:42:01,575 - logger.py:50 - Epoch: [130][0/9] loss: 0.21769, MAE: 0.09627, time/step=560618ms, lr=3.12e-06
2024-07-19 12:50:49,194 - logger.py:50 - Epoch: [130][8/9] loss: 1.17173, MAE: 0.20074, time/step=520915ms, lr=3.12e-06
2024-07-19 13:04:02,217 - logger.py:50 - Epoch: [130] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5481.27s
2024-07-19 13:04:02,219 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 13:13:16,729 - logger.py:50 - Epoch: [131][0/9] loss: 0.21769, MAE: 0.09627, time/step=554499ms, lr=2.91e-06
2024-07-19 14:22:02,430 - logger.py:50 - Epoch: [131][8/9] loss: 1.17173, MAE: 0.20074, time/step=520022ms, lr=2.91e-06
2024-07-19 14:35:18,711 - logger.py:50 - Epoch: [131] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5476.49s
2024-07-19 14:35:18,713 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 14:39:42,883 - logger.py:50 - Epoch: [132][0/9] loss: 0.95472, MAE: 0.32155, time/step=264159ms, lr=2.72e-06
2024-07-19 15:53:53,619 - logger.py:50 - Epoch: [132][8/9] loss: 1.17173, MAE: 0.20074, time/step=523877ms, lr=2.72e-06
2024-07-19 16:07:24,865 - logger.py:50 - Epoch: [132] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5526.15s
2024-07-19 16:07:24,867 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 16:19:48,928 - logger.py:50 - Epoch: [133][0/9] loss: 2.96458, MAE: 0.25491, time/step=744051ms, lr=2.54e-06
2024-07-19 17:25:40,840 - logger.py:50 - Epoch: [133][8/9] loss: 1.17173, MAE: 0.20074, time/step=521774ms, lr=2.54e-06
2024-07-19 17:38:59,864 - logger.py:50 - Epoch: [133] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5495.00s
2024-07-19 17:38:59,866 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 17:43:23,055 - logger.py:50 - Epoch: [134][0/9] loss: 1.37256, MAE: 0.36221, time/step=263174ms, lr=2.36e-06
2024-07-19 18:57:41,902 - logger.py:50 - Epoch: [134][8/9] loss: 1.17173, MAE: 0.20074, time/step=524669ms, lr=2.36e-06
2024-07-19 19:11:15,235 - logger.py:50 - Epoch: [134] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5535.37s
2024-07-19 19:11:15,236 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 19:23:42,487 - logger.py:50 - Epoch: [135][0/9] loss: 2.96458, MAE: 0.25491, time/step=747238ms, lr=2.20e-06
2024-07-19 20:30:20,801 - logger.py:50 - Epoch: [135][8/9] loss: 1.17173, MAE: 0.20074, time/step=527284ms, lr=2.20e-06
2024-07-19 20:43:56,395 - logger.py:50 - Epoch: [135] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5561.15s
2024-07-19 20:43:56,396 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 20:56:32,522 - logger.py:50 - Epoch: [136][0/9] loss: 1.41347, MAE: 0.25817, time/step=756111ms, lr=2.05e-06
2024-07-19 22:03:30,358 - logger.py:50 - Epoch: [136][8/9] loss: 1.17173, MAE: 0.20074, time/step=530439ms, lr=2.05e-06
2024-07-19 22:17:08,270 - logger.py:50 - Epoch: [136] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5591.87s
2024-07-19 22:17:08,272 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 22:25:18,565 - logger.py:50 - Epoch: [137][0/9] loss: 1.35237, MAE: 0.27714, time/step=490281ms, lr=1.90e-06
2024-07-19 23:36:32,040 - logger.py:50 - Epoch: [137][8/9] loss: 1.17173, MAE: 0.20074, time/step=529306ms, lr=1.90e-06
2024-07-19 23:50:06,674 - logger.py:50 - Epoch: [137] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5578.40s
2024-07-19 23:50:06,676 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-19 23:58:13,708 - logger.py:50 - Epoch: [138][0/9] loss: 1.35237, MAE: 0.27714, time/step=487020ms, lr=1.77e-06
2024-07-20 01:10:22,019 - logger.py:50 - Epoch: [138][8/9] loss: 1.17173, MAE: 0.20074, time/step=535037ms, lr=1.77e-06
2024-07-20 01:23:55,124 - logger.py:50 - Epoch: [138] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5628.45s
2024-07-20 01:23:55,125 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 01:36:40,707 - logger.py:50 - Epoch: [139][0/9] loss: 1.41347, MAE: 0.25817, time/step=765570ms, lr=1.65e-06
2024-07-20 02:43:48,177 - logger.py:50 - Epoch: [139][8/9] loss: 1.17173, MAE: 0.20074, time/step=532560ms, lr=1.65e-06
2024-07-20 02:57:31,558 - logger.py:50 - Epoch: [139] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5616.43s
2024-07-20 02:57:31,560 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 03:10:16,634 - logger.py:50 - Epoch: [140][0/9] loss: 2.96458, MAE: 0.25491, time/step=765064ms, lr=1.54e-06
2024-07-20 04:17:28,651 - logger.py:50 - Epoch: [140][8/9] loss: 1.17173, MAE: 0.20074, time/step=533009ms, lr=1.54e-06
2024-07-20 04:31:02,657 - logger.py:50 - Epoch: [140] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5611.10s
2024-07-20 04:31:02,659 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 04:41:30,819 - logger.py:50 - Epoch: [141][0/9] loss: 1.14258, MAE: 0.19084, time/step=628149ms, lr=1.43e-06
2024-07-20 05:51:43,171 - logger.py:50 - Epoch: [141][8/9] loss: 1.17173, MAE: 0.20074, time/step=537833ms, lr=1.43e-06
2024-07-20 06:05:31,886 - logger.py:50 - Epoch: [141] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5669.23s
2024-07-20 06:05:31,888 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 06:18:14,985 - logger.py:50 - Epoch: [142][0/9] loss: 1.41347, MAE: 0.25817, time/step=763081ms, lr=1.34e-06
2024-07-20 07:26:09,433 - logger.py:50 - Epoch: [142][8/9] loss: 1.17173, MAE: 0.20074, time/step=537503ms, lr=1.34e-06
2024-07-20 07:39:52,707 - logger.py:50 - Epoch: [142] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5660.81s
2024-07-20 07:39:52,708 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 07:50:08,776 - logger.py:50 - Epoch: [143][0/9] loss: 1.14258, MAE: 0.19084, time/step=616053ms, lr=1.26e-06
2024-07-20 09:00:23,564 - logger.py:50 - Epoch: [143][8/9] loss: 1.17173, MAE: 0.20074, time/step=536760ms, lr=1.26e-06
2024-07-20 09:14:08,213 - logger.py:50 - Epoch: [143] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5655.50s
2024-07-20 09:14:08,214 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 09:26:52,819 - logger.py:50 - Epoch: [144][0/9] loss: 1.41347, MAE: 0.25817, time/step=764592ms, lr=1.19e-06
2024-07-20 10:34:47,780 - logger.py:50 - Epoch: [144][8/9] loss: 1.17173, MAE: 0.20074, time/step=537728ms, lr=1.19e-06
2024-07-20 10:48:29,518 - logger.py:50 - Epoch: [144] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5661.30s
2024-07-20 10:48:29,519 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 11:01:09,734 - logger.py:50 - Epoch: [145][0/9] loss: 2.96458, MAE: 0.25491, time/step=760202ms, lr=1.13e-06
2024-07-20 12:08:12,835 - logger.py:50 - Epoch: [145][8/9] loss: 1.17173, MAE: 0.20074, time/step=531478ms, lr=1.13e-06
2024-07-20 12:21:54,055 - logger.py:50 - Epoch: [145] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5604.53s
2024-07-20 12:21:54,057 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 12:28:00,823 - logger.py:50 - Epoch: [146][0/9] loss: 0.09148, MAE: 0.12241, time/step=366753ms, lr=1.09e-06
2024-07-20 13:41:30,516 - logger.py:50 - Epoch: [146][8/9] loss: 1.17173, MAE: 0.20074, time/step=530716ms, lr=1.09e-06
2024-07-20 13:55:09,758 - logger.py:50 - Epoch: [146] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5595.70s
2024-07-20 13:55:09,760 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 14:07:55,066 - logger.py:50 - Epoch: [147][0/9] loss: 2.96458, MAE: 0.25491, time/step=765292ms, lr=1.05e-06
2024-07-20 15:15:07,579 - logger.py:50 - Epoch: [147][8/9] loss: 1.17173, MAE: 0.20074, time/step=533090ms, lr=1.05e-06
2024-07-20 15:28:44,546 - logger.py:50 - Epoch: [147] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5614.78s
2024-07-20 15:28:44,547 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 15:37:59,686 - logger.py:50 - Epoch: [148][0/9] loss: 0.02596, MAE: 0.04593, time/step=555126ms, lr=1.02e-06
2024-07-20 16:48:31,663 - logger.py:50 - Epoch: [148][8/9] loss: 1.17173, MAE: 0.20074, time/step=531900ms, lr=1.02e-06
2024-07-20 17:02:17,179 - logger.py:50 - Epoch: [148] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5612.63s
2024-07-20 17:02:17,181 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 17:11:57,700 - logger.py:50 - Epoch: [149][0/9] loss: 0.21769, MAE: 0.09627, time/step=580503ms, lr=1.01e-06
2024-07-20 18:22:40,269 - logger.py:50 - Epoch: [149][8/9] loss: 1.17173, MAE: 0.20074, time/step=535897ms, lr=1.01e-06
2024-07-20 18:36:30,483 - logger.py:50 - Epoch: [149] train loss: 1.17173, train MAE: 0.20074,val loss: 6.85961, val MAE: 0.38928,test loss: 0.33688, test MAE: 0.12788,Time: 5653.30s
2024-07-20 18:36:30,484 - logger.py:50 - Best -- epoch=0, train loss: 1.17173, val loss: 6.85961, test loss: 0.33688

2024-07-20 18:36:30,485 - logger.py:50 - fold_4 test LOSS:0.33688
